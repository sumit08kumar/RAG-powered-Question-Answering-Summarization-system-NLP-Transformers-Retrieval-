{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Demo Notebook\n",
    "\n",
    "This notebook demonstrates the core functionality of the RAG Question-Answering system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.13/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in ./.venv/lib/python3.13/site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./.venv/lib/python3.13/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in ./.venv/lib/python3.13/site-packages (from langchain) (0.4.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.13/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.13/site-packages (from langchain) (2.0.42)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.13/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: transformers and langchain not yet installed\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01membed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_index_from_chunks\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mretriever\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_retriever_from_chunks\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_generator\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/End-to-End Build Instructions/src/generator.py:26\u001b[39m\n\u001b[32m     21\u001b[39m     AutoModelForSeq2SeqLM = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     23\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mRAGGenerator\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"Text generator for RAG question answering and summarization.\"\"\"\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/flan-t5-base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/End-to-End Build Instructions/src/generator.py:128\u001b[39m, in \u001b[36mRAGGenerator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     summarization_prompt = PromptTemplate(\n\u001b[32m    116\u001b[39m         template=summarization_template,\n\u001b[32m    117\u001b[39m         input_variables=[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    118\u001b[39m     )\n\u001b[32m    120\u001b[39m     \u001b[38;5;28mself\u001b[39m.summarization_chain = LLMChain(\n\u001b[32m    121\u001b[39m         llm=\u001b[38;5;28mself\u001b[39m.llm,\n\u001b[32m    122\u001b[39m         prompt=summarization_prompt,\n\u001b[32m    123\u001b[39m         verbose=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    124\u001b[39m     )\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manswer_question\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[32m    127\u001b[39m                    question: \u001b[38;5;28mstr\u001b[39m, \n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m                    retrieved_docs: List[\u001b[43mDocument\u001b[49m],\n\u001b[32m    129\u001b[39m                    max_context_length: \u001b[38;5;28mint\u001b[39m = \u001b[32m2000\u001b[39m) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Generate an answer to a question using retrieved documents.\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m \u001b[33;03m        Dictionary containing answer and metadata\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.qa_chain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'Document' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('src')\n",
    "\n",
    "from ingest import ingest_documents\n",
    "from embed import build_index_from_chunks\n",
    "from retriever import create_retriever_from_chunks\n",
    "from generator import create_generator\n",
    "from langchain.schema import Document\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Ingestion\n",
    "\n",
    "First, let's ingest some sample documents and see how they are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample document\n",
    "sample_doc_content = \"\"\"\n",
    "Machine Learning Fundamentals\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that focuses on developing algorithms \n",
    "that can learn and make decisions from data without being explicitly programmed.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Uses labeled data to train models\n",
    "2. Unsupervised Learning: Finds patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learns through interaction with environment\n",
    "\n",
    "Deep Learning:\n",
    "Deep learning is a subset of machine learning that uses neural networks with multiple layers.\n",
    "It has been particularly successful in image recognition, natural language processing, and speech recognition.\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('../sample_ml_doc.txt', 'w') as f:\n",
    "    f.write(sample_doc_content)\n",
    "\n",
    "print(\"Sample document created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the document\n",
    "document_paths = ['../sample_ml_doc.txt']\n",
    "chunks = ingest_documents(document_paths)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(document_paths)} document(s)\")\n",
    "print(\"\\nFirst few chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Content: {chunk['content'][:100]}...\")\n",
    "    print(f\"Source: {chunk['source_document']}\")\n",
    "    print(f\"Chunk ID: {chunk['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding and Indexing\n",
    "\n",
    "Now let's create embeddings and build a FAISS index for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index\n",
    "embedding_manager = build_index_from_chunks(chunks, index_path='demo_index')\n",
    "print(\"FAISS index created successfully!\")\n",
    "print(f\"Index contains {embedding_manager.index.ntotal} vectors\")\n",
    "print(f\"Embedding dimension: {embedding_manager.dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test similarity search\n",
    "query = \"What is deep learning?\"\n",
    "results = embedding_manager.search(query, k=3)\n",
    "\n",
    "print(f\"Search results for: '{query}'\")\n",
    "print(\"=\" * 50)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Score: {result['score']:.3f}\")\n",
    "    print(f\"Content: {result['content'][:150]}...\")\n",
    "    print(f\"Source: {result['chunk_metadata'].get('source_document', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LangChain Retrieval\n",
    "\n",
    "Let's use LangChain for more advanced retrieval capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain retriever\n",
    "retriever = create_retriever_from_chunks(chunks, save_path='langchain_demo_store')\n",
    "print(\"LangChain retriever created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval with LangChain\n",
    "query = \"What are the types of machine learning?\"\n",
    "documents = retriever.retrieve_documents(query)\n",
    "\n",
    "print(f\"Retrieved {len(documents)} documents for: '{query}'\")\n",
    "print(\"=\" * 50)\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Question Answering\n",
    "\n",
    "Now let's use the generator to answer questions based on retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "generator = create_generator()\n",
    "print(\"Generator created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question answering\n",
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What are the main types of machine learning?\",\n",
    "    \"How is deep learning different from machine learning?\",\n",
    "    \"What applications is deep learning good for?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    docs = retriever.retrieve_documents(question)\n",
    "    \n",
    "    # Generate answer\n",
    "    result = generator.answer_question(question, docs)\n",
    "    \n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "    print(f\"Sources: {len(result['sources'])} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Summarization\n",
    "\n",
    "Let's test the summarization functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summarization\n",
    "all_docs = retriever.retrieve_documents(\"machine learning deep learning\")\n",
    "summary_result = generator.summarize_documents(all_docs)\n",
    "\n",
    "print(\"Document Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(summary_result['summary'])\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"Documents processed: {summary_result['num_documents']}\")\n",
    "print(f\"Input length: {summary_result['input_length']} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Analysis\n",
    "\n",
    "Let's analyze the performance of different components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Measure retrieval performance\n",
    "test_queries = [\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"supervised learning\",\n",
    "    \"neural networks\",\n",
    "    \"artificial intelligence\"\n",
    "]\n",
    "\n",
    "retrieval_times = []\n",
    "generation_times = []\n",
    "\n",
    "for query in test_queries:\n",
    "    # Measure retrieval time\n",
    "    start_time = time.time()\n",
    "    docs = retriever.retrieve_documents(query)\n",
    "    retrieval_time = time.time() - start_time\n",
    "    retrieval_times.append(retrieval_time)\n",
    "    \n",
    "    # Measure generation time\n",
    "    start_time = time.time()\n",
    "    result = generator.answer_question(query, docs)\n",
    "    generation_time = time.time() - start_time\n",
    "    generation_times.append(generation_time)\n",
    "\n",
    "print(\"Performance Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Average retrieval time: {sum(retrieval_times)/len(retrieval_times):.3f} seconds\")\n",
    "print(f\"Average generation time: {sum(generation_times)/len(generation_times):.3f} seconds\")\n",
    "print(f\"Total average time: {(sum(retrieval_times) + sum(generation_times))/len(test_queries):.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "\n",
    "Clean up temporary files created during the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Clean up files\n",
    "files_to_remove = [\n",
    "    '../sample_ml_doc.txt',\n",
    "    'demo_index.faiss',\n",
    "    'demo_index_metadata.json'\n",
    "]\n",
    "\n",
    "dirs_to_remove = [\n",
    "    'langchain_demo_store'\n",
    "]\n",
    "\n",
    "for file_path in files_to_remove:\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed: {file_path}\")\n",
    "\n",
    "for dir_path in dirs_to_remove:\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "        print(f\"Removed directory: {dir_path}\")\n",
    "\n",
    "print(\"\\nCleanup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the core functionality of the RAG system:\n",
    "\n",
    "1. **Document Ingestion**: Processing and chunking documents\n",
    "2. **Embedding & Indexing**: Creating vector representations and FAISS index\n",
    "3. **Retrieval**: Finding relevant document chunks for queries\n",
    "4. **Generation**: Answering questions and summarizing documents\n",
    "5. **Performance**: Measuring system response times\n",
    "\n",
    "The system successfully processes documents, creates searchable embeddings, retrieves relevant content, and generates coherent answers with source attribution.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try with your own documents\n",
    "- Experiment with different models\n",
    "- Tune parameters for your use case\n",
    "- Deploy using the provided Docker configuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
