{
  "model_name": "all-mpnet-base-v2",
  "dimension": 768,
  "num_vectors": 307,
  "chunks_metadata": [
    {
      "content": "Machine Learning Stanford,California Contents Acknowledgments viii Part I Supervised Learning 1 1 LinearRegression 3 1.1 Leastmeansquares(LMS)algorithm 4 1.2 Thenormalequations 8 1.2.1 Matrixderivatives 9 1.2.2 Leastsquaresrevisited 9 1.3 Probabilisticinterpretation 11 1.4 Locallyweightedlinearregression 13 2 ClassificationandLogisticRegression 16 2.1 Logisticregression 16 2.2 Digression:Theperceptronlearningalgorithm 19 2.3 Anotheralgorithmformaximizing(cid:96)(\u03b8) 20 3 GeneralizedLinearModels 22 3.1 Theexponentialfamily 22 contents iii 3.2 ConstructingGLMs 24 3.2.1 OrdinaryLeastSquares 25 3.2.2 LogisticRegression 26 3.2.3 SoftmaxRegression 26 Part II Generative Learning Algorithms 31 4 Gaussiandiscriminantanalysis 32 4.1 TheGaussianDiscriminantAnalysismodel 34 4.2 Discussion:GDAandlogisticregression 36 5 NaiveBayes 38 5.1 Laplacesmoothing 41 5.2 Eventmodelsfortextclassification 43 Part III Kernel Methods 46 6 Kernelmethods 46 6.1 Featuremaps 46 6.2 LMS(leastmeansquares)withfeatures 47 6.3 LMSwiththekerneltrick 47 6.4 Propertiesofkernels 51 Part IV Support Vector Machines 57 7 Supportvectormachines 57 7.1 Margins:Intuition 57 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.s",
      "chunk_id": 0,
      "start_pos": 0,
      "end_pos": 1200,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "6.3 LMSwiththekerneltrick 47 6.4 Propertiesofkernels 51 Part IV Support Vector Machines 57 7 Supportvectormachines 57 7.1 Margins:Intuition 57 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu iv contents 7.2 Notation 58 7.3 Functionalandgeometricmargins 59 7.4 Theoptimalmarginclassifier 61 7.5 Lagrangeduality(optionalreading) 62 7.6 Optimalmarginclassifiers 65 7.7 Regularizationandthenon-separablecase(optionalreading) 69 7.8 TheSMOalgorithm(optionalreading) 70 7.8.1 Coordinateascent 71 7.9 SMO 71 Part V Deep Learning 75 8 SupervisedLearningwithNon-LinearModels 75 9 NeuralNetworks 78 10 Backpropagation 87 10.1 Preliminary:chainrule 88 10.2 Backpropagationfortwo-layerneuralnetworks 88 10.2.1 Computing \u2202J 89 \u2202W[2] 10.2.2 Computing \u2202J 89 \u2202W[1] 10.2.3 Computing \u2202J 90 \u2202z 10.2.4 Computing \u2202J 91 \u2202a 10.2.5 Summaryfortwo-layerneuralnetworks 92 10.3 Multi-layerneuralnetworks 92 11 VectorizationOverTrainingExamples 95 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu contents v Part VI Regularization and Model Selection 98 12 Crossvalidation 98 13 FeatureSelection 100 14 Bayesianstatisticsandregularization 103 15 Somecalculationsfrombiasvariance 105 16 Bia",
      "chunk_id": 1,
      "start_pos": 1000,
      "end_pos": 2200,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ord.edu contents v Part VI Regularization and Model Selection 98 12 Crossvalidation 98 13 FeatureSelection 100 14 Bayesianstatisticsandregularization 103 15 Somecalculationsfrombiasvariance 105 16 Bias-varianceanderroranalysis 108 16.1 Thebias-variancetradeoff 108 16.2 Erroranalysis 110 16.3 Ablativeanalysis 111 16.3.1 Analyzeyourmistakes 112 Part VII Unsupervised Learning 114 17 The k-meansClusteringAlgorithm 114 18 MixturesofGaussiansandtheEMAlgorithm 115 Part VIII The EM Algorithm 119 19 Jensen\u2019sinequality 119 20 TheEMalgorithm 120 20.1 OtherinterpretationofELBO 126 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu vi contents 21 MixtureofGaussiansrevisited 126 22 Variationalinferenceandvariationalauto-encoder 128 Part IX Factor Analysis 133 \u03a3 23 Restrictionsof 134 24 MarginalsandconditionalsofGaussians 135 25 Thefactoranalysismodel 136 26 EMforfactoranalysis 138 Part X Principal Components Analysis 142 Part XI Independent Components Analysis 147 27 ICAambiguities 148 28 Densitiesandlineartransformations 149 29 ICAalgorithm 150 Part XII Reinforcement Learning and Control 154 30 Markovdecisionprocesses 155 31 Valueiterationandpolicyiteration 158 2021-05-2300:18:",
      "chunk_id": 2,
      "start_pos": 2000,
      "end_pos": 3200,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "148 28 Densitiesandlineartransformations 149 29 ICAalgorithm 150 Part XII Reinforcement Learning and Control 154 30 Markovdecisionprocesses 155 31 Valueiterationandpolicyiteration 158 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu contents vii 32 LearningamodelforanMDP 160 33 ContinuousstateMDPs 162 33.1 Discretization 162 33.2 Valuefunctionapproximation 163 33.2.1 Usingamodelorsimulator 164 33.2.2 Fittedvalueiteration 165 34 ConnectionsbetweenPolicyandValueIteration(Optional) 169 35 DerivationsforBellmanEquations 171 A LagrangeMultipliers 172 B Boosting 175 B.1 Boosting 175 B.1.1 Theboostingalgorithm 176 B.2 TheconvergenceofBoosting 178 B.3 Implementingweak-learners 180 B.3.1 Decisionstumps 180 B.3.2 Otherstrategies 181 B.4 ProofoflemmaB.1 183 References 184 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu Acknowledgments ThisworkistakenfromthelecturenotesforthecourseMachineLearningatStan- fordUniversity,CS229(cs229.stanford.edu).Thecontributorstothecontent ofthisworkareAndrewNg,ChristopherR\u00e9,MosesCharikar,TengyuMa,Anand Avati,KianKatanforoosh,YoannLeCalonnec,andJohnDuchi\u2014thiscollection issimplyatypesettingofexistinglecturenoteswithminormodi",
      "chunk_id": 3,
      "start_pos": 3000,
      "end_pos": 4200,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "tothecontent ofthisworkareAndrewNg,ChristopherR\u00e9,MosesCharikar,TengyuMa,Anand Avati,KianKatanforoosh,YoannLeCalonnec,andJohnDuchi\u2014thiscollection issimplyatypesettingofexistinglecturenoteswithminormodifications.We wouldliketothanktheoriginalauthorsfortheircontribution.Inaddition,we wishtothankMykelKochenderferandTimWheelerfortheircontributiontothe Tufte-AlgorithmsLATEXtemplate,basedoffofAlgorithmsforOptimization.1 1M.J. Kochenderfer and T.A. Wheeler, Algorithms for Optimiza- tion.MITPress,2019. Robert J. Moss Stanford,Calif. May23,2021 Ancillarymaterialisavailableonthetemplate\u2019swebpage: https://github.com/sisl/textbook_template Part I: Supervised Learning FromCS229Fall2020,TengyuMa, Let\u2019s start by talking about a few examples of supervised learning problems. Andrew Ng, Moses Charikar, & Supposewehaveadatasetgivingthelivingareasandpricesof47housesfrom ChristopherR\u00e9,StanfordUniver- sity. Portland,Oregon: Livingarea(feet2) Price(1000$s) Table1.HousingpricesinPortland, OR. 2104 400 1600 330 2400 369 1416 232 3000 540 . . . . . . Wecanplotthisdata: 800 600 400 200 1,000 2,000 3,000 4,000 5,000 squarefeet )0001$ni(ecirp housingprices Figure1. HousingpricesinPort- land,OR.",
      "chunk_id": 4,
      "start_pos": 4000,
      "end_pos": 5183,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "2104 400 1600 330 2400 369 1416 232 3000 540 . . . . . . Wecanplotthisdata: 800 600 400 200 1,000 2,000 3,000 4,000 5,000 squarefeet )0001$ni(ecirp housingprices Figure1. HousingpricesinPort- land,OR. 2 Givendatalikethis,howcanwelearntopredictthepricesofotherhousesin Portland,asafunctionofthesizeoftheirlivingareas? Toestablishnotationforfutureuse,we\u2019llusex(i)todenotethe\u2018\u2018input\u2019\u2019variables (livingareainthisexample),alsocalledinputfeatures,andy(i) todenotethe \u2018\u2018output\u2019\u2019ortargetvariablethatwearetryingtopredict(price).Apair(x(i),y(i)) iscalledatrainingexample,andthedatasetthatwe\u2019llbeusingtolearn\u2014alist ofntrainingexamples{(x(i),y(i)); i = 1,...,n}\u2014iscalledatrainingset.Note thatthesuperscript\u2018\u2018(i)\u2019\u2019inthenotationissimplyanindexintothetrainingset, andhasnothingtodowithexponentiation.WewillalsouseX denotethespace ofinputvalues,andY thespaceofoutputvalues.Inthisexample,X = Y =R. Todescribethesupervisedlearningproblemslightlymoreformally,ourgoal is,givenatrainingset,tolearnafunctionh : X (cid:55)\u2192 Y sothath(x)isa\u2018\u2018good\u2019\u2019 predictorforthecorrespondingvalueofy.Forhistoricalreasons,thisfunctionh iscalledahypothesis.Seenpictorially,theprocessisthereforelikethis: training Figure2.",
      "chunk_id": 5,
      "start_pos": 4983,
      "end_pos": 6164,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "tionh : X (cid:55)\u2192 Y sothath(x)isa\u2018\u2018good\u2019\u2019 predictorforthecorrespondingvalueofy.Forhistoricalreasons,thisfunctionh iscalledahypothesis.Seenpictorially,theprocessisthereforelikethis: training Figure2. Hypothesisdiagram. set learning algorithm x h predictedy (livingarea (predictedprice ofhouse) ofhouse) Whenthetargetvariablethatwe\u2019retryingtopredictiscontinuous,suchasin ourhousingexample,wecallthelearningproblemaregression2problem.When 2Thetermregressionwasoriginally ycantakeononlyasmallnumberofdiscretevalues(suchasif,giventheliving coineddueto\u2018\u2018regressing\u2019\u2019tothe mean(FrancisGalton,1886). area,wewantedtopredictifadwellingisahouseoranapartment,say),wecall itaclassificationproblem. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 1 Linear Regression Tomakeourhousingexamplemoreinteresting,let\u2019sconsideraslightlyricher datasetinwhichwealsoknowthenumberofbedroomsineachhouse: Livingarea(feet2) #Bedrooms Price(1000$s) Table1.1. Housingpriceswithbed- roomsinPortland,OR. 2104 3 400 1600 3 330 2400 3 369 1416 2 232 3000 4 540 . . . . . . . . .",
      "chunk_id": 6,
      "start_pos": 5964,
      "end_pos": 7033,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "owthenumberofbedroomsineachhouse: Livingarea(feet2) #Bedrooms Price(1000$s) Table1.1. Housingpriceswithbed- roomsinPortland,OR. 2104 3 400 1600 3 330 2400 3 369 1416 2 232 3000 4 540 . . . . . . . . . Here,thex\u2019saretwo-dimensionalvectorsinR2.Forinstance,x (i) istheliving 1 areaofthei-thhouseinthetrainingset,andx (i) isitsnumberofbedrooms.1 1In general, when designing a 2 Toperformsupervisedlearning,wemustdecidehowwe\u2019regoingtorepresent learning problem, it will be up toyoutodecidewhatfeaturesto functions/hypotheseshinacomputer.Asaninitialchoice,let\u2019ssaywedecideto choose,soifyouareoutinPortland approximateyasalinearfunctionofx: gatheringhousingdata,youmight also decide to include other fea- turessuchaswhethereachhouse h \u03b8 (x) = \u03b8 0 +\u03b8 1 x 1 +\u03b8 2 x 2 (1.1) hasafireplace,thenumberofbath- rooms,andsoon.We\u2019llsaymore Here, the \u03b8 i \u2019s are the parameters (also called weights) parameterizing the aboutfeatureselectionlater,butfor spaceoflinearfunctionsmappingfromX toY.Whenthereisnoriskofconfusion, nowlet\u2019stakethefeaturesasgiven.",
      "chunk_id": 7,
      "start_pos": 6833,
      "end_pos": 7866,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "i \u2019s are the parameters (also called weights) parameterizing the aboutfeatureselectionlater,butfor spaceoflinearfunctionsmappingfromX toY.Whenthereisnoriskofconfusion, nowlet\u2019stakethefeaturesasgiven. wewilldropthe\u03b8subscriptinh (x),andwriteitmoresimplyash(x).Tosimplify \u03b8 our notation, we also introduce the convention of letting x = 1 (this is the 0 interceptterm),sothat d h(x) = \u2211 \u03b8 x = \u03b8 (cid:62) x, (1.2) i i i=0 whereontheright-handsideaboveweareviewing\u03b8andxbothasvectors,and heredisthenumberofinputvariables(notcountingx ). 0 4 chapter 1. linear regression Now,givenatrainingset,howdowepick,orlearn,theparameters\u03b8?One reasonablemethodseemstobetomakeh(x)closetoy,atleastforthetraining exampleswehave.Toformalizethis,wewilldefineafunctionthatmeasures,for eachvalueofthe\u03b8\u2019s,howclosetheh(x(i))\u2019saretothecorrespondingy(i)\u2019s.We definethecostfunction: J(\u03b8) = 1 \u2211 n (cid:16) h (x (i))\u2212y (i) (cid:17)2 . (1.3) \u03b8 2 i=1 Ifyou\u2019veseenlinearregressionbefore,youmayrecognizethisasthefamiliar least-squarescostfunctionthatgivesrisetotheordinaryleastsquaresregression model.Whetherornotyouhaveseenitpreviously,let\u2019skeepgoing,andwe\u2019ll eventuallyshowthistobeaspecialcaseofamuchbroaderfamilyofalgorithms.",
      "chunk_id": 8,
      "start_pos": 7666,
      "end_pos": 8857,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "quarescostfunctionthatgivesrisetotheordinaryleastsquaresregression model.Whetherornotyouhaveseenitpreviously,let\u2019skeepgoing,andwe\u2019ll eventuallyshowthistobeaspecialcaseofamuchbroaderfamilyofalgorithms. 1.1 Leastmeansquares(LMS)algorithm Wewanttochoose\u03b8soastominimize J(\u03b8).Todoso,let\u2019suseasearchalgorithm that starts with some \u2018\u2018initial guess\u2019\u2019 for \u03b8, and that repeatedly changes \u03b8 to make J(\u03b8)smaller,untilhopefullyweconvergetoavalueof\u03b8 thatminimizes J(\u03b8).Specifically,let\u2019sconsiderthegradientdescentalgorithm,whichstartswith someinitial\u03b8,andrepeatedlyperformstheupdate:2 2This update is simultaneously performed for all values of j = \u2202 0,...,d. \u03b8 \u2190 \u03b8 \u2212\u03b1 J(\u03b8) (1.4) j j \u2202\u03b8 j Here,\u03b1iscalledthelearningrate.Thisisaverynaturalalgorithmthatrepeatedly takesastepinthedirectionofsteepestdecreaseof J. Inordertoimplementthisalgorithm,wehavetoworkoutwhatisthepartial derivativetermontherighthandside.Let\u2019sfirstworkitoutforthecaseofif wehaveonlyonetrainingexample(x,y),sothatwecanneglectthesuminthe definitionof J.Wehave: 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 1.1.",
      "chunk_id": 9,
      "start_pos": 8657,
      "end_pos": 9745,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "de.Let\u2019sfirstworkitoutforthecaseofif wehaveonlyonetrainingexample(x,y),sothatwecanneglectthesuminthe definitionof J.Wehave: 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 1.1. least mean squares (lms) algorithm 5 \u2202 \u2202 1 J(\u03b8) = (h (x)\u2212y)2 \u03b8 \u2202\u03b8 \u2202\u03b8 2 j j 1 \u2202 =2\u00b7 (h (x)\u2212y)\u00b7 (h (x)\u2212y) \u03b8 \u03b8 2 \u2202\u03b8 j (cid:32) (cid:33) \u2202 \u2211 d = (h (x)\u2212y)\u00b7 \u03b8 x \u2212y \u03b8 i i \u2202\u03b8 j i=0 = (h (x)\u2212y)x \u03b8 j Forasingletrainingexample,thisgivestheupdaterule:3 3Weusethenotation\u2018\u2018a \u2190 b\u2019\u2019to denoteanoperation(inacomputer \u03b8 \u2190 \u03b8 +\u03b1 (cid:16) y (i)\u2212h (x (i)) (cid:17) x (i) . (1.5) program)inwhichwesetthevalue j j \u03b8 j ofavariableatobeequaltothe valueofb(something:=isused). TheruleiscalledtheLMSupdaterule(LMSstandsfor\u2018\u2018leastmeansquares\u2019\u2019), Inotherwords,thisoperationover- andisalsoknownastheWidrow-Hofflearningrule.Thisrulehasseveralprop- writesawiththevalueofb.Incon- ertiesthatseemnaturalandintuitive.Forinstance,themagnitudeoftheupdate trast,wewillwrite\u2018\u2018a=b\u2019\u2019when weareassertingastatementoffact, isproportionaltotheerrorterm(y(i)\u2212h \u03b8 (x(i)));thus,forinstance,ifweareen- thatthevalueofaisequaltothe counteringatrainingexampleonwhichourpredictionnearlymatchestheactual valueofb.",
      "chunk_id": 10,
      "start_pos": 9545,
      "end_pos": 10704,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ngastatementoffact, isproportionaltotheerrorterm(y(i)\u2212h \u03b8 (x(i)));thus,forinstance,ifweareen- thatthevalueofaisequaltothe counteringatrainingexampleonwhichourpredictionnearlymatchestheactual valueofb. valueofy(i),thenwefindthatthereislittleneedtochangetheparameters;in contrast,alargerchangetotheparameterswillbemadeifourpredictionh (x(i)) \u03b8 hasalargeerror(i.e.,ifitisveryfarfromy(i)). We\u2019vederivedtheLMSruleforwhentherewasonlyasingletrainingexample. Therearetwowaystomodifythismethodforatrainingsetofmorethanone example.Thefirstisreplaceitwiththefollowingalgorithm: repeat Algorithm1.1.Gradientdescent. foreveryjdo n (cid:16) (cid:17) \u03b8 \u2190 \u03b8 +\u03b1 \u2211 y (i)\u2212h (x (i)) x (i) j j \u03b8 j i=1 endfor untilconvergence Bygroupingtheupdatesofthecoordinatesintoanupdateofthevector\u03b8,we canrewriteupdatealgorithm1.1inaslightlymoresuccinctway: Thereadercaneasilyverifythatthequantityinthesummationintheupdate ruleaboveisjust \u2202J(\u03b8)/\u2202\u03b8 (fortheoriginaldefinitionof J).So,thisissimply j toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 6 chapter 1. linear regression repeat Algorithm 1.2. Gradient descent n (cid:16) (cid:17) vectorized.",
      "chunk_id": 11,
      "start_pos": 10504,
      "end_pos": 11641,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ionof J).So,thisissimply j toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 6 chapter 1. linear regression repeat Algorithm 1.2. Gradient descent n (cid:16) (cid:17) vectorized. \u03b8 \u2190 \u03b8+\u03b1 \u2211 y (i)\u2212h (x (i)) x (i) \u03b8 i=1 untilconvergence gradient descent on the original cost function J. This method looks at every example in the entire training set on every step, and is called batch gradient descent.Notethat,whilegradientdescentcanbesusceptibletolocalminima ingeneral,theoptimizationproblemwehaveposedhereforlinearregression hasonlyoneglobal,andnootherlocal,optima;thusgradientdescentalways converges(assumingthelearningrate\u03b1isnottoolarge)totheglobalminimum. Indeed, Jisaconvexquadraticfunction. Hereisanexampleofgradientdescentasitisruntominimizeaquadratic Example1.1. Gradientdescenton aquadraticfunction. function. 40 20 0 \u221220 \u221240 \u221240 \u221220 0 20 40 Theellipsesshownabovearethecontoursofaquadraticfunction.Also shownisthetrajectorytakenbygradientdescent,whichwasinitializedat (48,30).Thearrowsinthefigure(joinedbystraightlines)markthesuccessive valuesof\u03b8thatgradientdescentwentthrough. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 1.1.",
      "chunk_id": 12,
      "start_pos": 11441,
      "end_pos": 12624,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "initializedat (48,30).Thearrowsinthefigure(joinedbystraightlines)markthesuccessive valuesof\u03b8thatgradientdescentwentthrough. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 1.1. least mean squares (lms) algorithm 7 Whenwerunbatchgradientdescenttofit\u03b8onourpreviousdataset,tolearn topredicthousingpriceasafunctionoflivingarea.Weobtain: \u03b8 =71.27 (intercept) 0 \u03b8 =0.1345 (slope) 1 Ifweploth (x)asafunctionofx(area),alongwiththetrainingdata,we \u03b8 obtainthefollowingfigure: 800 600 400 200 1,000 2,000 3,000 4,000 5,000 squarefeet )0001$ni(ecirp Example 1.2. Best fit line using batchgradientdescentonPortland, Oregonhousingprices. housingprices Ifthenumberofbedroomswereincludedasoneoftheinputfeaturesas well,weget\u03b8 =89.60,\u03b8 =0.1392,\u03b8 = \u22128.738. 0 1 2 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 8 chapter 1. linear regression Theresultsinexample1.2wereobtainedwithbatchgradientdescent.Thereis analternativetobatchgradientdescentthatalsoworksverywell.Considerthe followingalgorithm: repeat Algorithm1.3.Stochasticgradient descent.",
      "chunk_id": 13,
      "start_pos": 12424,
      "end_pos": 13498,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "resultsinexample1.2wereobtainedwithbatchgradientdescent.Thereis analternativetobatchgradientdescentthatalsoworksverywell.Considerthe followingalgorithm: repeat Algorithm1.3.Stochasticgradient descent. fori =1tondo foreveryjdo n (cid:16) (cid:17) \u03b8 \u2190 \u03b8 +\u03b1 \u2211 y (i)\u2212h (x (i)) x (i) j j \u03b8 j i=1 endfor endfor untilconvergence Bygroupingtheupdatesofthecoordinatesintoanupdateofthevector\u03b8,we canrewriteupdateinalgorithm1.3inaslightlymoresuccinctway: (cid:16) (cid:17) \u03b8 \u2190 \u03b8+\u03b1 y (i)\u2212h (i) x (i) (1.6) \u03b8 Inthisalgorithm,werepeatedlyrunthroughthetrainingset,andeachtime we encounter a training example, we update the parameters according to the gradientoftheerrorwithrespecttothatsingletrainingexampleonly.Thisalgo- rithmiscalledstochasticgradientdescent(alsoincrementalgradientdescent). Whereasbatchgradientdescenthastoscanthroughtheentiretrainingsetbefore takingasinglestep\u2014acostlyoperationifnislarge\u2014stochasticgradientdescent canstartmakingprogressrightaway,andcontinuestomakeprogresswitheach exampleitlooksat.Often,stochasticgradientdescentgets\u03b8\u2018\u2018close\u2019\u2019tothemini- mummuchfasterthanbatchgradientdescent.4Forthesereasons,particularly 4Note,however,thatitmaynever whenthetrainingsetislarge,stochasticgradien",
      "chunk_id": 14,
      "start_pos": 13298,
      "end_pos": 14498,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "sat.Often,stochasticgradientdescentgets\u03b8\u2018\u2018close\u2019\u2019tothemini- mummuchfasterthanbatchgradientdescent.4Forthesereasons,particularly 4Note,however,thatitmaynever whenthetrainingsetislarge,stochasticgradientdescentisoftenpreferredover \u2018\u2018converge\u2019\u2019totheminimum,and theparameters\u03b8willkeeposcillat- batchgradientdescent. ingaroundtheminimumofJ(\u03b8); butinpracticemostofthevalues neartheminimumwillbereason- 1.2 Thenormalequations ablygoodapproximationstothe trueminimum.Byslowlyletting thelearningrate\u03b1decreasetozero Gradientdescentgivesonewayofminimizing J.Let\u2019sdiscussasecondwayof asthealgorithmruns,itisalsopos- doingso,thistimeperformingtheminimizationexplicitlyandwithoutresorting sibletoensurethattheparameters toaniterativealgorithm.Inthismethod,wewillminimize Jbyexplicitlytaking will converge to the global mini- mumratherthanmerelyoscillate itsderivativeswithrespecttothe\u03b8 \u2019s,andsettingthemtozero.Toenableusto j aroundtheminimum. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 1.2. the normal equations 9 dothiswithouthavingtowritereamsofalgebraandpagesfullofmatricesof derivatives,let\u2019sintroducesomenotationfordoingcalculuswithmatrices.",
      "chunk_id": 15,
      "start_pos": 14298,
      "end_pos": 15459,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "sendcommentstomossr@cs.stanford.edu toc 1.2. the normal equations 9 dothiswithouthavingtowritereamsofalgebraandpagesfullofmatricesof derivatives,let\u2019sintroducesomenotationfordoingcalculuswithmatrices. 1.2.1 Matrixderivatives Forafunction f :Rn\u00d7d (cid:55)\u2192Rmappingfromn-by-dmatricestotherealnumbers, wedefinethederivativeof f withrespectto Atobe: \uf8ee \u2202f \u00b7\u00b7\u00b7 \u2202f \uf8f9 \u2202A11 \u2202A 1d \u2207 A f(A) = \uf8ef \uf8ef . . . ... . . . \uf8fa \uf8fa (1.7) \uf8f0 \uf8fb \u2202f \u00b7\u00b7\u00b7 \u2202f \u2202An1 \u2202And Thus,thegradient\u2207 f(A)isitselfann-by-dmatrix,whose(i,j)-elementis A \u2202f/\u2202A . ij (cid:34) (cid:35) Example1.3.Matrixderivative. A A Forexample,suppose A = 11 12 isa2-by-2matrix,andthefunction A A 21 22 f :R2\u00d72 (cid:55)\u2192Risgivenby 3 f(A) = A +5A2 +A A . 2 11 12 21 22 Here, A denotesthe(i,j)entryofthematrix A.Wethenhave: ij (cid:34) (cid:35) 3 10A \u2207 f(A) = 2 12 A A A 22 21 1.2.2 Leastsquaresrevisited Armedwiththetoolsofmatrixderivatives,letusnowproceedtofindinclosed- formthevalueof\u03b8thatminimizesJ(\u03b8).Webeginbyre-writingJinmatrix-vector notation. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 10 chapter 1.",
      "chunk_id": 16,
      "start_pos": 15259,
      "end_pos": 16327,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ives,letusnowproceedtofindinclosed- formthevalueof\u03b8thatminimizesJ(\u03b8).Webeginbyre-writingJinmatrix-vector notation. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 10 chapter 1. linear regression Givenatrainingset,definethedesignmatrixXtobethen-by-dmatrix(actually n-by-(d+1),ifweincludetheinterceptterm)thatcontainsthetrainingexamples\u2019 inputvaluesinitsrows: \uf8ee \u2014(x(1))(cid:62) \u2014 \uf8f9 \uf8ef\u2014(x(2))(cid:62) \u2014\uf8fa \uf8ef \uf8fa X= \uf8ef . \uf8fa (1.8) \uf8ef . \uf8fa . \uf8f0 \uf8fb \u2014(x(n))(cid:62) \u2014 Also,letybethen-dimensionalvectorcontainingallthetargetvaluesfromthe trainingset: \uf8ee y(1)\uf8f9 \uf8efy(2)\uf8fa \uf8ef \uf8fa y= \uf8ef . \uf8fa (1.9) \uf8ef . \uf8fa . \uf8f0 \uf8fb y(n) Now,sinceh (x(i)) = (x(i))(cid:62)\u03b8,wecaneasilyverifythat \u03b8 \uf8ee \uf8f9 \uf8ee \uf8f9 (x(1))(cid:62)\u03b8 y(1) \uf8ef . \uf8fa \uf8ef . \uf8fa X\u03b8\u2212y= \uf8ef . . \uf8fa\u2212\uf8ef . . \uf8fa \uf8f0 \uf8fb \uf8f0 \uf8fb (x(n))(cid:62)\u03b8 y(n) \uf8ee \uf8f9 h (x(1))\u2212y(1) \u03b8 \uf8ef . \uf8fa = \uf8ef . . \uf8fa. \uf8f0 \uf8fb h (x(n))\u2212y(n) \u03b8 Thus,usingthefactthatforavectorz,wehavethatz(cid:62)z = \u2211 z2: i i 1 (X\u03b8\u2212y)(cid:62)(X\u03b8\u2212y) = 1 \u2211 n (cid:16) h (x (i))\u2212y (i) (cid:17)2 \u03b8 2 2 i=1 = J(\u03b8) 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 1.3.",
      "chunk_id": 17,
      "start_pos": 16127,
      "end_pos": 17167,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "vectorz,wehavethatz(cid:62)z = \u2211 z2: i i 1 (X\u03b8\u2212y)(cid:62)(X\u03b8\u2212y) = 1 \u2211 n (cid:16) h (x (i))\u2212y (i) (cid:17)2 \u03b8 2 2 i=1 = J(\u03b8) 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 1.3. probabilistic interpretation 11 Finally,tominimize J,let\u2019sfinditsderivativewithrespectto\u03b8.Hence: 1 \u2207 J(\u03b8) = \u2207 (X\u03b8\u2212y)(cid:62)(X\u03b8\u2212y) \u03b8 \u03b8 2 1 (cid:16) (cid:17) = \u2207 (X\u03b8)(cid:62) X\u03b8\u2212(X\u03b8)(cid:62) y\u2212y (cid:62)(X\u03b8)+y (cid:62) y \u03b8 2 1 (cid:16) (cid:17) = \u2207 \u03b8 (cid:62)(X (cid:62) X)\u03b8\u2212y (cid:62)(X\u03b8)\u2212y (cid:62)(X\u03b8) (a(cid:62)b = b(cid:62)a) \u03b8 2 1 (cid:16) (cid:17) = \u2207 \u03b8 (cid:62)(X (cid:62) X)\u03b8\u22122(X (cid:62) y)(cid:62) \u03b8 \u03b8 2 1(cid:16) (cid:17) = 2X (cid:62) X\u03b8\u22122X (cid:62) y (\u2207 b(cid:62)x = band\u2207 x(cid:62)Ax =2Axforsym. A) x x 2 =X (cid:62) X\u03b8\u2212X (cid:62) y Tominimize J,wesetitsderivativestozero,andobtainthenormalequations: X (cid:62) X\u03b8 =X (cid:62) y (1.10) Thus,thevalueof\u03b8thatminimizesJ(\u03b8)isgiveninclosedformbytheequation:5 5Note that in the this step, we areimplicitlyassumingthatX(cid:62)X \u03b8 = (X (cid:62) X)\u22121X (cid:62) y (1.11) is an invertible matrix. This can becheckedbeforecalculatingthe inverse.",
      "chunk_id": 18,
      "start_pos": 16967,
      "end_pos": 18066,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "formbytheequation:5 5Note that in the this step, we areimplicitlyassumingthatX(cid:62)X \u03b8 = (X (cid:62) X)\u22121X (cid:62) y (1.11) is an invertible matrix. This can becheckedbeforecalculatingthe inverse. If either the number of 1.3 Probabilisticinterpretation linearlyindependentexamplesis fewerthanthenumberoffeatures, orifthefeaturesarenotlinearlyin- Whenfacedwitharegressionproblem,whymightlinearregression,andspecifi- dependent,thenX(cid:62)Xwillnotbe callywhymighttheleast-squarescostfunction J,beareasonablechoice?Inthis invertible.Eveninsuchcases,itis section,wewillgiveasetofprobabilisticassumptions,underwhichleast-squares possibleto\u2018\u2018fix\u2019\u2019thesituationwith additional techniques, which we regressionisderivedasaverynaturalalgorithm. skiphereforthesakeofsimplicty. Let us assume that the target variables and the inputs are related via the equation y (i) = \u03b8 (cid:62) x (i)+(cid:101) (i) , (1.12) where (cid:101)(i) is an error term that captures either unmodeled effects (such as if therearesomefeaturesverypertinenttopredictinghousingprice,butthatwe\u2019d leftoutoftheregression),orrandomnoise.Letusfurtherassumethatthe(cid:101)(i) aredistributedIID(independentlyandidenticallydistributed)accordin",
      "chunk_id": 19,
      "start_pos": 17866,
      "end_pos": 19066,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "efeaturesverypertinenttopredictinghousingprice,butthatwe\u2019d leftoutoftheregression),orrandomnoise.Letusfurtherassumethatthe(cid:101)(i) aredistributedIID(independentlyandidenticallydistributed)accordingtoa Gaussiandistribution(alsocalledaNormaldistribution)withmeanzeroand somevariance\u03c32.Wecanwritethisassumptionas(cid:101)(i) \u223c N(0,\u03c32),i.e.thedensity of(cid:101)(i) isgivenby (cid:32) (cid:33) 1 ((cid:101)(i))2 p((cid:101) (i)) = \u221a exp \u2212 . (1.13) 2\u03c0\u03c3 2\u03c32 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 12 chapter 1. linear regression Thisimpliesthat (cid:32) (cid:33) 1 (y(i)\u2212\u03b8(cid:62)x(i))2 p(y (i) | x (i) ;\u03b8) = \u221a exp \u2212 . (1.14) 2\u03c0\u03c3 2\u03c32 Thenotation\u2018\u2018p(y(i) | x(i);\u03b8)\u2019\u2019indicatesthatthisisthedistributionofy(i) given x(i)andparameterizedby\u03b8.Notethatweshouldnotconditionon\u03b8(i.e.\u2018\u2018p(y(i) | x(i),\u03b8)\u2019\u2019),since\u03b8isnotarandomvariable.Wecanalsowritethedistributionof y(i) as(y(i) | x(i);\u03b8) \u223c N(\u03b8(cid:62)x(i),\u03c32). GivenX(thedesignmatrix,whichcontainsallthex(i)\u2019s)and\u03b8,whatisthe distributionofthey(i)\u2019s?Theprobabilityofthedataisgivenby p(y|X;\u03b8).This quantityistypicallyviewedafunctionofy(andperhapsX),forafixedvalueof \u03b8.Whenwewishtoexplicitlyviewthisasafunctionof\u03b8,wewillinsteadcallit",
      "chunk_id": 20,
      "start_pos": 18866,
      "end_pos": 20066,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "butionofthey(i)\u2019s?Theprobabilityofthedataisgivenby p(y|X;\u03b8).This quantityistypicallyviewedafunctionofy(andperhapsX),forafixedvalueof \u03b8.Whenwewishtoexplicitlyviewthisasafunctionof\u03b8,wewillinsteadcallit thelikelihoodfunction: L(\u03b8) = L(\u03b8;X,y) = p(y|X;\u03b8) (1.15) Notethatbytheindependenceassumptiononthe(cid:101)(i)\u2019s(andhencealsothey(i)\u2019s giventhex(i)\u2019s),thiscanalsobewrittenas n L(\u03b8) = \u220f p(y (i) | x (i) ;\u03b8) (1.16) i=1 (cid:32) (cid:33) \u220f n 1 (y(i)\u2212\u03b8(cid:62)x(i))2 = \u221a exp \u2212 . (1.17) 2\u03c0\u03c3 2\u03c32 i=1 Now,giventhisprobabilisticmodelrelatingthe y(i)\u2019sandthe x(i)\u2019s,whatisa reasonablewayofchoosingourbestguessoftheparameters\u03b8?Theprincipal ofmaximumlikelihoodsaysthatweshouldchoose\u03b8soastomakethedataas highprobabilityaspossible\u2014i.e.weshouldchoose\u03b8tomaximizeL(\u03b8). Instead of maximizing L(\u03b8), we can also maximize any strictly increasing functionofL(\u03b8).Inparticular,thederivationswillbeabitsimplerifweinstead 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 1.4. locally weighted linear regression 13 maximizetheloglikelihood(cid:96)(\u03b8): (cid:96)(\u03b8) =logL(\u03b8) (cid:32) (cid:33) \u220f n 1 (y(i)\u2212\u03b8(cid:62)x(i))2 =log \u221a exp \u2212 2\u03c0\u03c3 2\u03c32 i=1 (cid:32) (cid:33) \u2211 n 1 (y(i)\u2212\u03b8(cid:62)x(i))2 = log\u221a exp \u2212 2\u03c0",
      "chunk_id": 21,
      "start_pos": 19866,
      "end_pos": 21066,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ession 13 maximizetheloglikelihood(cid:96)(\u03b8): (cid:96)(\u03b8) =logL(\u03b8) (cid:32) (cid:33) \u220f n 1 (y(i)\u2212\u03b8(cid:62)x(i))2 =log \u221a exp \u2212 2\u03c0\u03c3 2\u03c32 i=1 (cid:32) (cid:33) \u2211 n 1 (y(i)\u2212\u03b8(cid:62)x(i))2 = log\u221a exp \u2212 2\u03c0\u03c3 2\u03c32 i=1 = nlog\u221a 1 \u2212 1 \u00b7 1 \u2211 n (cid:16) y (i)\u2212\u03b8 (cid:62) x (i) (cid:17)2 2\u03c0\u03c3 \u03c32 2 i=1 Hence,maximizing(cid:96)(\u03b8)givesthesameanswerasminimizing 1 \u2211 n (cid:16) y (i)\u2212\u03b8 (cid:62) x (i) (cid:17)2 , 2 i=1 whichwerecognizetobe J(\u03b8),ouroriginalleast-squarescostfunction. Tosummarize. Underthepreviousprobabilisticassumptionsonthedata,least- squares regression corresponds to finding the maximum likelihood estimate of\u03b8.Thisisthusonesetofassumptionsunderwhichleast-squaresregression canbejustifiedasaverynaturalmethodthat\u2019sjustdoingmaximumlikelihood estimation.6 6Notehoweverthattheprobabilis- Notealsothat,inourpreviousdiscussion,ourfinalchoiceof\u03b8didnotdepend ticassumptionsarebynomeans necessaryforleast-squarestobea onwhatwas\u03c32,andindeedwe\u2019dhavearrivedatthesameresultevenif\u03c32were perfectlygoodandrationalproce- unknown.Wewillusethisfactagainlater,whenwetalkabouttheexponential dure,andtheremay\u2014andindeed thereare\u2014othernaturalassump- familyandgeneralizedlinearmodels. tionsthatcanalsobeusedtojustify it.",
      "chunk_id": 22,
      "start_pos": 20866,
      "end_pos": 22064,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "alproce- unknown.Wewillusethisfactagainlater,whenwetalkabouttheexponential dure,andtheremay\u2014andindeed thereare\u2014othernaturalassump- familyandgeneralizedlinearmodels. tionsthatcanalsobeusedtojustify it. 1.4 Locallyweightedlinearregression Consider the problem of predicting y from x \u2208 R. The leftmost figure below showstheresultoffittingay = \u03b8 +\u03b8 xtoadataset.Weseethatthedatadoesn\u2019t 0 1 reallylieonstraightline,andsothefitisnotverygood. Instead,ifwehadaddedanextrafeaturex2,andfity = \u03b8 +\u03b8 x+\u03b8 x2,then 0 1 2 weobtainaslightlybetterfittothedata.(Seemiddlefigure)Naively,itmight seemthatthemorefeaturesweadd,thebetter.However,thereisalsoadanger inaddingtoomanyfeatures:Therightmostfigureistheresultoffittinga5-th toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 14 chapter 1. linear regression 4 2 0 0 2 4 6 x y 1-storderpolynomial 4 2 0 0 2 4 6 x y 2-ndorderpolynomial 4 2 0 0 2 4 6 x y 5-thorderpolynomial Figure1.1. Polynomialregression withdifferentk-orderfits. orderpolynomialy = \u22115 \u03b8 xj.Weseethateventhoughthefittedcurvepasses j=0 j throughthedataperfectly,wewouldnotexpectthistobeaverygoodpredictor of,say,housingprices(y)fordifferentlivingareas(x).Withoutformallydefining wh",
      "chunk_id": 23,
      "start_pos": 21864,
      "end_pos": 23064,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "= \u22115 \u03b8 xj.Weseethateventhoughthefittedcurvepasses j=0 j throughthedataperfectly,wewouldnotexpectthistobeaverygoodpredictor of,say,housingprices(y)fordifferentlivingareas(x).Withoutformallydefining what these terms mean, we\u2019ll say the figure on the left shows an instance of underfitting\u2014in which the data clearly shows structure not captured by the model\u2014andthefigureontherightisanexampleofoverfitting.7 7Laterinthisclass,whenwetalk Asdiscussedpreviously,andasshowninfigure1.1,thechoiceoffeaturesis aboutlearningtheorywe\u2019llformal- izesomeofthesenotions,andalso importanttoensuringgoodperformanceofalearningalgorithm.(Whenwetalk definemorecarefullyjustwhatit aboutmodelselection,we\u2019llalsoseealgorithmsforautomaticallychoosingagood meansforahypothesistobegood orbad. setoffeatures.)Inthissection,letusbrieflytalkaboutthelocallyweightedlinear regression(LWR)algorithmwhich,assumingthereissufficienttrainingdata, makesthechoiceoffeatureslesscritical.Thistreatmentwillbebrief,sinceyou\u2019ll getachancetoexploresomeofthepropertiesoftheLWRalgorithmyourselfin thehomework. Intheoriginallinearregressionalgorithm,tomakeapredictionataquery pointx(i.e.toevaluateh(x)),wewould: (cid:16) (cid:17)2 1.",
      "chunk_id": 24,
      "start_pos": 22864,
      "end_pos": 24049,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "etachancetoexploresomeofthepropertiesoftheLWRalgorithmyourselfin thehomework. Intheoriginallinearregressionalgorithm,tomakeapredictionataquery pointx(i.e.toevaluateh(x)),wewould: (cid:16) (cid:17)2 1. Fit\u03b8tominimize\u2211 y(i)\u2212\u03b8(cid:62)x(i) . i 2. Output\u03b8(cid:62)x. Incontrast,thelocallyweightedlinearregressionalgorithmdoesthefollowing: (cid:16) (cid:17)2 1. Fit\u03b8tominimize\u2211 w(i) y(i)\u2212\u03b8(cid:62)x(i) . i 2. Output\u03b8(cid:62)x. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 1.4. locally weighted linear regression 15 Here,thew(i)\u2019sarenon-negativevaluedweights.Intuitively,ifw(i) islargefor aparticularvalueofi,theninpicking\u03b8 we\u2019lltryhardtomake(y(i)\u2212\u03b8(cid:62)x(i))2 small. If w(i) is small, then the (y(i)\u2212\u03b8(cid:62)x(i))2 error term will be pretty much ignoredinthefit. Afairlystandardchoicefortheweightsis:8 8Ifxisvector-valued,theweights w(i)canbegeneralizedto (cid:32) (cid:33) w (i) =exp \u2212 (x(i 2 ) \u03c4 \u2212 2 x)2 (1.18) exp (cid:32) \u2212 (x(i)\u2212x) 2 (cid:62) \u03c42 (x(i)\u2212x) (cid:33) or Notethattheweightsdependontheparticularpointxatwhichwe\u2019retryingto (cid:32) (cid:33) evaluatex.Moreover,if|x(i)\u2212x|issmall,thenw(i) iscloseto1;andif|x(i)\u2212x| exp \u2212 (x(i)\u2212x)(cid:62)\u03a3\u22121(x(i)\u2212x) 2\u03c42 islarge,the",
      "chunk_id": 25,
      "start_pos": 23849,
      "end_pos": 25049,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "thattheweightsdependontheparticularpointxatwhichwe\u2019retryingto (cid:32) (cid:33) evaluatex.Moreover,if|x(i)\u2212x|issmall,thenw(i) iscloseto1;andif|x(i)\u2212x| exp \u2212 (x(i)\u2212x)(cid:62)\u03a3\u22121(x(i)\u2212x) 2\u03c42 islarge,thenw(i) issmall.Hence,\u03b8ischosengivingamuchhigher\u2018\u2018weight\u2019\u2019to the(errorson)trainingexamplesclosetothequerypointx.9Theparameter\u03c4 forappropriatechoicesof\u03c4or\u03a3. 9Notealsothatwhiletheformula controlshowquicklytheweightofatrainingexamplefallsoffwithdistanceof fortheweightstakesaformthat itsx(i) fromthequerypointx;\u03c4iscalledthebandwidthparameter,andisalso iscosmeticallysimilartotheden- sityofaGaussiandistribution,the somethingthatyou\u2019llgettoexperimentwithinyourhomework. w(i)\u2019sdonotdirectlyhaveanything Locallyweightedlinearregressionisthefirstexamplewe\u2019reseeingofanon- todowithGaussians,andinpartic- parametricalgorithm.The(unweighted)linearregressionalgorithmthatwesaw ularthew(i)arenotrandomvari- ables,normallydistributedoroth- earlierisknownasaparametriclearningalgorithm,becauseithasafixed,finite erwise.",
      "chunk_id": 26,
      "start_pos": 24849,
      "end_pos": 25852,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "calgorithm.The(unweighted)linearregressionalgorithmthatwesaw ularthew(i)arenotrandomvari- ables,normallydistributedoroth- earlierisknownasaparametriclearningalgorithm,becauseithasafixed,finite erwise. numberofparameters(the\u03b8\u2019s),whicharefittothedata.Oncewe\u2019vefitthe\u03b8\u2019s i i andstoredthemaway,wenolongerneedtokeepthetrainingdataaroundto makefuturepredictions.Incontrast,tomakepredictionsusinglocallyweighted linearregression,weneedtokeeptheentiretrainingsetaround.Theterm\u2018\u2018non- parametric\u2019\u2019(roughly)referstothefactthattheamountofstuffweneedtokeep inordertorepresentthehypothesishgrowslinearlywiththesizeofthetraining set. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 2 Classification and Logistic Regression Let\u2019snowtalkabouttheclassificationproblem.Thisisjustliketheregression problem,exceptthatthevaluesywenowwanttopredicttakeononlyasmall numberofdiscretevalues.Fornow,wewillfocusonthebinaryclassification probleminwhichycantakeononlytwovalues,0and1.(Mostofwhatwesay herewillalsogeneralizetothemultiple-classcase.)Forinstance,ifwearetrying tobuildaspamclassifierforemail,thenx(i) maybesomefeaturesofapieceof email,andymaybe1ifitisapieceofspammail,and0otherwise.Theclass0is a",
      "chunk_id": 27,
      "start_pos": 25652,
      "end_pos": 26852,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "lalsogeneralizetothemultiple-classcase.)Forinstance,ifwearetrying tobuildaspamclassifierforemail,thenx(i) maybesomefeaturesofapieceof email,andymaybe1ifitisapieceofspammail,and0otherwise.Theclass0is alsocalledthenegativeclass,and1thepositiveclass,andtheyaresometimes alsodenotedbythesymbols\u2018\u2018\u2212\u2019\u2019and\u2018\u2018+\u2019\u2019.Givenx(i),thecorrespondingy(i) is alsocalledthelabelforthetrainingexample. 2.1 Logisticregression Wecouldapproachtheclassificationproblemignoringthefactthatyisdiscrete- valued,anduseouroldlinearregressionalgorithmtotrytopredictygivenx. However,itiseasytoconstructexampleswherethismethodperformsverypoorly. Intuitively, it also doesn\u2019tmake sense for h (x) to takevalueslargerthan 1 or \u03b8 smallerthan0whenweknowthaty \u2208 {0,1}. Tofixthis,let\u2019schangetheformforourhypothesesh (x).Wewillchoose \u03b8 1 h (x) = g(\u03b8 (cid:62) x) = \u03b8 1+e\u2212\u03b8(cid:62)x where 1 g(z) = 1+e\u2212z iscalledthelogisticfunctionorthesigmoidfunction.Hereisaplotshowing g(z): Notice that g(z) tends towards 1 as z \u2192 \u221e, and g(z) tends towards 0 as z \u2192 \u2212\u221e. Moreover, g(z), and hence also h(x), is always bounded between 0 and 1. As before, we are keeping the convention of letting x = 1, so that 0 \u03b8(cid:62)x = \u03b8 +\u2211d \u03b8 x . 0 j=1 j j 2.1.",
      "chunk_id": 28,
      "start_pos": 26652,
      "end_pos": 27842,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "owards 0 as z \u2192 \u2212\u221e. Moreover, g(z), and hence also h(x), is always bounded between 0 and 1. As before, we are keeping the convention of letting x = 1, so that 0 \u03b8(cid:62)x = \u03b8 +\u2211d \u03b8 x . 0 j=1 j j 2.1. logistic regression 17 Figure2.1. Sigmoidfunction(i.e. 1 logistic). 0.8 0.6 0.4 0.2 0 \u22126 \u22124 \u22122 0 2 4 6 For now, let\u2019s take the choice of g as given. Other functions that smoothly increasefrom0to1canalsobeused,butforacoupleofreasonsthatwe\u2019llsee later(whenwetalkaboutGLMs,andwhenwetalkaboutgenerativelearning algorithms), the choice of the logistic function is a fairly natural one. Before movingon,here\u2019sausefulpropertyofthederivativeofthesigmoidfunction, whichwewriteasg(cid:48): d 1 g (cid:48)(z) = (2.1) dz1+e\u2212z 1 = (e \u2212z) (2.2) (1+e\u2212z)2 (cid:18) (cid:19) 1 1 = \u00b7 1\u2212 (2.3) (1+e\u2212z) (1+e\u2212z) = g(z)(1\u2212g(z)) (2.4) So,giventhelogisticregressionmodel,howdowefit\u03b8forit?Followinghow wesawleastsquaresregressioncouldbederivedasthemaximumlikelihood estimatorunderasetofassumptions,let\u2019sendowourclassificationmodelwith a set of probabilistic assumptions, and then fit the parameters via maximum likelihood. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 18 chapter 2.",
      "chunk_id": 29,
      "start_pos": 27642,
      "end_pos": 28826,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ourclassificationmodelwith a set of probabilistic assumptions, and then fit the parameters via maximum likelihood. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 18 chapter 2. classification and logistic regression Letusassumethat P(y =1| x;\u03b8) = h (x) \u03b8 P(y =0| x;\u03b8) =1\u2212h (x) \u03b8 Notethatthiscanbewrittenmorecompactlyas p(y | x;\u03b8) = (h (x))y(1\u2212h (x))1\u2212y (2.5) \u03b8 \u03b8 Assumingthatthentrainingexamplesweregeneratedindependently,wecan thenwritedownthelikelihoodoftheparametersas L(\u03b8) = p(y| X;\u03b8) (2.6) n = \u220f p(y (i) | x (i) ;\u03b8) (2.7) i=1 = \u220f n (cid:16) h (x (i)) (cid:17)y(i)(cid:16) 1\u2212h (x (i)) (cid:17)1\u2212y(i) (2.8) \u03b8 \u03b8 i=1 Asbefore,itwillbeeasiertomaximizetheloglikelihood: (cid:96)(\u03b8) =logL(\u03b8) (2.9) n = \u2211 y (i) logh(x (i))+(1\u2212y (i))log(1\u2212h(x (i))) (2.10) i=1 Howdowemaximizethelikelihood?Similartoourderivationinthecaseof linearregression,wecanusegradientascent.Writteninvectorialnotation,our updateswillthereforebegivenby\u03b8 := \u03b8+\u03b1\u2207 (cid:96)(\u03b8).(Notethepositiverather \u03b8 thannegativesignintheupdateformula,sincewe\u2019remaximizing,ratherthan minimizing,afunctionnow.)Let\u2019sstartbyworkingwithjustonetrainingexample (x,y),andtakederivativestoderivethestochasticgradientascentrule: (cid:18",
      "chunk_id": 30,
      "start_pos": 28626,
      "end_pos": 29826,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "vesignintheupdateformula,sincewe\u2019remaximizing,ratherthan minimizing,afunctionnow.)Let\u2019sstartbyworkingwithjustonetrainingexample (x,y),andtakederivativestoderivethestochasticgradientascentrule: (cid:18) (cid:19) \u2202 1 1 \u2202 (cid:96)(\u03b8) = y \u2212(1\u2212y) g(\u03b8 (cid:62) x) (2.11) \u2202\u03b8 g(\u03b8(cid:62)x) 1\u2212g(\u03b8(cid:62)x) \u2202\u03b8 j j (cid:18) (cid:19) 1 1 \u2202 = y \u2212(1\u2212y) g(\u03b8 (cid:62) x)(1\u2212g(\u03b8 (cid:62) x)) \u03b8 (cid:62) x g(\u03b8(cid:62)x) 1\u2212g(\u03b8(cid:62)x) \u2202\u03b8 j (2.12) (cid:16) (cid:17) = y(1\u2212g(\u03b8 (cid:62) x))\u2212(1\u2212y)g(\u03b8 (cid:62) x) x (2.13) j = (y\u2212h (x))x (2.14) \u03b8 j 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 2.2. digression: the perceptron learning algorithm 19 Above,weusedthefactthatg(cid:48)(z) = g(z)(1\u2212g(z)).Thisthereforegivesusthe stochasticgradientascentrule (cid:16) (cid:17) \u03b8 := \u03b8 +\u03b1 y (i)\u2212h (x (i)) x (i) (2.15) j j \u03b8 j If we compare this to the LMS update rule, we see that it looks identical; but thisisnotthesamealgorithm,becauseh (x(i))isnowdefinedasanon-linear \u03b8 functionof\u03b8(cid:62)x(i).Nonetheless,it\u2019salittlesurprisingthatweendupwiththe sameupdateruleforaratherdifferentalgorithmandlearningproblem.Isthis coincidence,oristhereadeeperreasonbehindthis?We\u2019llanswerthiswhenwe gettoGLMmodels.",
      "chunk_id": 31,
      "start_pos": 29626,
      "end_pos": 30822,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "netheless,it\u2019salittlesurprisingthatweendupwiththe sameupdateruleforaratherdifferentalgorithmandlearningproblem.Isthis coincidence,oristhereadeeperreasonbehindthis?We\u2019llanswerthiswhenwe gettoGLMmodels. 2.2 Digression:Theperceptronlearningalgorithm Wenowdigresstotalkbrieflyaboutanalgorithmthat\u2019sofsomehistoricalinterest, andthatwewillalsoreturntolaterwhenwetalkaboutlearningtheory.Consider modifyingthelogisticregressionmethodto\u2018\u2018force\u2019\u2019ittooutputvaluesthatare either0or1orexactly.Todoso,itseemsnaturaltochangethedefinitionofgto bethethresholdfunction: \uf8f1 \uf8f21 ifz \u22650 g(z) = (2.16) \uf8f30 ifz <0 Ifwethenleth (x) = g(\u03b8(cid:62)x)asbeforebutusingthismodifieddefinitionofg, \u03b8 andifweusetheupdaterule (cid:16) (cid:17) \u03b8 := \u03b8 +\u03b1 y (i)\u2212h (x (i)) x (i) (2.17) j j \u03b8 j thenwehavetheperceptronlearningalgorithm. In the 1960s, this \u2018\u2018perceptron\u2019\u2019 was argued to be a rough model for how individual neurons in the brain work. Given how simple the algorithm is, it willalsoprovideastartingpointforouranalysiswhenwetalkaboutlearning theory later in this class.",
      "chunk_id": 32,
      "start_pos": 30622,
      "end_pos": 31661,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "d to be a rough model for how individual neurons in the brain work. Given how simple the algorithm is, it willalsoprovideastartingpointforouranalysiswhenwetalkaboutlearning theory later in this class. Note however that even though the perceptron may becosmeticallysimilartotheotheralgorithmswetalkedabout,itisactuallya verydifferenttypeofalgorithmthanlogisticregressionandleastsquareslinear regression;inparticular,itisdifficulttoendowtheperceptron\u2019spredic-tionswith meaningfulprobabilisticinterpretations,orderivetheperceptronasamaximum likelihoodestimationalgorithm. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 20 chapter 2. classification and logistic regression 2.3 Anotheralgorithmformaximizing (cid:96)(\u03b8) Returningtologisticregressionwithg(z)beingthesigmoidfunction,let\u2019snow talkaboutadifferentalgorithmformaximizing(cid:96)(\u03b8). Togetusstarted,let\u2019sconsiderNewton\u2019smethodforfindingazeroofafunction. Specifically,supposewehavesomefunction f : R (cid:55)\u2192 R,andwewishtofind avalueof\u03b8 sothat f(\u03b8) = 0.Here,\u03b8 \u2208 Risarealnumber.Newton\u2019smethod performsthefollowingupdate: f(\u03b8) \u03b8 := \u03b8\u2212 (2.18) f(cid:48)(\u03b8) Thismethodhasanaturalinterpretationinwhichwecanthinkofitasapproxi-",
      "chunk_id": 33,
      "start_pos": 31461,
      "end_pos": 32661,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "tofind avalueof\u03b8 sothat f(\u03b8) = 0.Here,\u03b8 \u2208 Risarealnumber.Newton\u2019smethod performsthefollowingupdate: f(\u03b8) \u03b8 := \u03b8\u2212 (2.18) f(cid:48)(\u03b8) Thismethodhasanaturalinterpretationinwhichwecanthinkofitasapproxi- matingthefunction f viaalinearfunctionthatistangentto f atthecurrentguess \u03b8,solvingforwherethatlinearfunctionequalstozero,andlettingthenextguess for\u03b8bewherethatlinearfunctioniszero. Here\u2019sapictureoftheNewton\u2019smethodinaction: 60 40 20 0 1 2 3 4 5 x )x(f 60 60 40 40 20 20 0 0 1 2 3 4 5 1 2 3 4 5 x x Figure2.2. Newton\u2019smethodfor Intheleftmostfigure,weseethefunctionfplottedalongwiththeliney =0. twosteps. We\u2019retryingtofind\u03b8sothat f(\u03b8) =0;thevalueof\u03b8thatachievesthisisabout 1.3.Supposeweinitializedthealgorithmwith\u03b8 = 4.5.Newton\u2019smethodthen fits a straight line tangent to f at \u03b8 = 4.5, and solves for the where that line evaluatesto0.(Middlefigure.)Thisgiveusthenextguessfor\u03b8,whichisabout 2.8.Therightmostfigureshowstheresultofrunningonemoreiteration,which the updates \u03b8 to about 1.8. After a few more iterations, we rapidly approach \u03b8 =1.3. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 2.3.",
      "chunk_id": 34,
      "start_pos": 32461,
      "end_pos": 33578,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "eresultofrunningonemoreiteration,which the updates \u03b8 to about 1.8. After a few more iterations, we rapidly approach \u03b8 =1.3. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 2.3. another algorithm for maximizing(cid:96)(\u03b8) 21 Newton\u2019smethodgivesawayofgettingto f(\u03b8) =0.Whatifwewanttouse ittomaximizesomefunction(cid:96)?Themaximaof(cid:96)correspondtopointswhere itsfirstderivative(cid:96)(cid:48)(\u03b8)iszero.So,byletting f(\u03b8) = (cid:96)(cid:48)(\u03b8),wecanusethesame algorithmtomaximize(cid:96),andweobtainupdaterule: (cid:96)(cid:48)(\u03b8) \u03b8 := \u03b8\u2212 . (2.19) (cid:96)(cid:48)(cid:48)(\u03b8) (Somethingtothinkabout:HowwouldthischangeifwewantedtouseNewton\u2019s methodtominimizeratherthanmaximizeafunction?) Lastly,inourlogisticregressionsetting,\u03b8isvector-valued,soweneedtogen- eralizeNewton\u2019smethodtothissetting.ThegeneralizationofNewton\u2019smethod tothismultidimensionalsetting(alsocalledtheNewton-Raphsonmethod)is givenby: \u03b8 := \u03b8\u2212H \u22121\u2207 (cid:96)(\u03b8). (2.20) \u03b8 Here, \u2207 (cid:96)(\u03b8)is,asusual,thevectorofpartialderivativesof (cid:96)(\u03b8)withrespect \u03b8 tothe\u03b8\u2019s;and Hisand-by-dmatrix(actually,d+1-by-d+1,assumingthatwe i includetheinterceptterm)calledtheHessian,whoseentriesaregivenby \u22022(cid:96)(\u03b8) Hij =",
      "chunk_id": 35,
      "start_pos": 33378,
      "end_pos": 34578,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "rofpartialderivativesof (cid:96)(\u03b8)withrespect \u03b8 tothe\u03b8\u2019s;and Hisand-by-dmatrix(actually,d+1-by-d+1,assumingthatwe i includetheinterceptterm)calledtheHessian,whoseentriesaregivenby \u22022(cid:96)(\u03b8) Hij = . (2.21) \u2202\u03b8 \u2202\u03b8 i j Newton\u2019smethodtypicallyenjoysfasterconvergencethan(batch)gradient descent,andrequiresmanyfeweriterationstogetveryclosetotheminimum. OneiterationofNewton\u2019scan,however,bemoreexpensivethanoneiterationof gradientdescent,sinceitrequiresfindingandinvertingand-by-dHessian;but solongas d isnottoolarge,itisusuallymuchfasteroverall.WhenNewton\u2019s methodisappliedtomaximizethelogisticregressionloglikelihoodfunction (cid:96)(\u03b8),theresultingmethodisalsocalledFisherscoring. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 3 Generalized Linear Models The presentation of the material So far, we\u2019ve seen a regression example, and a classification example. In the in this section takes inspiration regression example, we had y | x;\u03b8 \u223c N(\u00b5,\u03c32), and in the classification one, from Michael I.",
      "chunk_id": 36,
      "start_pos": 34378,
      "end_pos": 35397,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ar, we\u2019ve seen a regression example, and a classification example. In the in this section takes inspiration regression example, we had y | x;\u03b8 \u223c N(\u00b5,\u03c32), and in the classification one, from Michael I. Jordan, Learning in graphical models (unpublished y | x;\u03b8 \u223cBernoulli(\u03c6),forsomeappropriatedefinitionsof\u00b5and\u03c6asfunctions bookdraft),andalsoMcCullagh ofxand\u03b8.Inthissection,wewillshowthatbothofthesemethodsarespecial andNelder,GeneralizedLinearMod- casesofabroaderfamilyofmodels,calledGeneralizedLinearModels(GLMs).We els(2nded.). willalsoshowhowothermodelsintheGLMfamilycanbederivedandapplied tootherclassificationandregressionproblems. 3.1 Theexponentialfamily To work our way up to GLMs, we will begin by defining exponential family distributions.Wesaythataclassofdistributionsisintheexponentialfamilyifit canbewrittenintheform: p(y;\u03b7) = b(y)exp(\u03b7 (cid:62) T(y)\u2212a(\u03b7)) Here,\u03b7iscalledthenaturalparameter(alsocalledthecanonicalparameter)of thedistribution;T(y)isthesufficientstatistic(forthedistributionsweconsider, itwilloftenbethecasethat T(y) = y);and a(\u03b7) isthelogpartitionfunction.",
      "chunk_id": 37,
      "start_pos": 35197,
      "end_pos": 36280,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "henaturalparameter(alsocalledthecanonicalparameter)of thedistribution;T(y)isthesufficientstatistic(forthedistributionsweconsider, itwilloftenbethecasethat T(y) = y);and a(\u03b7) isthelogpartitionfunction. Thequantitye\u2212a(\u03b7) essentiallyplaystheroleofanormalizationconstant,that makessurethedistribution p(y;\u03b7)sums/integratesoveryto1. MLEw.r.t.\u03b7isconcave\u2192(neg.log- Afixedchoiceof T, aandbdefinesafamily(orset)ofdistributionsthatis likelihoodisconvex) parameterizedby\u03b7;aswevary\u03b7,wethengetdifferentdistributionswithinthis family. WenowshowthattheBernoulliandtheGaussiandistributionsareexamplesof exponentialfamilydistributions.TheBernoullidistributionwithmean\u03c6,written Bernoulli(\u03c6), specifies a distribution over y \u2208 {0,1}, so that p(y = 1;\u03c6) = 3.1. the exponential family 23 \u03c6;p(y = 0;\u03c6) = 1\u2212\u03c6. As we vary \u03c6, we obtain Bernoulli distributions with different means. We now show that this class of Bernoulli distributions, ones obtainedbyvarying\u03c6,isintheexponentialfamily;i.e.,thatthereisachoiceofT, aandbsothat3.1becomesexactlytheclassofBernoullidistributions.",
      "chunk_id": 38,
      "start_pos": 36080,
      "end_pos": 37131,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "eans. We now show that this class of Bernoulli distributions, ones obtainedbyvarying\u03c6,isintheexponentialfamily;i.e.,thatthereisachoiceofT, aandbsothat3.1becomesexactlytheclassofBernoullidistributions. WewritetheBernoullidistributionas: p(y;\u03c6) = \u03c6y(1\u2212\u03c6)1\u2212y (3.1) =exp(ylog\u03c6+(1\u2212y)log(1\u2212\u03c6)) (3.2) (cid:18) (cid:18) (cid:18) (cid:19)(cid:19) (cid:19) \u03c6 =exp y log +log(1\u2212\u03c6) . (3.3) 1\u2212\u03c6 Thus,thenaturalparameterisgivenby\u03b7 =log(\u03c6/(1\u2212\u03c6)).Interestingly,ifwe invertthisdefinitionfor\u03b7 bysolvingfor\u03c6intermsof\u03b7,weobtain\u03c6 = 1/(1+ e\u2212\u03b7).Thisisthefamiliarsigmoidfunction!Thiswillcomeupagainwhenwe derivelogisticregressionasaGLM.TocompletetheformulationoftheBernoulli distributionasanexponentialfamilydistribution,wealsohave: T(y) = y a(\u03b7) = \u2212log(1\u2212\u03c6) =log(1+e\u03b7) b(y) =1 ThisshowsthattheBernoullidistributioncanbewrittenintheformof3.1,using anappropriatechoiceofT,aandb. Let\u2019snowmoveontoconsidertheGaussiandistribution.Recallthat,when derivinglinearregression,thevalueof \u03c32 hadnoeffectonourfinalchoiceof \u03b8 and h (x). Thus, we can choose an arbitrary value for \u03c32 without changing \u03b8 anything.Tosimplifythederivationbelow,let\u2019sset\u03c32 =1.1Wethenhave: 1If we leave \u03c32 as a variable, the Gaussian distribution can (cid:18)",
      "chunk_id": 39,
      "start_pos": 36931,
      "end_pos": 38131,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "Thus, we can choose an arbitrary value for \u03c32 without changing \u03b8 anything.Tosimplifythederivationbelow,let\u2019sset\u03c32 =1.1Wethenhave: 1If we leave \u03c32 as a variable, the Gaussian distribution can (cid:18) (cid:19) 1 1 alsobeshowntobeintheexpo- p(y;\u00b5) = \u221a exp \u2212 (y\u2212\u00b5)2 (3.4) nential family, where \u03b7 \u2208 R2 is 2\u03c0 2 now a 2-dimension vector that (cid:18) (cid:19) (cid:18) (cid:19) = \u221a 1 exp \u2212 1 y2 \u00b7exp \u00b5y\u2212 1 \u00b52 (3.5) depends on both \u00b5 and \u03c3. For 2\u03c0 2 2 the purposes of GLMs, however, the \u03c32 parameter can also be treated by considering a more general definition of the expo- nential family: p(y;\u03b7,\u03c4) = b(a,\u03c4)exp((\u03b7(cid:62)T(y) \u2212 a(\u03b7))/c(\u03c4)).Here,\u03c4iscalledthe dispersionparameter,andforthe toc 2021-05-2300:18:27-07:00,draft: sendcommentGsatuossmiaons,scr ( @\u03c4) cs = .st\u03c3a 2 n;fbourtdg.ievdenu oursimplificationabove,wewon\u2019t needthemoregeneraldefinition fortheexampleswewillconsider here. 24 chapter 3. generalized linear models Thus,weseethattheGaussianisintheexponentialfamily,with \u03b7 = \u00b5 T(y) = y a(\u03b7) = \u00b52/2 = \u03b72/2 \u221a b(y) = (1/ 2\u03c0)exp(\u2212y2/2).",
      "chunk_id": 40,
      "start_pos": 37931,
      "end_pos": 38967,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "finition fortheexampleswewillconsider here. 24 chapter 3. generalized linear models Thus,weseethattheGaussianisintheexponentialfamily,with \u03b7 = \u00b5 T(y) = y a(\u03b7) = \u00b52/2 = \u03b72/2 \u221a b(y) = (1/ 2\u03c0)exp(\u2212y2/2). There\u2019remanyotherdistributionsthataremembersoftheexponentialfamily:The multinomial(whichwe\u2019llseelater),thePoisson(formodellingcount-data;also seetheproblemset);thegammaandtheexponential(formodellingcontinuous, non-negativerandomvariables,suchastime-intervals);thebetaandtheDirichlet (fordistributionsoverprobabilities);andmanymore.Inthenextsection,wewill describeageneral\u2018\u2018recipe\u2019\u2019forconstructingmodelsinwhichy(givenxand\u03b8) comesfromanyofthesedistributions. 3.2 ConstructingGLMs Inferenceiseasy: Supposeyouwouldliketobuildamodeltoestimatethenumberyofcustomers \u2202 arrivinginyourstore(ornumberofpage-viewsonyourwebsite)inanygiven E[y;\u03b7]= a(\u03b7) \u2202\u03b7 hour,basedoncertainfeatures xsuchasstorepromotions,recentadvertising, (logpartitionofexp.family). weather,day-of-week,etc.WeknowthatthePoissondistributionusuallygivesa goodmodelfornumbersofvisitors.Knowingthis,howcanwecomeupwitha modelforourproblem?Fortunately,thePoissonisanexponentialfamilydistri- bution,sowecanapplyaGeneralizedLinearModel(GLM).Inthissec",
      "chunk_id": 41,
      "start_pos": 38767,
      "end_pos": 39967,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "allygivesa goodmodelfornumbersofvisitors.Knowingthis,howcanwecomeupwitha modelforourproblem?Fortunately,thePoissonisanexponentialfamilydistri- bution,sowecanapplyaGeneralizedLinearModel(GLM).Inthissection,we willwewilldescribeamethodforconstructingGLMmodelsforproblemssuch asthese. Moregenerally,consideraclassificationorregressionproblemwherewewould liketopredictthevalueofsomerandomvariableyasafunctionofx.Toderivea GLMforthisproblem,wewillmakethefollowingthreeassumptionsaboutthe conditionaldistributionofygivenxandaboutourmodel: 1. y | x;\u03b8 \u223c ExponentialFamily(\u03b7). I.e., given x and \u03b8, the distribution of y followssomeexponentialfamilydistribution,withparameter\u03b7. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 3.2. constructing glms 25 2. Given x, our goal is to predict the expected value of T(y) given x. In most of our examples, we will have T(y) = y, so this means we would like the predictionh(x)outputbyourlearnedhypothesishtosatisfyh(x) =E[y | x]. (Notethatthisassumptionissatisfiedinthechoicesforh (x)forbothlogistic \u03b8 regressionandlinearregression.Forinstance,inlogisticregression,wehad h (x) = p(y =1| x;\u03b8) =0\u00b7p(y =0| x;\u03b8)+1\u00b7p(y =1| x;\u03b8) =E[y | x;\u03b8].) \u03b8 3.",
      "chunk_id": 42,
      "start_pos": 39767,
      "end_pos": 40963,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "isassumptionissatisfiedinthechoicesforh (x)forbothlogistic \u03b8 regressionandlinearregression.Forinstance,inlogisticregression,wehad h (x) = p(y =1| x;\u03b8) =0\u00b7p(y =0| x;\u03b8)+1\u00b7p(y =1| x;\u03b8) =E[y | x;\u03b8].) \u03b8 3. Thenaturalparameter\u03b7andtheinputsxarerelatedlinearly:\u03b7 = \u03b8(cid:62)x.(Or,if \u03b7isvector-valued,then\u03b7 = \u03b8(cid:62)x.) i i Thethirdoftheseassumptionsmightseemtheleastwelljustifiedoftheabove, anditmightbebetterthoughtofasa\u2018\u2018designchoice\u2019\u2019inourrecipefordesigning GLMs, rather than as an assumption per se. These three assumptions/design choiceswillallowustoderiveaveryelegantclassoflearningalgorithms,namely GLMs,thathavemanydesirablepropertiessuchaseaseoflearning.Furthermore, the resulting models are often very effective for modelling different types of distributionsovery;forexample,wewillshortlyshowthatbothlogisticregression andordinaryleastsquarescanbothbederivedasGLMs. 3.2.1 OrdinaryLeastSquares ToshowthatordinaryleastsquaresisaspecialcaseoftheGLMfamilyofmodels, considerthesettingwherethetargetvariabley(alsocalledtheresponsevariable inGLMterminology)iscontinuous,andwemodeltheconditionaldistribution of y given x as a Gaussian N(\u00b5,\u03c32).",
      "chunk_id": 43,
      "start_pos": 40763,
      "end_pos": 41902,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "seoftheGLMfamilyofmodels, considerthesettingwherethetargetvariabley(alsocalledtheresponsevariable inGLMterminology)iscontinuous,andwemodeltheconditionaldistribution of y given x as a Gaussian N(\u00b5,\u03c32). (Here, \u00b5 may depend x.) So, we let the ExponentialFamily(\u03b7) distribution above be the Gaussian distribution. As we saw previously, in the formulation of the Gaussian as an exponential family distribution,wehad\u00b5 = \u03b7.So,wehave h (x) =E[y | x;\u03b8] \u03b8 = \u00b5 = \u03b7 = \u03b8 (cid:62) x. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 26 chapter 3. generalized linear models ThefirstequalityfollowsfromAssumption2,above;thesecondequalityfollows fromthefactthaty | x;\u03b8 \u223c N(\u00b5,\u03c32),andsoitsexpectedvalueisgivenby\u00b5;the thirdequalityfollowsfromAssumption1(andourearlierderivationshowingthat \u00b5 = \u03b7intheformulationoftheGaussianasanexponentialfamilydistribution); andthelastequalityfollowsfromAssumption3. 3.2.2 LogisticRegression Wenowconsiderlogisticregression.Hereweareinterestedinbinaryclassification, soy \u2208 {0,1}.Giventhatyisbinary-valued,itthereforeseemsnaturaltochoose the Bernoulli family of distributions to model the conditional distribution of y given x.",
      "chunk_id": 44,
      "start_pos": 41702,
      "end_pos": 42865,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "reweareinterestedinbinaryclassification, soy \u2208 {0,1}.Giventhatyisbinary-valued,itthereforeseemsnaturaltochoose the Bernoulli family of distributions to model the conditional distribution of y given x. In our formulation of the Bernoulli distribution as an exponential familydistribution,wehad\u03c6 =1/(1+e\u2212\u03b7).Furthermore,notethatify | x;\u03b8 \u223c Bernoulli(\u03c6),thenE[y | x;\u03b8] = \u03c6.So,followingasimilarderivationastheone forordinaryleastsquares,weget: h (x) =E[y | x;\u03b8] \u03b8 = \u03c6 =1/(1+e \u2212\u03b7) =1/(1+e \u2212\u03b8(cid:62)x) So,thisgivesushypothesisfunctionsoftheformh (x) =1/(1+e\u2212\u03b8(cid:62)x).Ifyou \u03b8 arepreviouslywonderinghowwecameupwiththeformofthelogisticfunction 1/(1+e\u2212z),thisgivesoneanswer:Onceweassumethatyconditionedonxis Bernoulli,itarisesasaconsequenceofthedefinitionofGLMsandexponential familydistributions. Tointroducealittlemoreterminology,thefunctionggivingthedistribution\u2019s meanasafunctionofthenaturalparameter(g(\u03b7) = E[T(y);\u03b7])iscalledthe canonicalresponsefunction.Itsinverse,g\u22121,iscalledthecanonicallinkfunc- tion.Thus,thecanonicalresponsefunctionfortheGaussianfamilyisjustthe identityfunction;andthecanonicalresponsefunctionfortheBernoulliisthe logisticfunction.2 2Many texts use g to denote the linkfunction,an",
      "chunk_id": 45,
      "start_pos": 42665,
      "end_pos": 43865,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "n.Thus,thecanonicalresponsefunctionfortheGaussianfamilyisjustthe identityfunction;andthecanonicalresponsefunctionfortheBernoulliisthe logisticfunction.2 2Many texts use g to denote the linkfunction,andg\u22121todenotethe responsefunction;butthenotation 3.2.3 SoftmaxRegression we\u2019re using here, inherited from theearlymachinelearninglitera- Let\u2019slookatonemoreexampleofaGLM.Consideraclassificationproblemin ture,willbemoreconsistentwith whichtheresponsevariableycantakeonanyoneofkvalues,soy \u2208 {1,2,...,k}. thenotationusedintherestofthe class. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 3.2. constructing glms 27 For example, rather than classifying email into the two classes spam or not- spam\u2014whichwouldhavebeenabinaryclassificationproblem\u2014wemightwant toclassifyitintothreeclasses,suchasspam,personalmail,andwork-relatedmail. Theresponsevariableisstilldiscrete,butcannowtakeonmorethantwovalues. Wewillthusmodelitasdistributedaccordingtoamultinomialdistribution. Let\u2019sderiveaGLMformodellingthistypeofmultinomialdata.Todoso,we willbeginbyexpressingthemultinomialasanexponentialfamilydistribution.",
      "chunk_id": 46,
      "start_pos": 43665,
      "end_pos": 44783,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "Wewillthusmodelitasdistributedaccordingtoamultinomialdistribution. Let\u2019sderiveaGLMformodellingthistypeofmultinomialdata.Todoso,we willbeginbyexpressingthemultinomialasanexponentialfamilydistribution. To parameterize a multinomial over k possible outcomes, one could use k parameters\u03c6 ,...,\u03c6 specifyingtheprobabilityofeachoftheoutcomes.However, 1 k these parameters would be redundant, or more formally, they would not be independent(sinceknowinganyk\u22121ofthe\u03c6\u2019suniquelydeterminesthelast i one, as they must satisfy \u2211k \u03c6 = 1). So, we will instead parameterize the i=1 i multinomialwithonly k\u22121parameters, \u03c6 1 ,...,\u03c6 k\u22121 ,where \u03c6 i = p(y = i;\u03c6), and p(y = k;\u03c6) = 1\u2212\u2211k\u22121\u03c6. For notational convenience, we will also let i=1 i \u03c6 =1\u2212\u2211k\u22121\u03c6,butweshouldkeepinmindthatthisisnotaparameter,and k i=1 i thatitisfullyspecifiedby\u03c6 1 ,...,\u03c6 k\u22121 . Toexpressthemultinomialasanexponentialfamilydistribution,wewilldefine T(y) \u2208Rk\u22121asfollows: \uf8ee \uf8f9 \uf8ee \uf8f9 \uf8ee \uf8f9 \uf8ee \uf8f9 1 0 0 0 \uf8ef0\uf8fa \uf8ef1\uf8fa \uf8ef0\uf8fa \uf8ef0\uf8fa \uf8ef \uf8fa \uf8ef \uf8fa \uf8ef \uf8fa \uf8ef \uf8fa T(1) = \uf8ef \uf8ef 0\uf8fa \uf8fa,T(2) = \uf8ef \uf8ef 0\uf8fa \uf8fa,\u00b7\u00b7\u00b7 ,T(k\u22121) = \uf8ef \uf8ef 0\uf8fa \uf8fa,T(k) = \uf8ef \uf8ef 0\uf8fa \uf8fa, \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8f0.\uf8fb \uf8f0.\uf8fb \uf8f0.\uf8fb \uf8f0.\uf8fb 0 0 1 0 Unlikeourpreviousexamples,herewedonothaveT(y) = y;also,T(y)isnow ak\u22121dimensionalvector,rathertha",
      "chunk_id": 47,
      "start_pos": 44583,
      "end_pos": 45783,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\uf8ef 0\uf8fa \uf8fa,\u00b7\u00b7\u00b7 ,T(k\u22121) = \uf8ef \uf8ef 0\uf8fa \uf8fa,T(k) = \uf8ef \uf8ef 0\uf8fa \uf8fa, \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8ef.\uf8fa \uf8f0.\uf8fb \uf8f0.\uf8fb \uf8f0.\uf8fb \uf8f0.\uf8fb 0 0 1 0 Unlikeourpreviousexamples,herewedonothaveT(y) = y;also,T(y)isnow ak\u22121dimensionalvector,ratherthanarealnumber.Wewillwrite(T(y)) to i denotethei-thelementofthevectorT(y).Weintroduceonemoreveryuseful pieceofnotation.Anindicatorfunction1{\u00b7}takesonavalueof1ifitsargumentis true,and0otherwise(1{True} =1,1{False} =0).Forexample,1{2=3} =0, and1{3=5\u22122} =1.So,wecanalsowritetherelationshipbetweenT(y)and y as (T(y)) = 1{y = i}.(Beforeyoucontinuereading,pleasemakesureyou i understandwhythisistrue!)Further,wehavethatE[(T(y)) ] = P(y =i) = \u03c6. i i toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 28 chapter 3. generalized linear models Wearenowreadytoshowthatthemultinomialisamemberoftheexponential family.Wehave: p(y;\u03c6) = \u03c6 1{y=1} \u03c6 1{y=2} \u00b7\u00b7\u00b7\u03c6 1{y=k} 1 2 k = \u03c6 1{y=1} \u03c6 1{y=2} \u00b7\u00b7\u00b7\u03c6 1\u2212\u2211 i k = \u2212 1 11{y=i} 1 2 k = \u03c6 (T(y)) 1\u03c6 (T(y)) 2\u00b7\u00b7\u00b7\u03c6 1\u2212\u2211 i k = \u2212 1 1(T(y)) i 1 2 k (cid:32) (cid:32) (cid:33) (cid:33) k\u22121 \u2211 =exp (T(y)) log(\u03c6 )+(T(y)) log(\u03c6 )+\u00b7\u00b7\u00b7+ 1\u2212 (T(y)) log(\u03c6 ) i 1 2 2 i k i=1 =exp((T(y)) i log(\u03c6 1 /\u03c6 k )+(T(y)) 2 log(\u03c6 2 /\u03c6 k )+\u00b7\u00b7\u00b7+(T(k)) k\u22121 log(\u03c6 k\u22121 /\u03c6 k )+log(\u03c6 k )) =",
      "chunk_id": 48,
      "start_pos": 45583,
      "end_pos": 46783,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "32) (cid:33) (cid:33) k\u22121 \u2211 =exp (T(y)) log(\u03c6 )+(T(y)) log(\u03c6 )+\u00b7\u00b7\u00b7+ 1\u2212 (T(y)) log(\u03c6 ) i 1 2 2 i k i=1 =exp((T(y)) i log(\u03c6 1 /\u03c6 k )+(T(y)) 2 log(\u03c6 2 /\u03c6 k )+\u00b7\u00b7\u00b7+(T(k)) k\u22121 log(\u03c6 k\u22121 /\u03c6 k )+log(\u03c6 k )) = b(y)exp(\u03b7 (cid:62) T(y)\u2212a(\u03b7)) where \uf8ee \uf8f9 log(\u03c6 /\u03c6 ) 1 k \uf8ef log(\u03c6 /\u03c6 ) \uf8fa \uf8ef 2 k \uf8fa \u03b7 = \uf8ef . \uf8fa, \uf8ef . \uf8fa . \uf8f0 \uf8fb log(\u03c6 k\u22121 /\u03c6 k ) a(\u03b7) = \u2212log(\u03c6 ) k b(y) =1. This completes our formulation of the multinomial as an exponential family distribution. Thelinkfunctionisgiven(fori =1,...,k)by: \u03c6 \u03b7 =log i i \u03c6 k Forconvenience,wehavealsodefined\u03b7 = log(\u03c6 /\u03c6 ) = 0.Toinvertthelink k k k functionandderivetheresponsefunction,wethereforehavethat \u03c6 e\u03b7i = i (3.6) \u03c6 k \u03c6 e\u03b7i = \u03c6 (3.7) k i k k \u03c6 \u2211 e\u03b7i = \u2211 \u03c6 =1 (3.8) k i i=1 i=1 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 3.2. constructing glms 29 This implies that \u03c6 k = 1/\u2211 i k =1 e\u03b7i, which can be substituted back into equa- tion(3.7)togivetheresponsefunction e\u03b7i \u03c6 = i \u2211k e\u03b7j j=1 Thisfunctionmappingfromthe\u03b7\u2019stothe\u03c6\u2019siscalledthesoftmaxfunction. To complete our model, we use Assumption 3, given earlier, that the \u03b7\u2019s i are linearly related to the x\u2019s.",
      "chunk_id": 49,
      "start_pos": 46583,
      "end_pos": 47690,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "unction e\u03b7i \u03c6 = i \u2211k e\u03b7j j=1 Thisfunctionmappingfromthe\u03b7\u2019stothe\u03c6\u2019siscalledthesoftmaxfunction. To complete our model, we use Assumption 3, given earlier, that the \u03b7\u2019s i are linearly related to the x\u2019s. So, have \u03b7 = \u03b8(cid:62)x (for i = 1,...,k\u22121), where i i \u03b8 1 ,...,\u03b8 k\u22121 \u2208Rd+1aretheparametersofourmodel.Fornotationalconvenience, wecanalsodefine\u03b8 =0,sothat\u03b7 = \u03b8(cid:62)x =0,asgivenpreviously.Hence,our k k k modelassumesthattheconditionaldistributionofygivenxisgivenby: p(y =1| x;\u03b8) = \u03c6 (3.9) i e\u03b7i = (3.10) \u2211k e\u03b7j j=1 e\u03b8 i (cid:62)x = (3.11) \u2211k e \u03b8 j (cid:62)x j=1 Thismodel,whichappliestoclassificationproblemswherey \u2208 {1,...,k},iscalled softmaxregression.Itisageneralizationoflogisticregression. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 30 chapter 3. generalized linear models Ourhypothesiswilloutput: h (x) =E[T(y) | x;\u03b8] (3.12) \u03b8 \uf8ee 1{y =1} \uf8f9 \uf8ef 1{y =2} \uf8fa =E\uf8ef \uf8ef . | x;\u03b8 \uf8fa \uf8fa (3.13) \uf8ef . \uf8fa . \uf8f0 \uf8fb 1{y = k\u22121} \uf8ee \uf8f9 \u03c6 1 \uf8ef \u03c6 \uf8fa \uf8ef 2 \uf8fa = \uf8ef . \uf8fa (3.14) \uf8ef . \uf8fa . \uf8f0 \uf8fb \u03c6 k\u22121 \uf8ee exp(\u03b8(cid:62)x) \uf8f9 1 \uf8ef \u2211k j=1 exp(\u03b8 j (cid:62)x) \uf8fa \uf8ef exp(\u03b8(cid:62)x) \uf8fa \uf8ef 2 \uf8fa = \uf8ef \uf8ef \u2211k j=1 exp(\u03b8 j (cid:62)x)\uf8fa \uf8fa (3.15) \uf8ef . \uf8fa \uf8ef . .",
      "chunk_id": 50,
      "start_pos": 47490,
      "end_pos": 48611,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\uf8f0 \uf8fb 1{y = k\u22121} \uf8ee \uf8f9 \u03c6 1 \uf8ef \u03c6 \uf8fa \uf8ef 2 \uf8fa = \uf8ef . \uf8fa (3.14) \uf8ef . \uf8fa . \uf8f0 \uf8fb \u03c6 k\u22121 \uf8ee exp(\u03b8(cid:62)x) \uf8f9 1 \uf8ef \u2211k j=1 exp(\u03b8 j (cid:62)x) \uf8fa \uf8ef exp(\u03b8(cid:62)x) \uf8fa \uf8ef 2 \uf8fa = \uf8ef \uf8ef \u2211k j=1 exp(\u03b8 j (cid:62)x)\uf8fa \uf8fa (3.15) \uf8ef . \uf8fa \uf8ef . . \uf8fa \uf8ef \uf8fa \uf8f0 exp(\u03b8(cid:62) x) \uf8fb k\u22121 \u2211k j=1 exp(\u03b8 j (cid:62)x) Inotherwords,ourhypothesiswilloutputtheestimatedprobabilitythat p(y = i | x;\u03b8),foreveryvalueofi = 1,...,k.(Eventhoughh (x)asdefinedaboveis \u03b8 onlyk\u22121dimensional,clearly p(y = k | x;\u03b8)canbeobtainedas1\u2212\u2211k\u22121\u03c6.) i=1 i Lastly, let\u2019s discuss parameter fitting. Similar to our original derivation of ordinary least squares and logistic regression, if we have a training set of n examples{(x(i),y(i));i =1,...,n}andwouldliketolearntheparameters\u03b8 of i thismodel,wewouldbeginbywritingdownthelog-likelihood n (cid:96)(\u03b8) = \u2211 logp(y (i) | x (i) ;\u03b8) (3.16) i=1 \uf8eb \uf8f61{y(i)=l} \u2211 n \u220f k e\u03b8 l (cid:62)x(i) = log \uf8ed \uf8f8 (3.17) i=1 l=1 \u2211k j=1 e \u03b8 j (cid:62)x(i) Toobtainthesecondlineabove,weusedthedefinitionfor p(y | x;\u03b8)givenin 3.11. We can now obtain the maximum likelihood estimate of the parameters by maximizing (cid:96)(\u03b8) in terms of \u03b8, using a method such as gradient ascent or Newton\u2019smethod.",
      "chunk_id": 51,
      "start_pos": 48411,
      "end_pos": 49546,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "nitionfor p(y | x;\u03b8)givenin 3.11. We can now obtain the maximum likelihood estimate of the parameters by maximizing (cid:96)(\u03b8) in terms of \u03b8, using a method such as gradient ascent or Newton\u2019smethod. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc Part II: Generative Learning Algorithms FromCS229Spring2021,Andrew Sofar,we\u2019vemainlybeentalkingaboutlearningalgorithmsthatmodelp(y | x;\u03b8), Ng,MosesCharikar,&Christopher theconditionaldistributionofygivenx.Forinstance,logisticregressionmodeled R\u00e9,StanfordUniversity. p(y | x;\u03b8) as h (x) = g(\u03b8(cid:62)x) where g is the sigmoid function. In these notes, \u03b8 we\u2019lltalkaboutadifferenttypeoflearningalgorithm. Consideraclassificationprobleminwhichwewanttolearntodistinguish between elephants (y = 1) and dogs (y = 0), based on some features of an animal.Givenatrainingset,analgorithmlikelogisticregressionortheperceptron algorithm(basically)triestofindastraightline\u2014thatis,adecisionboundary\u2014 thatseparatestheelephantsanddogs.Then,toclassifyanewanimalaseitheran elephantoradog,itchecksonwhichsideofthedecisionboundaryitfalls,and makesitspredictionaccordingly.",
      "chunk_id": 52,
      "start_pos": 49346,
      "end_pos": 50469,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ghtline\u2014thatis,adecisionboundary\u2014 thatseparatestheelephantsanddogs.Then,toclassifyanewanimalaseitheran elephantoradog,itchecksonwhichsideofthedecisionboundaryitfalls,and makesitspredictionaccordingly. Here\u2019sadifferentapproach.First,lookingatelephants,wecanbuildamodel ofwhatelephantslooklike.Then,lookingatdogs,wecanbuildaseparatemodel ofwhatdogslooklike.Finally,toclassifyanewanimal,wecanmatchthenew animalagainsttheelephantmodel,andmatchitagainstthedogmodel,tosee whetherthenewanimallooksmoreliketheelephantsormorelikethedogswe hadseeninthetrainingset. Algorithms that try to learn p(y | x) directly (such as logistic regression), oralgorithmsthattrytolearnmappingsdirectlyfromthespaceofinputsX to thelabels{0,1},(suchastheperceptronalgorithm)arecalleddiscriminative learningalgorithms.Here,we\u2019lltalkaboutalgorithmsthatinsteadtrytomodel p(x | y)(and p(y)).Thesealgorithmsarecalledgenerativelearningalgorithms. Forinstance,ifyindicateswhetheranexampleisadog(0)oranelephant(1), then p(x | y = 0) modelsthedistributionofdogs\u2019features,and p(x | y = 1) modelsthedistributionofelephants\u2019features.",
      "chunk_id": 53,
      "start_pos": 50269,
      "end_pos": 51361,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "learningalgorithms. Forinstance,ifyindicateswhetheranexampleisadog(0)oranelephant(1), then p(x | y = 0) modelsthedistributionofdogs\u2019features,and p(x | y = 1) modelsthedistributionofelephants\u2019features. Aftermodeling p(y)(calledtheclasspriors)and p(x | y),ouralgorithmcan thenuseBayesruletoderivetheposteriordistributiononygivenx: p(x | y)p(y) p(y | x) = (3.18) p(x) Here, the denominator is given by p(x) = p(x | y = 1)p(y = 1)+p(x | y = 0)p(y =0)(youshouldbeabletoverifythatthisistruefromthestandardprop- ertiesofprobabilities),andthuscanalsobeexpressedintermsofthequantities p(x | y)andp(y)thatwe\u2019velearned.Actually,ifwe\u2019recalculatingp(y | x)inorder tomakeaprediction,thenwedon\u2019tactuallyneedtocalculatethedenominator, since p(x | y)p(y) argmaxp(y | x) =argmax p(x) y y =argmaxp(x | y)p(y). y 4 Gaussian discriminant analysis Thefirstgenerativelearningalgorithmthatwe\u2019lllookatisGaussiandiscrim- inantanalysis(GDA).Inthismodel,we\u2019llassumethat p(x | y) isdistributed accordingtoamultivariatenormaldistribution.Let\u2019stalkbrieflyabouttheprop- ertiesofmultivariatenormaldistributionsbeforemovingontotheGDAmodel itself.",
      "chunk_id": 54,
      "start_pos": 51161,
      "end_pos": 52273,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ").Inthismodel,we\u2019llassumethat p(x | y) isdistributed accordingtoamultivariatenormaldistribution.Let\u2019stalkbrieflyabouttheprop- ertiesofmultivariatenormaldistributionsbeforemovingontotheGDAmodel itself. Themultivariatenormaldistributionind-dimensions,alsocalledthemulti- variateGaussiandistribution,isparameterizedbyameanvector\u00b5 \u2208Rd anda covariancematrix\u03a3 \u2208Rd\u00d7d,where\u03a3 \u22650issymmetricandpositivesemi-definite. Alsowritten\u2018\u2018N(\u00b5,\u03a3)\u2019\u2019,itsdensityisgivenby: (cid:18) (cid:19) 1 1 p(x;\u00b5,\u03a3) = exp \u2212 (x\u2212\u00b5)(cid:62)\u03a3\u22121(x\u2212\u00b5) (4.1) (2\u03c0)d/2|\u03a3|1/2 2 Intheequationabove,\u2018\u2018|\u03a3|\u2019\u2019denotesthedeterminantofthematrix\u03a3. Forarandomvariable X distributedN(\u00b5,\u03a3),themeanis(unsurprisingly) givenby\u00b5: (cid:90) E[X] = xp(x;\u00b5,\u03a3)dx = \u00b5 (4.2) x Thecovarianceofavector-valuedrandomvariableZisdefinedasCov(Z) = E[(Z\u2212E[Z])(Z\u2212E[Z])(cid:62)].Thisgeneralizesthenotionofthevarianceofareal- valuedrandomvariable.ThecovariancecanalsobedefinedasCov(Z) =E[ZZ(cid:62)]\u2212 (E[Z])(E[Z])(cid:62).(Youshouldbeabletoprovetoyourselfthatthesetwodefinitions areequivalent.)IfX \u223c N(\u00b5,\u03a3),then Cov(X) = \u03a3.",
      "chunk_id": 55,
      "start_pos": 52073,
      "end_pos": 53116,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "l- valuedrandomvariable.ThecovariancecanalsobedefinedasCov(Z) =E[ZZ(cid:62)]\u2212 (E[Z])(E[Z])(cid:62).(Youshouldbeabletoprovetoyourselfthatthesetwodefinitions areequivalent.)IfX \u223c N(\u00b5,\u03a3),then Cov(X) = \u03a3. (4.3) 33 HerearesomeexamplesofwhatthedensityofaGaussiandistributionlooks like: Theleft-mostfigureshowsaGaussianwithmeanzero(thatis,the2\u00d71zero- vector)andcovariancematrix\u03a3 = I(the2\u00d72identitymatrix).AGaussianwith zeromeanandidentitycovarianceisalsocalledthestandardnormaldistribution. ThemiddlefigureshowsthedensityofaGaussianwithzeromeanand\u03a3 =0.6I; andintherightmostfigureshowsonewith,\u03a3 = 2I.Weseethatas\u03a3becomes larger,theGaussianbecomesmore\u2018\u2018spread-out,\u2019\u2019andasitbecomessmaller,the distributionbecomesmore\u2018\u2018compressed.\u2019\u2019 Let\u2019slookatsomemoreexamples. ThefiguresaboveshowGaussianswithmean0,andwithcovariancematrices respectively: (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 0 1 0.5 1 0.8 \u03a3 = ; \u03a3 = ; \u03a3 = . (4.4) 0 1 0.5 1 0.8 1 The leftmost figure shows the familiar standard normal distribution, and we seethatasweincreasetheoff-diagonalentryin \u03a3,thedensitybecomesmore \u2018\u2018compressed\u2019\u2019towardsthe45\u25e6 line(givenby x = x ).Wecanseethismore 1 2 clearlywhenwelookatthecontoursofthesamethreedensit",
      "chunk_id": 56,
      "start_pos": 52916,
      "end_pos": 54116,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ution, and we seethatasweincreasetheoff-diagonalentryin \u03a3,thedensitybecomesmore \u2018\u2018compressed\u2019\u2019towardsthe45\u25e6 line(givenby x = x ).Wecanseethismore 1 2 clearlywhenwelookatthecontoursofthesamethreedensities: \u00b5=[0,0] \u00b5=[0,0] \u00b5=[0,0] (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) \u03a3= 1 0 \u03a3= 1 0.5 \u03a3= 1 0.8 0 1 0.5 1 0.8 1 4 2 0 \u22122 \u22124 \u22124 \u22122 0 2 4 \u22124 \u22122 0 2 4 \u22124 \u22122 0 2 4 Here\u2019sonelastsetofexamplesgeneratedbyvarying\u03a3: Theplotsaboveused,respectively, (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 3 0.8 1 \u22120.5 1 \u22120.8 \u03a3 = ; \u03a3 = ; \u03a3 = . (4.5) 0.8 1 \u22120.5 1 \u22120.8 1 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 34 chapter 4. gaussian discriminant analysis \u00b5=[0,0] \u00b5=[0,0] \u00b5=[0,0] \u03a3= (cid:20) 3 0.8 (cid:21) \u03a3= (cid:20) 1 \u22120.5 (cid:21) \u03a3= (cid:20) 1 \u22120.8 (cid:21) 0.8 3 \u22120.5 1 \u22120.8 1 4 2 0 \u22122 \u22124 \u22124 \u22122 0 2 4 \u22124 \u22122 0 2 4 \u22124 \u22122 0 2 4 Fromtheleftmostandmiddlefigures,weseethatbydecreasingtheoff-diagonal elementsofthecovariancematrix,thedensitynowbecomes\u2018\u2018compressed\u2019\u2019again, butintheoppositedirection.Lastly,aswevarytheparameters,moregenerally thecontourswillformellipses(therightmostfigureshowinganexample).",
      "chunk_id": 57,
      "start_pos": 53916,
      "end_pos": 55050,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "entsofthecovariancematrix,thedensitynowbecomes\u2018\u2018compressed\u2019\u2019again, butintheoppositedirection.Lastly,aswevarytheparameters,moregenerally thecontourswillformellipses(therightmostfigureshowinganexample). Asourlastsetofexamples,fixing\u03a3 = I,byvarying\u00b5,wecanalsomovethe meanofthedensityaround. Thefiguresaboveweregeneratedusing\u03a3 = I,andrespectively (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 \u22120.5 \u22121 \u00b5 = ; \u00b5 = ; \u00b5 = . (4.6) 0 0 \u22121.5 4.1 TheGaussianDiscriminantAnalysismodel Whenwehaveaclassificationprobleminwhichtheinputfeaturesxarecontinuous- valuedrandomvariables,wecanthenusetheGaussianDiscriminantAnalysis (GDA)model,whichmodels p(x | y)usingamultivariatenormaldistribution. Themodelis: y \u223cBernoulli(\u03c6) (4.7) x | y =0\u223c N(\u00b5 ,\u03a3) (4.8) 0 x | y =1\u223c N(\u00b5 ,\u03a3) (4.9) 1 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 4.1. the gaussian discriminant analysis model 35 Writingoutthedistributions,thisis: p(y) = \u03c6y(1\u2212\u03c6)1\u2212y (4.10) (cid:18) (cid:19) 1 1 p(x | y =0) = exp \u2212 (x\u2212\u00b5 )(cid:62)\u03a3\u22121(x\u2212\u00b5 ) (4.11) (2\u03c0)d/2|\u03a3|1/2 2 0 0 (cid:18) (cid:19) 1 1 p(x | y =1) = exp \u2212 (x\u2212\u00b5 )(cid:62)\u03a3\u22121(x\u2212\u00b5 ) (4.12) (2\u03c0)d/2|\u03a3|1/2 2 1 1 Here,theparametersofourmodelare\u03c6,\u03a3,\u00b5 and\u00b5 .(Notethatwhilethe",
      "chunk_id": 58,
      "start_pos": 54850,
      "end_pos": 56050,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ")(cid:62)\u03a3\u22121(x\u2212\u00b5 ) (4.11) (2\u03c0)d/2|\u03a3|1/2 2 0 0 (cid:18) (cid:19) 1 1 p(x | y =1) = exp \u2212 (x\u2212\u00b5 )(cid:62)\u03a3\u22121(x\u2212\u00b5 ) (4.12) (2\u03c0)d/2|\u03a3|1/2 2 1 1 Here,theparametersofourmodelare\u03c6,\u03a3,\u00b5 and\u00b5 .(Notethatwhilethere\u2019re 0 1 twodifferentmeanvectors\u00b5 and\u00b5 ,thismodelisusuallyappliedusingonly 0 1 onecovariancematrix\u03a3.)Thelog-likelihoodofthedataisgivenby n (cid:96)(\u03c6,\u00b5 ,\u00b5 ,\u03a3) =log \u220f p(x (i) ,y (i) ;\u03c6,\u00b5 ,\u00b5 ,\u03a3) (4.13) 0 1 o 1 i=1 n =log \u220f p(x (i) | y (i) ;\u03c6,\u00b5 ,\u00b5 ,\u03a3)p(y (i) ;\u03c6). (4.14) o 1 i=1 Bymaximizing(cid:96)withrespecttotheparameters,wefindthemaximumlikelihood estimateoftheparameters(seeproblemset1)tobe: \u03c6 = 1 \u2211 n 1{y (i) =1} (4.15) n i=1 \u2211n 1{y(i) =0}x(i) \u00b5 = i=1 (4.16) 0 \u2211n 1{y(i) =0} i=1 \u2211n 1{y(i) =1}x(i) \u00b5 = i=1 (4.17) 1 \u2211n 1{y(i) =1} i=1 \u03a3 = 1 \u2211 n (x (i)\u2212\u00b5 )(x (i)\u2212\u00b5 )(cid:62) (4.18) n y(i) y(i) i=1 Pictorially,whatthealgorithmisdoingcanbeseeninasfollows: Shown in the figure are the training set, as well as the contours of the two Gaussiandistributionsthathavebeenfittothedataineachofthetwoclasses.Note thatthetwoGaussianshavecontoursthatarethesameshapeandorientation, since they share a covariance matrix \u03a3, but they have different means \u00b5 and 0 \u00b5 .Alsoshowninthefigureisthestraightlinegivingthede",
      "chunk_id": 59,
      "start_pos": 55850,
      "end_pos": 57050,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "Note thatthetwoGaussianshavecontoursthatarethesameshapeandorientation, since they share a covariance matrix \u03a3, but they have different means \u00b5 and 0 \u00b5 .Alsoshowninthefigureisthestraightlinegivingthedecisionboundaryat 1 which p(y =1| x) =0.5.Ononesideoftheboundary,we\u2019llpredicty =1tobe themostlikelyoutcome,andontheotherside,we\u2019llpredicty =0. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 36 chapter 4. gaussian discriminant analysis 4.2 Discussion:GDAandlogisticregression TheGDAmodelhasaninterestingrelationshiptologisticregression.Ifweview thequantity p(y = 1 | x;\u03c6,\u00b5 ,\u00b5 ,\u03a3)asafunctionofx,we\u2019llfindthatitcanbe 0 1 expressedintheform 1 p(y =1| x;\u03c6,\u03a3,\u00b5 ,\u00b5 ) = , (4.19) 0 1 1+exp(\u2212\u03b8(cid:62)x) where\u03b8issomeappropriatefunctionof\u03c6,\u03a3,\u00b5 0 ,\u00b5 1 .1Thisisexactlytheformthat 1This uses the convention of re- logisticregression\u2014adiscriminativealgorithm\u2014usedtomodel p(y =1| x). defining the x(i)\u2019s on the right- hand-side to be (d+1)- dimen- Whenwouldwepreferonemodeloveranother?GDAandlogisticregression sionalvectorsbyaddingtheextra will, in general, give different decision boundaries when trained on the same coordinatex (i) = 1;seeproblem 0 dataset.Whichisbetter? set1.",
      "chunk_id": 60,
      "start_pos": 56850,
      "end_pos": 58037,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "other?GDAandlogisticregression sionalvectorsbyaddingtheextra will, in general, give different decision boundaries when trained on the same coordinatex (i) = 1;seeproblem 0 dataset.Whichisbetter? set1. Wejustarguedthatif p(x | y)ismultivariateGaussian(withshared\u03a3),then p(y | x) necessarily follows a logistic function. The converse, however, is not true;i.e., p(y | x)beingalogisticfunctiondoesnotimply p(x | y)ismultivariate Gaussian. This shows that GDA makes stronger modeling assumptions about the data than does logistic regression. It turns out that when these modeling assumptionsarecorrect,thenGDAwillfindbetterfitstothedata,andisabetter model.Specifically,when p(x | y)isindeedGaussian(withshared\u03a3),thenGDA isasymptoticallyefficient.Informally,thismeansthatinthelimitofverylarge trainingsets(largen),thereisnoalgorithmthatisstrictlybetterthanGDA(in termsof,say,howaccuratelytheyestimatep(y | x)).Inparticular,itcanbeshown thatinthissetting,GDAwillbeabetteralgorithmthanlogisticregression;and moregenerally,evenforsmalltrainingsetsizes,wewouldgenerallyexpectGDA tobebetter.",
      "chunk_id": 61,
      "start_pos": 57837,
      "end_pos": 58918,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "lytheyestimatep(y | x)).Inparticular,itcanbeshown thatinthissetting,GDAwillbeabetteralgorithmthanlogisticregression;and moregenerally,evenforsmalltrainingsetsizes,wewouldgenerallyexpectGDA tobebetter. Incontrast,bymakingsignificantlyweakerassumptions,logisticregression isalsomorerobustandlesssensitivetoincorrectmodelingassumptions.There aremanydifferentsetsofassumptionsthatwouldleadto p(y | x) takingthe formofalogisticfunction.Forexample,ifx | y =0\u223cPoisson(\u03bb ),andx | y = 0 1\u223cPoisson(\u03bb ),then p(y | x)willbelogistic.Logisticregressionwillalsowork 1 wellonPoissondatalikethis.ButifweweretouseGDAonsuchdata\u2014andfit Gaussiandistributionstosuchnon-Gaussiandata\u2014thentheresultswillbeless predictable,andGDAmay(ormaynot)dowell. Tosummarize:GDAmakesstrongermodelingassumptions,andismoredata efficient (i.e., requires less training data to learn \u2018\u2018well\u2019\u2019) when the modeling 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 4.2. discussion: gda and logistic regression 37 assumptions are correct or at least approximately correct.",
      "chunk_id": 62,
      "start_pos": 58718,
      "end_pos": 59764,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\u2019\u2019) when the modeling 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 4.2. discussion: gda and logistic regression 37 assumptions are correct or at least approximately correct. Logistic regression makesweakerassumptions,andissignificantlymorerobusttodeviationsfrom modelingassumptions.Specifically,whenthedataisindeednon-Gaussian,then inthelimitoflargedatasets,logisticregressionwillalmostalwaysdobetterthan GDA.Forthisreason,inpracticelogisticregressionisusedmoreoftenthanGDA. (Somerelatedconsiderationsaboutdiscriminativevs.generativemodelsalso applyfortheNaiveBayesalgorithmthatwediscussnext,buttheNaiveBayes algorithmisstillconsideredaverygood,andiscertainlyalsoaverypopular, classificationalgorithm.) toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 5 Naive Bayes InGDA,thefeaturevectors x werecontinuous,real-valuedvectors.Let\u2019snow talkaboutadifferentlearningalgorithminwhichthex \u2019sarediscrete-valued. j For our motivating example, consider building an email spam filter using machinelearning.Here,wewishtoclassifymessagesaccordingtowhetherthey are unsolicited commercial (spam) email, or non-spam email.",
      "chunk_id": 63,
      "start_pos": 59564,
      "end_pos": 60721,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "For our motivating example, consider building an email spam filter using machinelearning.Here,wewishtoclassifymessagesaccordingtowhetherthey are unsolicited commercial (spam) email, or non-spam email. After learning todothis,wecanthenhaveourmailreaderautomaticallyfilteroutthespam messagesandperhapsplacetheminaseparatemailfolder.Classifyingemailsis oneexampleofabroadersetofproblemscalledtextclassification. Let\u2019ssaywehaveatrainingset(asetofemailslabeledasspamornon-spam). We\u2019llbeginourconstructionofourspamfilterbyspecifyingthefeaturesx used j torepresentanemail. Wewillrepresentanemailviaafeaturevectorwhoselengthisequaltothe numberofwordsinthedictionary.Specifically,ifanemailcontainsthej-thword ofthedictionary,thenwewillsetx =1;otherwise,weletx =0.Forinstance, j j thevector \uf8ee \uf8f9 1 a \uf8ef0\uf8fa aardvark \uf8ef \uf8fa \uf8ef \uf8fa \uf8ef0\uf8fa aardwolf \uf8ef \uf8fa x = \uf8ef \uf8ef . . . \uf8fa \uf8fa . . . \uf8ef \uf8fa \uf8ef1\uf8fa buy \uf8ef \uf8fa \uf8ef.\uf8fa . \uf8ef. .\uf8fa . . \uf8f0 \uf8fb 0 zygmurgy is used to represent an email that contains the words \u2018\u2018a\u2019\u2019 and \u2018\u2018buy,\u2019\u2019 but not \u2018\u2018aardvark,\u2019\u2019 \u2018\u2018aardwolf\u2019\u2019 or \u2018\u2018zygmurgy.\u2019\u20191 The set of words encoded into the 1Actually, rather than looking featurevectoriscalledthevocabulary,sothedimensionofxisequaltothesize through an English dictionary for the list",
      "chunk_id": 64,
      "start_pos": 60521,
      "end_pos": 61721,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "olf\u2019\u2019 or \u2018\u2018zygmurgy.\u2019\u20191 The set of words encoded into the 1Actually, rather than looking featurevectoriscalledthevocabulary,sothedimensionofxisequaltothesize through an English dictionary for the list of all English words, ofthevocabulary. inpracticeitismorecommonto Havingchosenourfeaturevector,wenowwanttobuildagenerativemodel. lookthroughourtrainingsetand encodeinourfeaturevectoronly So,wehavetomodel p(x | y).Butifwehave,say,avocabularyof50000words, thewordsthatoccuratleastonce then x \u2208 {0,1}50000 (xisa50000-dimensionalvectorof0\u2019sand1\u2019s),andifwe there. Apart from reducing the weretomodelxexplicitlywithamultinomialdistributionoverthe250000possible number of words modeled and hencereducingourcomputational andspacerequirements,thisalso has the advantage of allowing ustomodel/includeasafeature many words that may appear in your email (such as \u2018\u2018cs229\u2019\u2019) but that you won\u2019t find in a dictionary. Sometimes (as in the homework), we also exclude the very high frequency words (whichwillbewordslike\u2018\u2018the,\u2019\u2019 \u2018\u2018of,\u2019\u2019\u2018\u2018and\u2019\u2019;thesehighfrequency, \u2018\u2018content free\u2019\u2019 words are called stopwords)sincetheyoccurinso manydocumentsanddolittleto indicatewhetheranemailisspam ornon-spam.",
      "chunk_id": 65,
      "start_pos": 61521,
      "end_pos": 62698,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ords (whichwillbewordslike\u2018\u2018the,\u2019\u2019 \u2018\u2018of,\u2019\u2019\u2018\u2018and\u2019\u2019;thesehighfrequency, \u2018\u2018content free\u2019\u2019 words are called stopwords)sincetheyoccurinso manydocumentsanddolittleto indicatewhetheranemailisspam ornon-spam. 39 outcomes,thenwe\u2019dendupwitha(250000\u22121)-dimensionalparametervector. Thisisclearlytoomanyparameters. Tomodel p(x | y),wewillthereforemakeaverystrongassumption.Wewill assumethatthex\u2019sareconditionallyindependentgiveny.Thisassumptionis i calledtheNaiveBayes(NB)assumption,andtheresultingalgorithmiscalled theNaiveBayesclassifier.Forinstance,if y = 1meansspamemail;\u2018\u2018buy\u2019\u2019is word2087and\u2018\u2018price\u2019\u2019isword39831;thenweareassumingthatifItellyouy =1 (thataparticularpieceofemailisspam),thenknowledgeofx (knowledge 2087 ofwhether\u2018\u2018buy\u2019\u2019appearsinthemessage)willhavenoeffectonyourbeliefs about the value of x (whether \u2018\u2018price\u2019\u2019 appears). More formally, this can 39831 bewritten p(x | y) = p(x | y,x ).(Notethatthisisnotthesameas 2087 2087 39831 saying that x and x are independent, which would have been written 2087 39831 \u2018\u2018p(x ) = p(x | x )\u2019\u2019;rather,weareonlyassumingthatx andx 2087 2087 39831 2087 39831 areconditionallyindependentgiveny.) Wenowhave: p(x ,...,x | y) (5.1) 1 50000 = p(x | y)p(x | y,x )p(x | y,",
      "chunk_id": 66,
      "start_pos": 62498,
      "end_pos": 63698,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "087 39831 \u2018\u2018p(x ) = p(x | x )\u2019\u2019;rather,weareonlyassumingthatx andx 2087 2087 39831 2087 39831 areconditionallyindependentgiveny.) Wenowhave: p(x ,...,x | y) (5.1) 1 50000 = p(x | y)p(x | y,x )p(x | y,x ,x )\u00b7\u00b7\u00b7p(x | y,x ,...,x ) (5.2) 1 2 1 3 1 2 50000 1 49999 = p(x | y)p(x | y)p(x | y)\u00b7\u00b7\u00b7p(x | y) (5.3) 1 2 3 50000 d \u220f = p(x | y) (5.4) j j=1 Thefirstequalitysimplyfollowsfromtheusualpropertiesofprobabilities,and thesecondequalityusedtheNBassumption.WenotethateventhoughtheNaive Bayesassumptionisanextremelystrongassumptions,theresultingalgorithm workswellonmanyproblems. Ourmodelisparameterizedby\u03c6 = p(x =1| y =1),\u03c6 = p(x =1| j|y=1 j j|y=0 j y =0),and\u03c6 = p(y =1).Asusual,givenatrainingset{(x(i),y(i));i =1,...,n}, y wecanwritedownthejointlikelihoodofthedata: n L(\u03c6 ,\u03c6 ,\u03c6 ) = \u220f p(x (i) ,y (i)) (5.5) y j|y=0 j|y=1 i=1 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 40 chapter 5. naive bayes Maximizingthiswithrespectto\u03c6 ,\u03c6 and\u03c6 givesthemaximumlikeli- y j|y=0 j|y=1 hoodestimates: \u2211n 1{x (i) =1\u2227y(i) =1} i=1 j \u03c6 = (5.6) j|y=1 \u2211n 1{y(i) =1} i=1 \u2211n 1{x (i) =1\u2227y(i) =0} i=1 j \u03c6 = (5.7) j|y=0 \u2211n 1{y(i) =0} i=1 \u2211n 1{y(i) =1} \u03c6 = i=1 (5.8) y n Intheequationsabove,the\u2018\u2018\u2227\u2019\u2019symbolme",
      "chunk_id": 67,
      "start_pos": 63498,
      "end_pos": 64698,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ates: \u2211n 1{x (i) =1\u2227y(i) =1} i=1 j \u03c6 = (5.6) j|y=1 \u2211n 1{y(i) =1} i=1 \u2211n 1{x (i) =1\u2227y(i) =0} i=1 j \u03c6 = (5.7) j|y=0 \u2211n 1{y(i) =0} i=1 \u2211n 1{y(i) =1} \u03c6 = i=1 (5.8) y n Intheequationsabove,the\u2018\u2018\u2227\u2019\u2019symbolmeans\u2018\u2018and.\u2019\u2019Theparametershavea verynaturalinterpretation.Forinstance,\u03c6 isjustthefractionofthespam j|y=1 (y =1)emailsinwhichwordjdoesappear. Havingfitalltheseparameters,tomakeapredictiononanewexamplewith featuresx,wethensimplycalculate p(x | y =1)p(y =1) p(y =1| x) = (5.9) p(x) (cid:16) (cid:17) \u220fd p(x | y =1) p(y =1) j=1 j = (cid:16) (cid:17) (cid:16) (cid:17) , \u220fd p(x | y =1) p(y =1)+ \u220fd p(x | y =0) p(y =0) j=1 j j=1 j (5.10) andpickwhicheverclasshasthehigherposteriorprobability. Lastly,wenotethatwhilewehavedevelopedtheNaiveBayesalgorithmmainly forthecaseofproblemswherethefeaturesx arebinary-valued,thegeneralization j towherex cantakevaluesin{1,2,...,k }isstraightforward.Here,wewould j j simplymodel p(x | y)asmultinomialratherthanasBernoulli.Indeed,evenif j someoriginalinputattribute(say,thelivingareaofahouse,asinourearlier example)werecontinuousvalued,itisquitecommontodiscretizeit\u2014thatis,turn itintoasmallsetofdiscretevalues\u2014andapplyNaiveBayes.Forinstance,ifwe usesomefeaturex toreprese",
      "chunk_id": 68,
      "start_pos": 64498,
      "end_pos": 65698,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "helivingareaofahouse,asinourearlier example)werecontinuousvalued,itisquitecommontodiscretizeit\u2014thatis,turn itintoasmallsetofdiscretevalues\u2014andapplyNaiveBayes.Forinstance,ifwe usesomefeaturex torepresentlivingarea,wemightdiscretizethecontinuous j valuesasfollows: Livingarea(ft2) <400 400\u2212800 800\u22121200 1200\u22121600 >1600 Table5.1. Discretizedlivingarea. x 1 2 3 4 5 i 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 5.1. laplace smoothing 41 Thus,forahousewithlivingarea890squarefeet,wewouldsetthevalueofthe correspondingfeaturex to3.WecanthenapplytheNaiveBayesalgorithm,and j model p(x | y)withamultinomialdistribution,asdescribedpreviously.When j theoriginal,continuous-valuedattributesarenotwell-modeledbyamultivariate normaldistribution,discretizingthefeaturesandusingNaiveBayes(insteadof GDA)willoftenresultinabetterclassifier. 5.1 Laplacesmoothing TheNaiveBayesalgorithmaswehavedescribeditwillworkfairlywellformany problems,butthereisasimplechangethatmakesitworkmuchbetter,especially fortextclassification.Let\u2019sbrieflydiscussaproblemwiththealgorithminits currentform,andthentalkabouthowwecanfixit.",
      "chunk_id": 69,
      "start_pos": 65498,
      "end_pos": 66621,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "fairlywellformany problems,butthereisasimplechangethatmakesitworkmuchbetter,especially fortextclassification.Let\u2019sbrieflydiscussaproblemwiththealgorithminits currentform,andthentalkabouthowwecanfixit. Considerspam/emailclassification,andlet\u2019ssupposethat,weareintheyear of20xx,aftercompletingCS229andhavingdoneexcellentworkontheproject, youdecidearoundMay20xxtosubmitworkyoudidtotheNeurIPSconference forpublication.2 Becauseyouendupdiscussingtheconferenceinyouremails, 2NeurIPSisoneofthetopmachine youalsostartgettingmessageswiththeword\u2018\u2018neurips\u2019\u2019init.Butthisisyour learningconferences.Thedeadline forsubmittingapaperistypically firstNeurIPSpaper,anduntilthistime,youhadnotpreviouslyseenanyemails inMay-June. containingtheword\u2018\u2018neurips\u2019\u2019;inparticular\u2018\u2018neurips\u2019\u2019didnoteverappearin yourtrainingsetofspam/non-spamemails.Assumingthat\u2018\u2018neurips\u2019\u2019wasthe 35000thwordinthedictionary,yourNaiveBayesspamfilterthereforehadpicked itsmaximumlikelihoodestimatesoftheparameters\u03c6 tobe 35000|y \u2211n 1{x (i) =1\u2227y(i) =1} \u03c6 = i=1 35000 =0 (5.11) 35000|y=1 \u2211n 1{y(i) =1} i=1 \u2211n 1{x (i) =1\u2227y(i) =0} \u03c6 = i=1 35000 =0, (5.12) 35000|y=0 \u2211n 1{y(i) =0} i=1 i.e., because it has never seen \u2018\u2018neurips\u2019\u2019 before in either spam or non-s",
      "chunk_id": 70,
      "start_pos": 66421,
      "end_pos": 67621,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "= i=1 35000 =0 (5.11) 35000|y=1 \u2211n 1{y(i) =1} i=1 \u2211n 1{x (i) =1\u2227y(i) =0} \u03c6 = i=1 35000 =0, (5.12) 35000|y=0 \u2211n 1{y(i) =0} i=1 i.e., because it has never seen \u2018\u2018neurips\u2019\u2019 before in either spam or non-spam trainingexamples,itthinkstheprobabilityofseeingitineithertypeofemailis zero.Hence,whentryingtodecideifoneofthesemessagescontaining\u2018\u2018neurips\u2019\u2019 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 42 chapter 5. naive bayes isspam,itcalculatestheclassposteriorprobabilities,andobtains \u220fd p(x | y =1)p(y =1) j=1 j p(y =1| x) = \u220fd p(x | y =1)p(y =1)+\u220fd p(x | y =0)p(y =0) j=1 j j=1 j (5.13) 0 = (5.14) 0 Thisisbecauseeachoftheterms\u2018\u2018\u220fd p(x | y)\u2019\u2019includesatermp(x | y) =0 j=1 j 35000 thatismultipliedintoit.Hence,ouralgorithmobtains0/0,anddoesn\u2019tknow howtomakeaprediction. Statingtheproblemmorebroadly,itisstatisticallyabadideatoestimatethe probabilityofsomeeventtobezerojustbecauseyouhaven\u2019tseenitbeforeinyour finitetrainingset.Taketheproblemofestimatingthemeanofamultinomialran- domvariableztakingvaluesin{1,...,k}.Wecanparameterizeourmultinomial with\u03c6 = p(z = j).Givenasetofnindependentobservations{z(1),...,z(n)}, j themaximumlikelihoodestimatesaregivenby \u2211n 1{z(i) = j} \u03c6 = i=1",
      "chunk_id": 71,
      "start_pos": 67421,
      "end_pos": 68621,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "mvariableztakingvaluesin{1,...,k}.Wecanparameterizeourmultinomial with\u03c6 = p(z = j).Givenasetofnindependentobservations{z(1),...,z(n)}, j themaximumlikelihoodestimatesaregivenby \u2211n 1{z(i) = j} \u03c6 = i=1 . (5.15) j n Aswesawpreviously,ifweweretousethesemaximumlikelihoodestimates, thensomeofthe\u03c6\u2019smightendupaszero,whichwasaproblem.Toavoidthis, j wecanuseLaplacesmoothing,whichreplacestheaboveestimatewith 1+\u2211n 1{z(i) = j} \u03c6 = i=1 . (5.16) j k+n Here, we\u2019ve added 1 to the numerator, and k to the denominator. Note that \u2211k \u03c6 =1stillholds(checkthisyourself!),whichisadesirablepropertysince j=1 j the\u03c6\u2019sareestimatesforprobabilitiesthatweknowmustsumto1.Also,\u03c6 (cid:54)=0 j j forallvaluesofj,solvingourproblemofprobabilitiesbeingestimatedaszero. Undercertain(arguablyquitestrong)conditions,itcanbeshownthattheLaplace smoothingactuallygivestheoptimalestimatorofthe\u03c6\u2019s. j 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 5.2. event models for text classification 43 ReturningtoourNaiveBayesclassifier,withLaplacesmoothing,wetherefore obtainthefollowingestimatesoftheparameters: 1+\u2211n 1{x (i) =1\u2227y(i) =1} i=1 j \u03c6 = (5.17) j|y=1 2+\u2211n 1{y(i) =1} i=1 1+\u2211n 1{x (i) =1\u2227y(i) =0} i=1 j \u03c6 = (5.18)",
      "chunk_id": 72,
      "start_pos": 68421,
      "end_pos": 69621,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "esclassifier,withLaplacesmoothing,wetherefore obtainthefollowingestimatesoftheparameters: 1+\u2211n 1{x (i) =1\u2227y(i) =1} i=1 j \u03c6 = (5.17) j|y=1 2+\u2211n 1{y(i) =1} i=1 1+\u2211n 1{x (i) =1\u2227y(i) =0} i=1 j \u03c6 = (5.18) j|y=0 2+\u2211n 1{y(i) =0} i=1 (Inpractice,itusuallydoesn\u2019tmattermuchwhetherweapplyLaplacesmoothing to\u03c6 ornot,sincewewilltypicallyhaveafairfractioneachofspamandnon-spam y messages,so\u03c6 willbeareasonableestimateof p(y = 1)andwillbequitefar y from0anyway.) 5.2 Eventmodelsfortextclassification Tocloseoffourdiscussionofgenerativelearningalgorithms,let\u2019stalkaboutone moremodelthatisspecificallyfortextclassification.WhileNaiveBayesaswe\u2019ve presenteditwillworkwellformanyclassificationproblems,fortextclassification, thereisarelatedmodelthatdoesevenbetter. Inthespecificcontextoftextclassification,NaiveBayesaspresentedusesthe what\u2019scalledtheBernoullieventmodel(orsometimesmulti-variateBernoulli eventmodel).Inthismodel,weassumedthatthewayanemailisgeneratedis thatfirstitisrandomlydetermined(accordingtotheclasspriors p(y))whether aspammerornon-spammerwillsendyouyournextmessage.Then,theperson sendingtheemailrunsthroughthedictionary,decidingwhethertoincludeeach wordjinthatemailindependentlyandaccordingtothepr",
      "chunk_id": 73,
      "start_pos": 69421,
      "end_pos": 70621,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "priors p(y))whether aspammerornon-spammerwillsendyouyournextmessage.Then,theperson sendingtheemailrunsthroughthedictionary,decidingwhethertoincludeeach wordjinthatemailindependentlyandaccordingtotheprobabilities p(x =1| j y) = \u03c6 .Thus,theprobabilityofamessagewasgivenby p(y)\u220fd p(x | y) j|y j=1 j Here\u2019s a different model, called the Multinomial event model. To describe thismodel,wewilluseadifferentnotationandsetoffeaturesforrepresenting emails.Weletx denotetheidentityofthej-thwordintheemail.Thus,x isnow j j anintegertakingvaluesin{1,...,|V|},where|V|isthesizeofourvocabulary (dictionary).Anemailofdwordsisnowrepresentedbyavector(x ,x ,...,x ) 1 2 d oflengthd;notethatdcanvaryfordifferentdocuments.Forinstance,ifanemail startswith\u2018\u2018ANeurIPS\u2026,\u2019\u2019thenx =1(\u2018\u2018a\u2019\u2019isthefirstwordinthedictionary), 1 andx =35000(if\u2018\u2018neurips\u2019\u2019isthe35000thwordinthedictionary). 2 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 44 chapter 5. naive bayes Inthemultinomialeventmodel,weassumethatthewayanemailisgenerated isviaarandomprocessinwhichspam/non-spamisfirstdetermined(according top(y))asbefore.Then,thesenderoftheemailwritestheemailbyfirstgenerating x fromsomemultinomialdistributionoverwords(",
      "chunk_id": 74,
      "start_pos": 70421,
      "end_pos": 71621,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "nemailisgenerated isviaarandomprocessinwhichspam/non-spamisfirstdetermined(according top(y))asbefore.Then,thesenderoftheemailwritestheemailbyfirstgenerating x fromsomemultinomialdistributionoverwords(p(x | y)).Next,thesecond 1 1 wordx ischosenindependentlyofx butfromthesamemultinomialdistribution, 2 1 and similarly for x , x , and so on, until all d words of the email have been 3 4 generated.Thus,theoverallprobabilityofamessageisgivenby p(y)\u220fd p(x | j=1 j y).Notethatthisformulalooksliketheonewehadearlierfortheprobabilityofa messageundertheBernoullieventmodel,butthatthetermsintheformulanow meanverydifferentthings.Inparticularx | yisnowamultinomial,ratherthan j aBernoullidistribution. Theparametersforournewmodelare\u03c6 = p(y)asbefore,\u03c6 = p(x = y k|y=1 j k | y =1)(foranyj)and\u03c6 = p(x = k | y =0).Notethatwehaveassumed k|y=0 j that p(x | y)isthesameforallvaluesofj(i.e.,thatthedistributionaccordingto j whichawordisgenerateddoesnotdependonitspositionjwithintheemail). Ifwearegivenatrainingset{(x(i),y(i));i =1,...,n}wherex(i) = (x (i) ,x (i) ,...,x (i) ) 1 2 di (here,d isthenumberofwordsinthei-trainingexample),thelikelihoodofthe i dataisgivenby n L(\u03c6 ,\u03c6 ,\u03c6 ) = \u220f p(x (i) ,y (i)) (5.19) y k|y=0 k",
      "chunk_id": 75,
      "start_pos": 71421,
      "end_pos": 72621,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "),y(i));i =1,...,n}wherex(i) = (x (i) ,x (i) ,...,x (i) ) 1 2 di (here,d isthenumberofwordsinthei-trainingexample),thelikelihoodofthe i dataisgivenby n L(\u03c6 ,\u03c6 ,\u03c6 ) = \u220f p(x (i) ,y (i)) (5.19) y k|y=0 k|y=1 i=1 (cid:32) (cid:33) = \u220f n \u220f di p(x (i) | y;\u03c6 ,\u03c6 ) p(y (i) ;\u03c6 ). (5.20) j k|y=0 k|y=1 y i=1 j=1 Maximizingthisyieldsthemaximumlikelihoodestimatesoftheparameters: \u2211n \u2211di 1{x (i) = k\u2227y(i) =1} i=1 j=1 j \u03c6 = (5.21) k|y=1 \u2211n 1{y(i) =1}d i=1 i \u2211n \u2211di 1{x (i) = k\u2227y(i) =0} i=1 j=1 j \u03c6 = (5.22) k|y=0 \u2211n 1{y(i) =0}d i=1 i \u2211n 1{y(i) =1} \u03c6 = i=1 . (5.23) y n IfweweretoapplyLaplacesmoothing(whichisneededinpracticeforgood performance)whenestimating\u03c6 and\u03c6 ,weadd1tothenumeratorsand k|y=0 k|y=1 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 5.2. event models for text classification 45 |V|tothedenominators,andobtain: 1+\u2211n \u2211di 1{x (i) = k\u2227y(i) =1} i=1 j=1 j \u03c6 = (5.24) k|y=1 |V|+\u2211n 1{y(i) =1}d i=1 i 1+\u2211n \u2211di 1{x (i) = k\u2227y(i) =0} i=1 j=1 j \u03c6 = (5.25) k|y=0 |V|+\u2211n 1{y(i) =0}d i=1 i While not necessarily the very best classification algorithm, the Naive Bayes classifieroftenworkssurprisinglywell.Itisoftenalsoaverygood\u2018\u2018firstthingto try,\u2019\u2019givenitssimplicityandeaseofimplementatio",
      "chunk_id": 76,
      "start_pos": 72421,
      "end_pos": 73621,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "i While not necessarily the very best classification algorithm, the Naive Bayes classifieroftenworkssurprisinglywell.Itisoftenalsoaverygood\u2018\u2018firstthingto try,\u2019\u2019givenitssimplicityandeaseofimplementation. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu Part III: Kernel Methods 6 Kernel methods FromCS229Fall2020,TengyuMa, Moses Charikar, Andrew Ng & ChristopherR\u00e9,StanfordUniver- sity. 6.1 Featuremaps Recallthatinourdiscussionaboutlinearregression,weconsideredtheproblem ofpredictingthepriceofahouse(denotedbyy)fromthelivingareaofthehouse (denotedbyx),andwefitalinearfunctionofxtothetrainingdata.Whatifthe priceycanbemoreaccuratelyrepresentedasanon-linearfunctionofx?Inthis case,weneedamoreexpressivefamilyofmodelsthanlinearmodels. Westartbyconsideringfittingcubicfunctionsy = \u03b8 x3+\u03b8 x2+\u03b8 x+\u03b8 .It 3 2 1 0 turnsoutthatwecanviewthecubicfunctionasalinearfunctionoveradifferent setoffeaturevariables(definedbelow).Concretely,letthefunction\u03c6 :R(cid:55)\u2192R4 bedefinedas \uf8ee \uf8f9 1 \uf8efx\uf8fa \u03c6(x) = \uf8ef \uf8fa \u2208R4.",
      "chunk_id": 77,
      "start_pos": 73421,
      "end_pos": 74433,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "x+\u03b8 .It 3 2 1 0 turnsoutthatwecanviewthecubicfunctionasalinearfunctionoveradifferent setoffeaturevariables(definedbelow).Concretely,letthefunction\u03c6 :R(cid:55)\u2192R4 bedefinedas \uf8ee \uf8f9 1 \uf8efx\uf8fa \u03c6(x) = \uf8ef \uf8fa \u2208R4. (6.1) \uf8ef \uf8f0x2\uf8fa \uf8fb x3 Let\u03b8 \u2208R4bethevectorcontaining\u03b8 ,\u03b8 ,\u03b8 ,\u03b8 asentries.Thenwecanrewrite 0 1 2 3 thecubicfunctioninxas: \u03b8 x3+\u03b8 x2+\u03b8 x+\u03b8 = \u03b8 (cid:62) \u03c6(x) 3 2 1 0 Thus,acubicfunctionofthevariablexcanbeviewedasalinearfunctionoverthe variables\u03c6(x).Todistinguishbetweenthesetwosetsofvariables,inthecontext ofkernelmethods,wewillcallthe\u2018\u2018original\u2019\u2019inputvaluetheinputattributesof aproblem(inthiscase,x,thelivingarea).Whentheoriginalinputismappedto somenewsetofquantities\u03c6(x),wewillcallthosenewquantitiesthefeatures variables.(Unfortunately,differentauthorsusedifferenttermstodescribethese twothingsindifferentcontexts.)Wewillcall\u03c6afeaturemap,whichmapsthe attributestothefeatures. 6.2. lms (least mean squares) with features 47 6.2 LMS(leastmeansquares)withfeatures Wewillderivethegradientdescentalgorithmforfittingthemodel\u03b8(cid:62)\u03c6(x).First recallthatforordinaryleastsquareproblemwhereweweretofit\u03b8(cid:62)x,thebatch gradientdescentupdateis(seethefirstlecturenoteforitsderivation): n (cid:16) (cid:17) \u03b8 := \u03b8+",
      "chunk_id": 78,
      "start_pos": 74233,
      "end_pos": 75433,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ttingthemodel\u03b8(cid:62)\u03c6(x).First recallthatforordinaryleastsquareproblemwhereweweretofit\u03b8(cid:62)x,thebatch gradientdescentupdateis(seethefirstlecturenoteforitsderivation): n (cid:16) (cid:17) \u03b8 := \u03b8+\u03b1 \u2211 y (i)\u2212h (x (i)) x (i) (6.2) \u03b8 i=1 n (cid:16) (cid:17) := \u03b8+\u03b1 \u2211 y (i)\u2212\u03b8 (cid:62) x (i) x (i) . (6.3) i=1 Let\u03c6 :Rd (cid:55)\u2192Rp beafeaturemapthatmapsattributex(inRd)tothefeatures \u03c6(x)inRp.(Inthemotivatingexampleintheprevioussubsection,wehaved =1 andp =4.)Nowourgoalistofitthefunction\u03b8(cid:62)\u03c6(x),with\u03b8beingavectorinRp insteadofRd.Wecanreplacealltheoccurrencesofx(i)inthealgorithmaboveby \u03c6(x(i))toobtainthenewupdate: n (cid:16) (cid:17) \u03b8 := \u03b8+\u03b1 \u2211 y (i)\u2212\u03b8 (cid:62) \u03c6(x (i)) \u03c6(x (i)). (6.4) i=1 Similarly,thecorrespondingstochasticgradientdescentupdateruleis: (cid:16) (cid:17) \u03b8 := \u03b8+\u03b1 y (i)\u2212\u03b8 (cid:62) \u03c6(x (i)) \u03c6(x (i)). (6.5) 6.3 LMSwiththekerneltrick Thegradientdescentupdate,orstochasticgradientupdateabovebecomescompu- tationallyexpensivewhenthefeatures\u03c6(x)ishigh-dimensional.Forexample,con- siderthedirectextensionofthefeaturemapinequation6.1tohigh-dimensional inputx:supposex \u2208Rd,andlet\u03c6(x)bethevectorthatcontainsallthemonomials toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stan",
      "chunk_id": 79,
      "start_pos": 75233,
      "end_pos": 76433,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "derthedirectextensionofthefeaturemapinequation6.1tohigh-dimensional inputx:supposex \u2208Rd,andlet\u03c6(x)bethevectorthatcontainsallthemonomials toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 48 chapter 6. kernel methods ofxwithdegree\u22643 \uf8ee \uf8f9 1 \uf8ef x \uf8fa \uf8ef 1 \uf8fa \uf8ef \uf8fa \uf8ef x 2 \uf8fa \uf8ef \uf8fa \uf8ef . . \uf8fa \uf8ef . \uf8fa \uf8ef \uf8fa \uf8ef x2 \uf8fa \uf8ef 1 \uf8fa \uf8ef \uf8fa \uf8efx 1 x 2\uf8fa \uf8ef \uf8fa \uf8efx x \uf8fa \u03c6(x) = \uf8ef 1 3\uf8fa (6.6) \uf8ef . \uf8fa \uf8ef . . \uf8fa \uf8ef \uf8fa \uf8ef \uf8fa \uf8efx 2 x 1\uf8fa \uf8ef . \uf8fa \uf8ef . \uf8fa \uf8ef . \uf8fa \uf8ef \uf8fa \uf8ef x3 \uf8fa \uf8ef 1 \uf8fa \uf8efx2x \uf8fa \uf8ef 1 2\uf8fa \uf8f0 . \uf8fb . . Thedimensionofthefeatures\u03c6(x)isontheorderofd3.1Thisisaprohibitively 1Here,forsimplicity,weincludeall longvectorforcomputationalpurpose\u2014whend =1000,eachupdaterequiresat themonomialswithrepetitions(so leastcomputingandstoringa10003 =109dimensionalvector,whichis106times t a h p a p t e , a e r .g i . n ,x \u03c6 1 ( x x 2 ) x ) 3 . a T n h d er x e 2 fo x r 3 e x , 1 t b h o e t r h e slowerthantheupdateruleforforordinaryleastsquaresupdatesinequation aretotally1+d+d2+d3entries in\u03c6(x). 6.3. Itmayappearatfirstthatsuchd3runtimeperupdateandmemoryusageare inevitable,becausethevector\u03b8itselfisofdimension p \u2248 d3,andwemayneed toupdateeveryentryof\u03b8 andstoreit.However,wewillintroducethekernel trickwithwhichwewillnotneedtostore\u03b8 explicitly,andtheruntimeca",
      "chunk_id": 80,
      "start_pos": 76233,
      "end_pos": 77433,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "are inevitable,becausethevector\u03b8itselfisofdimension p \u2248 d3,andwemayneed toupdateeveryentryof\u03b8 andstoreit.However,wewillintroducethekernel trickwithwhichwewillnotneedtostore\u03b8 explicitly,andtheruntimecanbe significantlyimproved. Forsimplicity,weassumetheinitializethevalue\u03b8 =0,andwefocusonthe iterativeupdateinequation6.4.Themainobservationisthatatanytime,\u03b8can berepresentedasalinearcombinationofthevectors\u03c6(x(1)),...,\u03c6(x(n)).Indeed, wecanshowthisinductivelyasfollows.Atinitialization,\u03b8 =0= \u2211n 0\u00b7\u03c6(x(i)). i=1 Assumeatsomepoint,\u03b8canberepresentedas n \u03b8 = \u2211 \u03b2 \u03c6(x (i)) (6.7) i i=1 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 6.3. lms with the kernel trick 49 forsome\u03b2 ,...,\u03b2 \u2208 R.Thenweclaimthatinthenextround,\u03b8isstillalinear 1 n combinationof\u03c6(x(1)),...,\u03c6(x(n))because n (cid:16) (cid:17) \u03b8 := \u03b8+\u03b1 \u2211 y (i)\u2212\u03b8 (cid:62) \u03c6(x (i)) \u03c6(x (i)) (6.8) i=1 n n (cid:16) (cid:17) = \u2211 \u03b2 \u03c6(x (i))+\u03b1 \u2211 y (i)\u2212\u03b8 (cid:62) \u03c6(x (i)) \u03c6(x (i)) (6.9) i i=1 i=1 n (cid:16) (cid:16) (cid:17)(cid:17) = \u2211 \u03b2 +\u03b1 y (i)\u2212\u03b8 (cid:62) \u03c6(x (i)) \u03c6(x (i)) (6.10) i i=1(cid:124) (cid:123)(cid:122) (cid:125) new\u03b2i Youmayrealizethatourgeneralstrategyistoimplicitlyrepresentthep-dimensional vector\u03b8byasetofcoefficients",
      "chunk_id": 81,
      "start_pos": 77233,
      "end_pos": 78433,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\u03b2 +\u03b1 y (i)\u2212\u03b8 (cid:62) \u03c6(x (i)) \u03c6(x (i)) (6.10) i i=1(cid:124) (cid:123)(cid:122) (cid:125) new\u03b2i Youmayrealizethatourgeneralstrategyistoimplicitlyrepresentthep-dimensional vector\u03b8byasetofcoefficients\u03b2 ,...,\u03b2 .Towardsdoingthis,wederivetheup- 1 n dateruleofthecoefficients\u03b2 ,...,\u03b2 .Usingtheequationabove,weseethatthe 1 n new\u03b2 dependsontheoldonevia: i (cid:16) (cid:17) \u03b2 := \u03b2 +\u03b1 y (i)\u2212\u03b8 (cid:62) \u03c6(x (i)) (6.11) i i Here we still have the old \u03b8 on the RHS of the equation. Replacing \u03b8 by \u03b8 = \u2211n \u03b2 \u03c6(x(j))gives: j=1 j (cid:32) (cid:33) n \u2200 \u2208 {1,...,n},\u03b2 := \u03b2 +\u03b1 y (i)\u2212 \u2211 \u03b2 \u03c6(x (j))(cid:62) \u03c6(x (i)) i i i j j=1 We often rewrite \u03c6(x(j))(cid:62)\u03c6(x(i)) as (cid:104)\u03c6(x(j)),\u03c6(x(i))(cid:105) to emphasize that it\u2019s the innerproductofthetwofeaturevectors.Viewing\u03b2 \u2019sasthenewrepresentation i of\u03b8,wehavesuccessfullytranslatedthebatchgradientdescentalgorithminto analgorithmthatupdatesthevalueof\u03b2iteratively.Itmayappearthatatevery iteration,westillneedtocomputethevaluesof(cid:104)\u03c6(x(j)),\u03c6(x(i))(cid:105)forallpairsof i,j,eachofwhichmaytakeroughlyO(p) operation.However,twoimportant propertiescometorescue: 1.",
      "chunk_id": 82,
      "start_pos": 78233,
      "end_pos": 79333,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "mayappearthatatevery iteration,westillneedtocomputethevaluesof(cid:104)\u03c6(x(j)),\u03c6(x(i))(cid:105)forallpairsof i,j,eachofwhichmaytakeroughlyO(p) operation.However,twoimportant propertiescometorescue: 1. Wecanpre-computethepairwiseinnerproducts(cid:104)\u03c6(x(j)),\u03c6(x(i))(cid:105)forallpairs ofi,jbeforetheloopstarts. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 50 chapter 6. kernel methods 2. Forthefeaturemap\u03c6definedin6.6(ormanyotherinterestingfeaturemaps), computing(cid:104)\u03c6(x(j)),\u03c6(x(i))(cid:105)canbeefficientanddoesnotnecessarilyrequire computing\u03c6(x(i))explicitly.Thisisbecause: d \u2211 \u2211 \u2211 (cid:104)\u03c6(x),\u03c6(z)(cid:105) =1+ x z + x x z z + x x x z z z i i i j i j i j k i j k i=1 i,j\u2208{1,...,d} i,j,k\u2208{1,...,d} (6.12) (cid:32) (cid:33)2 (cid:32) (cid:33)3 d d d \u2211 \u2211 \u2211 =1+ x z + x z + x z (6.13) i i i i i i i=1 i=1 i=1 =1+(cid:104)x,z(cid:105)+(cid:104)x,z(cid:105)2+(cid:104)x,z(cid:105)3 (6.14) Therefore,tocompute(cid:104)\u03c6(x),\u03c6(z)(cid:105),wecanfirstcompute(cid:104)x,z(cid:105)withO(d)time andthentakeanotherconstantnumberofoperationstocompute1+(cid:104)x,z(cid:105)+ (cid:104)x,z(cid:105)2+(cid:104)x,z(cid:105)3.",
      "chunk_id": 83,
      "start_pos": 79133,
      "end_pos": 80276,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "d:104)\u03c6(x),\u03c6(z)(cid:105),wecanfirstcompute(cid:104)x,z(cid:105)withO(d)time andthentakeanotherconstantnumberofoperationstocompute1+(cid:104)x,z(cid:105)+ (cid:104)x,z(cid:105)2+(cid:104)x,z(cid:105)3. Asyouwillsee,theinnerproductsbetweenthefeatures(cid:104)\u03c6(x),\u03c6(z)(cid:105)areessen- tialhere.WedefinetheKernelcorrespondingtothefeaturemap\u03c6asafunction thatmapsX \u00d7X (cid:55)\u2192Rsatisfying:2 2RecallthatX isthespaceofthe inputx.Inourrunningexample, K(x,z)(cid:44)(cid:104)\u03c6(x),\u03c6(z)(cid:105) (6.15) X =Rd Towrapupthediscussion,wewritethedownthefinalalgorithmasfollows: 1. ComputeallthevaluesK(x(i),x(j))(cid:44)(cid:104)\u03c6(x(i)),\u03c6(x(j))(cid:105)usingequation6.14for alli,j \u2208 {1,...,n}.Set\u03b2 :=0. 2. Loop: (cid:32) (cid:33) n \u2200 \u2208 {1,...,n},\u03b2 := \u03b2 +\u03b1 y (i)\u2212 \u2211 \u03b2 K(x (i) ,x (j)) (6.16) i i i j j=1 Orinvectornotation,lettingKbethen\u00d7nmatrixwithK = K(x(i),x(j)),we ij have: \u03b2 := \u03b2+\u03b1(y\u2212K\u03b2) Withthealgorithmabove,wecanupdatetherepresentation\u03b2ofthevector\u03b8 efficientlywithO(n2)timeperupdate.Finally,weneedtoshowthattheknowl- edgeoftherepresentation\u03b2sufficestocomputetheprediction\u03b8(cid:62)\u03c6(x).Indeed, wehave: n n \u03b8 (cid:62) \u03c6(x) = \u2211 \u03b2 \u03c6(x (i))(cid:62) \u03c6(x) = \u2211 \u03b2 K(x (i) ,x) (6.17) i i i=1 i=1 2021-05-2300:18:27-07:",
      "chunk_id": 84,
      "start_pos": 80076,
      "end_pos": 81276,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "heknowl- edgeoftherepresentation\u03b2sufficestocomputetheprediction\u03b8(cid:62)\u03c6(x).Indeed, wehave: n n \u03b8 (cid:62) \u03c6(x) = \u2211 \u03b2 \u03c6(x (i))(cid:62) \u03c6(x) = \u2211 \u03b2 K(x (i) ,x) (6.17) i i i=1 i=1 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 6.4. properties of kernels 51 Youmayrealizethatfundamentallyallweneedtoknowaboutthefeaturemap \u03c6(\u00b7)isencapsulatedinthecorrespondingkernelfunctionK(\u00b7,\u00b7).Wewillexpand onthisinthenextsection. 6.4 Propertiesofkernels Inthelastsubsection,westartedwithanexplicitlydefinedfeaturemap\u03c6,which inducesthekernelfunctionK(x,z)(cid:44)(cid:104)\u03c6(x),\u03c6(z)(cid:105).Thenwesawthatthekernel functionissointrinsicsothataslongasthekernelfunctionisdefined,thewhole trainingalgorithmcanbewrittenentirelyinthelanguageofthekernelwithout referringtothefeaturemap\u03c6,socanthepredictionofatestexamplex(equation 6.17.) Therefore,itwouldbetemptingtodefineotherkernelfunctionsK(\u00b7,\u00b7)and runthealgorithm6.16.Notethatthealgorithm6.16doesnotneedtoexplicitly accessthefeaturemap\u03c6,andthereforeweonlyneedtoensuretheexistenceof thefeaturemap \u03c6,butdonotnecessarilyneedtobeabletoexplicitlywrite \u03c6 down.",
      "chunk_id": 85,
      "start_pos": 81076,
      "end_pos": 82184,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "algorithm6.16.Notethatthealgorithm6.16doesnotneedtoexplicitly accessthefeaturemap\u03c6,andthereforeweonlyneedtoensuretheexistenceof thefeaturemap \u03c6,butdonotnecessarilyneedtobeabletoexplicitlywrite \u03c6 down. WhatkindsoffunctionsK(\u00b7,\u00b7)cancorrespondtosomefeaturemap\u03c6?Inother words,canwetellifthereissomefeaturemapping\u03c6sothatK(x,z) = \u03c6(x)(cid:62)\u03c6(z) forallx,z? Ifwecananswerthisquestionbygivingaprecisecharacterizationofvalid kernelfunctions,thenwecancompletelychangetheinterfaceofselectingfeature maps\u03c6totheinterfaceofselectingkernelfunctionK.Concretely,wecanpicka functionK,verifythatitsatisfiesthecharacterization(sothatthereexistsafeature map\u03c6thatKcorrespondsto),andthenwecanrunupdaterule6.16.Thebenefit hereisthatwedon\u2019thavetobeabletocompute\u03c6orwriteitdownanalytically, andweonlyneedtoknowitsexistence.Wewillanswerthisquestionattheend ofthissubsectionafterwegothroughseveralconcreteexamplesofkernels. Supposex,z \u2208Rd,andlet\u2019sfirstconsiderthefunctionK(\u00b7,\u00b7)definedas: K(x,z) = (x (cid:62) z)2 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 52 chapter 6.",
      "chunk_id": 86,
      "start_pos": 81984,
      "end_pos": 83054,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "oncreteexamplesofkernels. Supposex,z \u2208Rd,andlet\u2019sfirstconsiderthefunctionK(\u00b7,\u00b7)definedas: K(x,z) = (x (cid:62) z)2 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 52 chapter 6. kernel methods Wecanalsowritethisas (cid:32) (cid:33)(cid:32) (cid:33) d d \u2211 \u2211 K(x,z) = x z x z i i j j i=1 j=1 d d \u2211 \u2211 = x x z z i j i j i=1j=1 d \u2211 = (x x )(z z ) i j i j i,j=1 Thus,weseethatK(x,z) = (cid:104)\u03c6(x),\u03c6(z)(cid:105)isthekernelfunctionthatcorresponds tothethefeaturemapping\u03c6given(shownhereforthecaseofd =3)by \uf8ee \uf8f9 x x 1 1 \uf8efx x \uf8fa \uf8ef 1 2\uf8fa \uf8ef \uf8fa \uf8efx 1 x 3\uf8fa \uf8ef \uf8fa \uf8efx x \uf8fa \uf8ef 2 1\uf8fa \u03c6(x) = \uf8efx x \uf8fa. \uf8ef 2 2\uf8fa \uf8ef \uf8fa \uf8efx 2 x 3\uf8fa \uf8ef \uf8fa \uf8efx x \uf8fa \uf8ef 3 1\uf8fa \uf8efx x \uf8fa \uf8f0 3 2\uf8fb x x 3 3 Revisitingthecomputationalefficiencyperspectiveofkernel,notethatwhereas calculatingthehigh-dimensional\u03c6(x)requiresO(d2)time,findingK(x,z)takes onlyO(d)time\u2014linearinthedimensionoftheinputattributes. Foranotherrelatedexample,alsoconsiderK(\u00b7,\u00b7)definedby K(x,z) = (x (cid:62) z+c)2 d d (cid:16)\u221a (cid:17)(cid:16)\u221a (cid:17) = \u2211 (x x )(z z )+ \u2211 2cx 2cz +c2. i j i j i i i,j=1 i=1 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 6.4.",
      "chunk_id": 87,
      "start_pos": 82854,
      "end_pos": 83960,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "K(x,z) = (x (cid:62) z+c)2 d d (cid:16)\u221a (cid:17)(cid:16)\u221a (cid:17) = \u2211 (x x )(z z )+ \u2211 2cx 2cz +c2. i j i j i i i,j=1 i=1 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 6.4. properties of kernels 53 (Checkthisyourself.)ThisfunctionKisakernelfunctionthatcorrespondsto thefeaturemapping(againshownford =3) \uf8ee \uf8f9 x x 1 1 \uf8ef x x \uf8fa \uf8ef 1 2 \uf8fa \uf8ef \uf8fa \uf8ef x 1 x 3 \uf8fa \uf8ef \uf8fa \uf8ef x x \uf8fa \uf8ef 2 1 \uf8fa \uf8ef x x \uf8fa \uf8ef 2 2 \uf8fa \uf8ef \uf8fa \uf8ef x 2 x 3 \uf8fa \uf8ef \uf8fa \u03c6(x) = \uf8ef x x \uf8fa, \uf8ef 3 1 \uf8fa \uf8ef x x \uf8fa \uf8ef 3 2 \uf8fa \uf8ef \uf8fa \uf8ef\u221a x 3 x 3 \uf8fa \uf8ef \uf8fa \uf8ef 2cx \uf8fa \uf8ef\u221a 1\uf8fa \uf8ef 2cx \uf8fa \uf8ef\u221a 2\uf8fa \uf8ef \uf8fa \uf8f0 2cx 3\uf8fb c andtheparameterccontrolstherelativeweightingbetweenthex (firstorder) i andthex x (secondorder)terms. i j Morebroadly,thekernelK(x,z) = (x(cid:62)z+c)k correspondstoafeaturemap- ping to an (d+k) feature space, corresponding of all monomials of the form k x x \u00b7\u00b7\u00b7x that are up to order k. However, despite working in this O(dk)- i1 i2 ik dimensionalspace,computingK(x,z)stilltakesonlyO(d)time,andhencewe neverneedtoexplicitlyrepresentfeaturevectorsinthisveryhighdimensional featurespace. Kernels as similarity metrics.",
      "chunk_id": 88,
      "start_pos": 83760,
      "end_pos": 84811,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "his O(dk)- i1 i2 ik dimensionalspace,computingK(x,z)stilltakesonlyO(d)time,andhencewe neverneedtoexplicitlyrepresentfeaturevectorsinthisveryhighdimensional featurespace. Kernels as similarity metrics. Now, let\u2019s talk about a slightly different view ofkernels.Intuitively,(andtherearethingswrongwiththisintuition,butnev- ermind), if \u03c6(x) and \u03c6(z) are close together, then we might expect K(x,z) = \u03c6(x)(cid:62)\u03c6(z) to be large. Conversely, if \u03c6(x) and \u03c6(z) are far apart\u2014 say nearly orthogonaltoeachother\u2014thenK(x,z) = \u03c6(x)(cid:62)\u03c6(z)willbesmall.So,wecan thinkof K(x,z)assomemeasurementofhowsimilarare \u03c6(x)and \u03c6(z),orof howsimilararexandz. Giventhisintuition,supposethatforsomelearningproblemthatyou\u2019rework- ingon,you\u2019vecomeupwithsomefunction K(x,z) thatyouthinkmightbea toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 54 chapter 6. kernel methods reasonablemeasureofhowsimilarxandzare.Forinstance,perhapsyouchose (cid:18) (cid:107)x\u2212z(cid:107)2(cid:19) K(x,z) =exp \u2212 . 2\u03c32 Thisisareasonablemeasureofxandz\u2019ssimilarity,andiscloseto1whenxandz areclose,andnear0whenxandzarefarapart.Doesthereexistafeaturemap \u03c6 such that the kernel K defined above satisfies K(x,z) = \u03c6(x)(cid:62)\u03c6(z",
      "chunk_id": 89,
      "start_pos": 84611,
      "end_pos": 85811,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "isareasonablemeasureofxandz\u2019ssimilarity,andiscloseto1whenxandz areclose,andnear0whenxandzarefarapart.Doesthereexistafeaturemap \u03c6 such that the kernel K defined above satisfies K(x,z) = \u03c6(x)(cid:62)\u03c6(z)? In this particularexample,theanswerisyes.ThiskerneliscalledtheGaussiankernel, andcorrespondstoaninfinitedimensionalfeaturemapping\u03c6.Wewillgivea precisecharacterizationaboutwhatpropertiesafunctionKneedstosatisfyso thatitcanbeavalidkernelfunctionthatcorrespondstosomefeaturemap\u03c6. Necessary conditions for valid kernels. Suppose for now that K is indeed a validkernelcorrespondingtosomefeaturemapping\u03c6,andwewillfirstseewhat propertiesitsatisfies.Now,considersomefinitesetofnpoints(notnecessarily the training set) {x(1),...,x(n)}, and let a square, n-by-n matrix K be defined so that its (i,j)-entry is given by K = K(x(i),x(j)). This matrix is called the ij kernelmatrix.Notethatwe\u2019veoverloadedthenotationandused K todenote boththekernelfunctionK(x,z)andthekernelmatrixK,duetotheirobvious closerelationship. Now, if K is a valid kernel, then K = K(x(i),x(j)) = \u03c6(x(i))(cid:62)\u03c6(x(j)) = ij \u03c6(x(j))(cid:62)\u03c6(x(i)) = K(x(j),x(i)) = K , and hence K must be symmetric.",
      "chunk_id": 90,
      "start_pos": 85611,
      "end_pos": 86774,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "matrixK,duetotheirobvious closerelationship. Now, if K is a valid kernel, then K = K(x(i),x(j)) = \u03c6(x(i))(cid:62)\u03c6(x(j)) = ij \u03c6(x(j))(cid:62)\u03c6(x(i)) = K(x(j),x(i)) = K , and hence K must be symmetric. More- ji over,letting\u03c6 (x)denotethek-thcoordinateofthevector\u03c6(x),wefindthatfor k anyvectorz,wehave z (cid:62) Kz = \u2211\u2211 z K z i ij j i j = \u2211\u2211 z \u03c6(x (i))(cid:62) \u03c6(x (j))z i j i j = \u2211\u2211 z \u2211 \u03c6 (x (i))\u03c6 (x (j))z i k k j i j k = \u2211\u2211\u2211 z \u03c6 (x (i))\u03c6 (x (j))z i k k j k i j (cid:32) (cid:33)2 = \u2211 \u2211 z \u03c6 (x (i)) i k k i \u22650. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 6.4. properties of kernels 55 Thesecond-to-laststepusesthefactthat\u2211 a a = (\u2211 a )2 for a = z \u03c6 (x(i)). i,j i j i i i i k Sincezwasarbitrary,thisshowsthatKispositivesemi-definite(K \u22650). Hence,we\u2019veshownthatifKisavalidkernel(i.e.,ifitcorrespondstosome featuremapping\u03c6),thenthecorrespondingkernelmatrixK \u2208Rn\u00d7nissymmetric positivesemidefinite. Sufficient conditions for valid kernels. More generally, the condition above turns out to be not only a necessary, but also a sufficient, condition for K to bea validkernel (alsocalled a Mercerkernel).",
      "chunk_id": 91,
      "start_pos": 86574,
      "end_pos": 87698,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ufficient conditions for valid kernels. More generally, the condition above turns out to be not only a necessary, but also a sufficient, condition for K to bea validkernel (alsocalled a Mercerkernel). Thefollowingresult isdue to Mercer.3 3ManytextspresentMercer\u2019stheo- reminaslightlymorecomplicated form involving L2 functions, but Theorem (Mercer). Let K : Rd\u00d7Rd (cid:55)\u2192 R be given. Then for K to be a valid whentheinputattributestakeval- (Mercer)kernel,itisnecessaryandsufficientthatforany{x(1),...,x(n)},(n < uesinRd,theversiongivenhereis \u221e),thecorrespondingkernelmatrixissymmetricpositivesemi-definite. equivalent. GivenafunctionK,apartfromtryingtofindafeaturemapping\u03c6thatcorre- spondstoit,thistheoremthereforegivesanotherwayoftestingifitisavalid kernel.You\u2019llalsohaveachancetoplaywiththeseideasmoreinproblemset2. Inclass,wealsobrieflytalkedaboutacoupleofotherexamplesofkernels. Forinstance,considerthedigitrecognitionproblem,inwhichgivenanimage (16\u00d716pixels)ofahandwrittendigit(0-9),wehavetofigureoutwhichdigitit was.UsingeitherasimplepolynomialkernelK(x,z) = (x(cid:62)z)k ortheGaussian kernel, support vector machines (SVMs) were able to obtain extremely good performanceonthisproblem.Thiswa",
      "chunk_id": 92,
      "start_pos": 87498,
      "end_pos": 88698,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "gureoutwhichdigitit was.UsingeitherasimplepolynomialkernelK(x,z) = (x(cid:62)z)k ortheGaussian kernel, support vector machines (SVMs) were able to obtain extremely good performanceonthisproblem.Thiswasparticularlysurprisingsincetheinput attributesxwerejust256-dimensionalvectorsoftheimagepixelintensityvalues, andthesystemhadnopriorknowledgeaboutvision,orevenaboutwhichpixels areadjacenttowhichotherones.Anotherexamplethatwebrieflytalkedabout inlecturewasthatiftheobjectsxthatwearetryingtoclassifyarestrings(say,x isalistofaminoacids,whichstrungtogetherformaprotein),thenitseemshard toconstructareasonable,\u2018\u2018small\u2019\u2019setoffeaturesformostlearningalgorithms, especiallyifdifferentstringshavedifferentlengths.However,considerletting \u03c6(x)beafeaturevectorthatcountsthenumberofoccurrencesofeachlength-k substringinx.Ifwe\u2019reconsideringstringsofEnglishletters,thenthereare26k suchstrings.Hence,\u03c6(x)isa26k-dimensionalvector;evenformoderatevalues ofk,thisisprobablytoobigforustoefficientlyworkwith.(e.g.,264 \u2248460000.) However,using(dynamicprogramming-ish)stringmatchingalgorithms,itis toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 56 chapter 6.",
      "chunk_id": 93,
      "start_pos": 88498,
      "end_pos": 89656,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "gforustoefficientlyworkwith.(e.g.,264 \u2248460000.) However,using(dynamicprogramming-ish)stringmatchingalgorithms,itis toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 56 chapter 6. kernel methods possibletoefficientlycomputeK(x,z) = \u03c6(x)(cid:62)\u03c6(z),sothatwecannowimplicitly workinthis26k-dimensionalfeaturespace,butwithouteverexplicitlycomputing featurevectorsinthisspace. Applicationofkernelmethods. We\u2019veseentheapplicationofkernelstolinear regression.Inthenextpart,wewillintroducethesupportvectormachinesto which kernels can be directly applied. We won\u2019t dwell too much longer on it here.Infact,theideaofkernelshassignificantlybroaderapplicabilitythanlinear regressionandSVMs.Specifically,ifyouhaveanylearningalgorithmthatyoucan writeintermsofonlyinnerproducts(cid:104)x,z(cid:105)betweeninputattributevectors,then byreplacingthiswithK(x,z)whereKisakernel,youcan\u2018\u2018magically\u2019\u2019allowyour algorithmtoworkefficientlyinthehighdimensionalfeaturespacecorresponding toK.Forinstance,thiskerneltrickcanbeappliedwiththeperceptrontoderivea kernelperceptronalgorithm.Manyofthealgorithmsthatwe\u2019llseelaterinthis classwillalsobeamenabletothismethod,whichhascometobeknownasthe \u2018\u2018kerneltrick.\u2019\u2019 2",
      "chunk_id": 94,
      "start_pos": 89456,
      "end_pos": 90656,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "kerneltrickcanbeappliedwiththeperceptrontoderivea kernelperceptronalgorithm.Manyofthealgorithmsthatwe\u2019llseelaterinthis classwillalsobeamenabletothismethod,whichhascometobeknownasthe \u2018\u2018kerneltrick.\u2019\u2019 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc Part IV: Support Vector Machines 7 Support vector machines FromCS229Fall2020,TengyuMa, Andrew Ng, Moses Charikar, & ChristopherR\u00e9,StanfordUniver- sity. ThissetofnotespresentstheSupportVectorMachine(SVM)learningal-gorithm. SVMsareamongthebest(andmanybelieveareindeedthebest)\u2018\u2018off-the-shelf\u2019\u2019 supervised learning algorithms. To tell the SVM story, we\u2019ll need to first talk aboutmarginsandtheideaofseparatingdatawithalarge\u2018\u2018gap.\u2019\u2019Next,we\u2019ll talk about the optimal margin classifier, which will lead us into a digression on Lagrange duality. We\u2019ll also see kernels, which give a way to apply SVMs efficientlyinveryhighdimensional(suchasinfinite-dimensional)featurespaces, and finally, we\u2019ll close off the story with the SMO algorithm, which gives an efficientimplementationofSVMs.",
      "chunk_id": 95,
      "start_pos": 90456,
      "end_pos": 91503,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ay to apply SVMs efficientlyinveryhighdimensional(suchasinfinite-dimensional)featurespaces, and finally, we\u2019ll close off the story with the SMO algorithm, which gives an efficientimplementationofSVMs. 7.1 Margins:Intuition We\u2019llstartourstoryonSVMsbytalkingaboutmargins.Thissectionwillgivethe intuitionsaboutmarginsandaboutthe\u2018\u2018confidence\u2019\u2019ofourpredictions;these ideaswillbemadeformalinSection7.3. Considerlogisticregression,wheretheprobability p(y =1| x;\u03b8)ismodeled byh (x) = g(\u03b8(cid:62)x).Wethenpredict\u2018\u20181\u2019\u2019onaninputxifandonlyifh (x) \u22650.5, \u03b8 \u03b8 or equivalently, if and only if \u03b8(cid:62)x \u2265 0. Consider a positive training example (y =1).Thelarger\u03b8(cid:62)xis,thelargeralsoish (x) = p(y =1| x;\u03b8),andthusalso \u03b8 thehigherourdegreeof\u2018\u2018confidence\u2019\u2019thatthelabelis1.Thus,informallywecan thinkofourpredictionasbeingveryconfidentthaty =1if\u03b8(cid:62)x (cid:29)0.Similarly, wethinkoflogisticregressionasconfidentlypredictingy =0,if\u03b8(cid:62)x (cid:28)0.Given atrainingset,againinformallyitseemsthatwe\u2019dhavefoundagoodfittothe trainingdataifwecanfind\u03b8sothat\u03b8(cid:62)x(i) (cid:29)0whenevery(i) =1,and\u03b8(cid:62)x(i) (cid:28)0 whenevery(i) =0,sincethiswouldreflectaveryconfident(andcorrect)setof classificationsforallth",
      "chunk_id": 96,
      "start_pos": 91303,
      "end_pos": 92503,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "oodfittothe trainingdataifwecanfind\u03b8sothat\u03b8(cid:62)x(i) (cid:29)0whenevery(i) =1,and\u03b8(cid:62)x(i) (cid:28)0 whenevery(i) =0,sincethiswouldreflectaveryconfident(andcorrect)setof classificationsforallthetrainingexamples.Thisseemstobeanicegoaltoaim for,andwe\u2019llsoonformalizethisideausingthenotionoffunctionalmargins. 58 chapter 7. support vector machines Foradifferenttypeofintuition,considerthefollowingfigure,inwhichx\u2019s representpositivetrainingexamples,o\u2019sdenotenegativetrainingexamples,a decisionboundary(thisisthelinegivenbytheequation\u03b8(cid:62)x = 0,andisalso calledtheseparatinghyperplane)isalsoshown,andthreepointshavealsobeen labeledA,BandC. NoticethatthepointAisveryfarfromthedecisionboundary.Ifweareasked tomakeapredictionforthevalueofyatA,itseemsweshouldbequiteconfident thaty =1there.Conversely,thepointCisveryclosetothedecisionboundary, andwhileit\u2019sonthesideofthedecisionboundaryonwhichwewouldpredict y = 1,itseemslikelythatjustasmallchangetothedecisionboundarycould easilyhavecausedoutpredictiontobey =0.Hence,we\u2019remuchmoreconfident aboutourpredictionatAthanatC.ThepointBliesin-betweenthesetwocases, andmorebroadly,weseethatifapointisfarfromtheseparatinghyperplane, thenwemaybesignificantl",
      "chunk_id": 97,
      "start_pos": 92303,
      "end_pos": 93503,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ictiontobey =0.Hence,we\u2019remuchmoreconfident aboutourpredictionatAthanatC.ThepointBliesin-betweenthesetwocases, andmorebroadly,weseethatifapointisfarfromtheseparatinghyperplane, thenwemaybesignificantlymoreconfidentinourpredictions.Again,informally wethinkitwouldbeniceif,givenatrainingset,wemanagetofindadecision boundarythatallowsustomakeallcorrectandconfident(meaningfarfromthe decisionboundary)predictionsonthetrainingexamples.We\u2019llformalizethis laterusingthenotionofgeometricmargins. 7.2 Notation TomakeourdiscussionofSVMseasier,we\u2019llfirstneedtointroduceanewno- tationfortalkingaboutclassification.Wewillbeconsideringalinearclassifier forabinaryclassificationproblemwithlabelsyandfeaturesx.Fromnow,we\u2019ll usey \u2208 {\u22121,1}(insteadof{0,1})todenotetheclasslabels.Also,ratherthan parameterizingourlinearclassifierwiththevector\u03b8,wewilluseparametersw,b, andwriteourclassifieras h (x) = g(w (cid:62) x+b). w,b Here,g(z) =1ifz \u22650,andg(z) = \u22121otherwise.This\u2018\u2018w,b\u2019\u2019notationallowsus toexplicitlytreattheintercepttermbseparatelyfromtheotherparameters.(We alsodroptheconventionwehadpreviouslyoflettingx =1beanextracoordinate 0 intheinputfeaturevector.)Thus,btakestheroleofwhatwaspreviously\u03b8 ,and 0 wtakestheroleof",
      "chunk_id": 98,
      "start_pos": 93303,
      "end_pos": 94503,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ttermbseparatelyfromtheotherparameters.(We alsodroptheconventionwehadpreviouslyoflettingx =1beanextracoordinate 0 intheinputfeaturevector.)Thus,btakestheroleofwhatwaspreviously\u03b8 ,and 0 wtakestheroleof[\u03b8 ...\u03b8 ](cid:62). 1 d 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 7.3. functional and geometric margins 59 Notealsothat,fromourdefinitionofgabove,ourclassifierwilldirectlypredict either1or \u22121(cf.theperceptronalgorithm),withoutfirstgoingthroughthe intermediatestepofestimatingp(y =1)(whichiswhatlogisticregressiondoes). 7.3 Functionalandgeometricmargins Let\u2019sformalizethenotionsofthefunctionalandgeometricmargins.Givenatrain- ingexample(x(i),y(i)),wedefinethefunctionalmarginof(w,b)withrespectto thetrainingexampleas \u03b3\u02c6 (i) = y (i)(w (cid:62) x (i)+b). Note that if y(i) = 1, then for the functional margin to be large (i.e., for our predictiontobeconfidentandcorrect),weneedw(cid:62)x(i)+btobealargepositive number.Conversely,ify(i) = \u22121,thenforthefunctionalmargintobelarge,we needw(cid:62)x(i)+btobealargenegativenumber.Moreover,ify(i)(w(cid:62)x(i)+b) >0, thenourpredictiononthisexampleiscorrect.(Checkthisyourself.)Hence,a largefunctionalmarginrepresentsaconfidentanda",
      "chunk_id": 99,
      "start_pos": 94303,
      "end_pos": 95503,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ge,we needw(cid:62)x(i)+btobealargenegativenumber.Moreover,ify(i)(w(cid:62)x(i)+b) >0, thenourpredictiononthisexampleiscorrect.(Checkthisyourself.)Hence,a largefunctionalmarginrepresentsaconfidentandacorrectprediction. Foralinearclassifierwiththechoiceofggivenabove(takingvaluesin{\u22121,1}), there\u2019s one property of the functional margin that makes it not a very good measureofconfidence,however.Givenourchoiceofg,wenotethatifwereplace wwith2wandbwith2b,thensinceg(w(cid:62)x+b) = g(2w(cid:62)x+2b),thiswouldnot change h (x) atall.I.e., g,andhencealso h (x),dependsonlyonthesign, w,b w,b butnotonthemagnitude,ofw(cid:62)x+b.However,replacing(w,b)with(2w,2b) alsoresultsinmultiplyingourfunctionalmarginbyafactorof2.Thus,itseems that by exploiting our freedom to scale w and b, we can make the functional marginarbitrarilylargewithoutreallychanginganythingmeaningful.Intuitively, itmightthereforemakesensetoimposesomesortofnormalizationcondition suchasthat(cid:107)w(cid:107) =1;i.e.,wemightreplace(w,b)with(w/(cid:107)w(cid:107) ,b/(cid:107)w(cid:107) ),and 2 2 2 insteadconsiderthefunctionalmarginof(w/(cid:107)w(cid:107) ,b/(cid:107)w(cid:107) ).We\u2019llcomebackto 2 2 thislater.",
      "chunk_id": 100,
      "start_pos": 95303,
      "end_pos": 96477,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "i.e.,wemightreplace(w,b)with(w/(cid:107)w(cid:107) ,b/(cid:107)w(cid:107) ),and 2 2 2 insteadconsiderthefunctionalmarginof(w/(cid:107)w(cid:107) ,b/(cid:107)w(cid:107) ).We\u2019llcomebackto 2 2 thislater. GivenatrainingsetS = {(x(i),y(i));i =1,...,n},wealsodefinethefunction marginof(w,b)withrespecttoSasthesmallestofthefunctionalmarginsofthe individualtrainingexamples.Denotedby\u03b3\u02c6,thiscanthereforebewritten: \u03b3\u02c6 = min \u03b3\u02c6 (i) i=1,...,n Next,let\u2019stalkaboutgeometricmargins.Considerthepicturebelow: toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 60 chapter 7. support vector machines Thedecisionboundarycorrespondingto(w,b)isshown,alongwiththevector w.Notethatwisorthogonal(at90\u25e6)totheseparatinghyperplane.(Youshould convince yourself that this must be the case.) Consider the point at A, which representstheinputx(i)ofsometrainingexamplewithlabely(i) =1.Itsdistance tothedecisionboundary,\u03b3(i),isgivenbythelinesegmentAB. Howcanwefindthevalueof\u03b3(i)?Well,w/(cid:107)w(cid:107)isaunit-lengthvectorpointing inthesamedirectionasw.SinceArepresentsx(i),wethereforefindthatthepoint Bisgivenbyx(i)\u2212\u03b3(i)\u00b7w/(cid:107)w(cid:107).Butthispointliesonthedecisionboundary,and allpointsxonthedecisionb",
      "chunk_id": 101,
      "start_pos": 96277,
      "end_pos": 97477,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "lengthvectorpointing inthesamedirectionasw.SinceArepresentsx(i),wethereforefindthatthepoint Bisgivenbyx(i)\u2212\u03b3(i)\u00b7w/(cid:107)w(cid:107).Butthispointliesonthedecisionboundary,and allpointsxonthedecisionboundarysatisfytheequationw(cid:62)x+b =0.Hence, (cid:18) (cid:19) w w (cid:62) x (i)\u2212\u03b3 (i) +b =0. (cid:107)w(cid:107) Solvingfor\u03b3(i) yields w(cid:62)x(i)+b (cid:18) w (cid:19)(cid:62) b \u03b3 (i) = = x (i)+ . (cid:107)w(cid:107) (cid:107)w(cid:107) (cid:107)w(cid:107) ThiswasworkedoutforthecaseofapositivetrainingexampleatAinthefigure, where being on the \u2018\u2018positive\u2019\u2019 side of the decision boundary is good. More generally, we define the geometric margin of (w,b) with respect to a training example(x(i),y(i))tobe (cid:32)(cid:18) (cid:19)(cid:62) (cid:33) w b \u03b3 (i) = y (i) x (i)+ . (cid:107)w(cid:107) (cid:107)w(cid:107) Notethatif(cid:107)w(cid:107) =1,thenthefunctionalmarginequalsthegeometricmargin\u2014 thisthusgivesusawayofrelatingthesetwodifferentnotionsofmargin.Also, thegeometricmarginisinvarianttorescalingoftheparameters;i.e.,ifwereplace wwith2wandbwith2b,thenthegeometricmargindoesnotchange.Thiswill infactcomeinhandylater.Specifically,becauseofthisinvariancetothescaling oftheparameters,whentr",
      "chunk_id": 102,
      "start_pos": 97277,
      "end_pos": 98477,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "orescalingoftheparameters;i.e.,ifwereplace wwith2wandbwith2b,thenthegeometricmargindoesnotchange.Thiswill infactcomeinhandylater.Specifically,becauseofthisinvariancetothescaling oftheparameters,whentryingtofit w and b totrainingdata,wecanimpose anarbitraryscalingconstraintonwwithoutchanginganythingimportant;for instance,wecandemandthat(cid:107)w(cid:107) =1,or|w | =5,or|w +b|+|w | =2,and 1 1 2 anyofthesecanbesatisfiedsimplybyrescalingwandb. Finally,givenatrainingsetS = {(x(i),y(i));i = 1,...,n},wealsodefinethe geometricmarginof(w,b)withrespecttoStobethesmallestofthegeometric marginsontheindividualtrainingexamples: \u03b3 = min \u03b3 (i) . i=1,...,n 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 7.4. the optimal margin classifier 61 7.4 Theoptimalmarginclassifier Givenatrainingset,itseemsfromourpreviousdiscussionthatanaturaldesider- atumistotrytofindadecisionboundarythatmaximizesthe(geometric)margin, sincethiswouldreflectaveryconfidentsetofpredictionsonthetrainingsetand agood\u2018\u2018fit\u2019\u2019tothetrainingdata.Specifically,thiswillresultinaclassifierthat separatesthepositiveandthenegativetrainingexampleswitha\u2018\u2018gap\u2019\u2019(geometric margin).",
      "chunk_id": 103,
      "start_pos": 98277,
      "end_pos": 99433,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "onfidentsetofpredictionsonthetrainingsetand agood\u2018\u2018fit\u2019\u2019tothetrainingdata.Specifically,thiswillresultinaclassifierthat separatesthepositiveandthenegativetrainingexampleswitha\u2018\u2018gap\u2019\u2019(geometric margin). Fornow,wewillassumethatwearegivenatrainingsetthatislinearlysep- arable; i.e., that it is possible to separate the positive and negative examples usingsomeseparatinghyperplane.Howwillwefindtheonethatachievesthe maximumgeometricmargin?Wecanposethefollowingoptimizationproblem: max \u03b3 \u03b3,w,b s.t. y (i)(w (cid:62) x (i)+b) \u2265 \u03b3, i =1,...,n (cid:107)w(cid:107) =1. I.e.,wewanttomaximize\u03b3,subjecttoeachtrainingexamplehavingfunctional marginatleast\u03b3.The(cid:107)w(cid:107) =1constraintmoreoverensuresthatthefunctional marginequalstothegeometricmargin,sowearealsoguaranteedthatallthe geometricmarginsareatleast\u03b3.Thus,solvingthisproblemwillresultin(w,b) withthelargestpossiblegeometricmarginwithrespecttothetrainingset. If we could solve the optimization problem above, we\u2019d be done. But the \u2018\u2018(cid:107)w(cid:107) = 1\u2019\u2019constraintisanasty(non-convex)one,andthisproblemcertainly isn\u2019tinanyformatthatwecanplugintostandardoptimizationsoftwaretosolve.",
      "chunk_id": 104,
      "start_pos": 99233,
      "end_pos": 100369,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "mization problem above, we\u2019d be done. But the \u2018\u2018(cid:107)w(cid:107) = 1\u2019\u2019constraintisanasty(non-convex)one,andthisproblemcertainly isn\u2019tinanyformatthatwecanplugintostandardoptimizationsoftwaretosolve. So,let\u2019strytransformingtheproblemintoanicerone.Consider: \u03b3\u02c6 max \u03b3\u02c6,w,b (cid:107)w(cid:107) s.t. y (i)(w (cid:62) x (i)+b) \u2265 \u03b3\u02c6, i =1,...,n Here,we\u2019regoingtomaximize\u03b3\u02c6/(cid:107)w(cid:107),subjecttothefunctionalmarginsallbeing atleast\u03b3\u02c6.Sincethegeometricandfunctionalmarginsarerelatedby\u03b3 = \u03b3\u02c6/(cid:107)w(cid:107), thiswillgiveustheanswerwewant.Moreover,we\u2019vegottenridoftheconstraint (cid:107)w(cid:107) =1thatwedidn\u2019tlike.Thedownsideisthatwenowhaveanasty(again, \u03b3\u02c6 non-convex) objective function; and, we still don\u2019t have any off-the-shelf (cid:107)w(cid:107) softwarethatcansolvethisformofanoptimizationproblem. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 62 chapter 7. support vector machines Let\u2019skeepgoing.Recallourearlierdiscussionthatwecanaddanarbitrary scalingconstraintonwandbwithoutchanginganything.Thisisthekeyidea we\u2019llusenow.Wewillintroducethescalingconstraintthatthefunctionalmargin ofw,bwithrespecttothetrainingsetmustbe1: \u03b3\u02c6 =1 Sincemultiplyingwandbbysomecon",
      "chunk_id": 105,
      "start_pos": 100169,
      "end_pos": 101369,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "aintonwandbwithoutchanginganything.Thisisthekeyidea we\u2019llusenow.Wewillintroducethescalingconstraintthatthefunctionalmargin ofw,bwithrespecttothetrainingsetmustbe1: \u03b3\u02c6 =1 Sincemultiplyingwandbbysomeconstantresultsinthefunctionalmarginbeing multipliedbythatsameconstant,thisisindeedascalingconstraint,andcanbe satisfiedbyrescalingw,b.Pluggingthisintoourproblemabove,andnotingthat maximizing \u03b3\u02c6/(cid:107)w(cid:107) = 1/(cid:107)w(cid:107) is the same thing as minimizing (cid:107)w(cid:107)2, we now havethefollowingoptimizationproblem: 1 min (cid:107)w(cid:107)2 w,b 2 s.t. y (i)(w (cid:62) x (i)+b) \u22651, i =1,...,n We\u2019venowtransformedtheproblemintoaformthatcanbeefficientlysolved. The above is an optimization problem with a convex quadratic objective and onlylinearconstraints.Itssolutiongivesustheoptimalmarginclassifier.This optimizationproblemcanbesolvedusingcommercialquadraticprogramming (QP)code.1 1You may be familiar with lin- Whilewecouldcalltheproblemsolvedhere,whatwewillinsteaddoismake ear programming, which solves optimizationproblemsthathave adigressiontotalkaboutLagrangeduality.Thiswillleadustoouroptimization linear objectives and linear con- problem\u2019sdualform,whichwillplayakeyrolei",
      "chunk_id": 106,
      "start_pos": 101169,
      "end_pos": 102369,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "programming, which solves optimizationproblemsthathave adigressiontotalkaboutLagrangeduality.Thiswillleadustoouroptimization linear objectives and linear con- problem\u2019sdualform,whichwillplayakeyroleinallowingustousekernelsto straints.QPsoftwareisalsowidely available, which allows convex getoptimalmarginclassifierstoworkefficientlyinveryhighdimensionalspaces. quadraticobjectivesandlinearcon- Thedualformwillalsoallowustoderiveanefficientalgorithmforsolvingthe straints. aboveoptimizationproblemthatwilltypicallydomuchbetterthangenericQP software. 7.5 Lagrangeduality(optionalreading) Let\u2019stemporarilyputasideSVMsandmaximummarginclassifiers,andtalkabout solvingconstrainedoptimizationproblems.Consideraproblemofthefollowing form: min f(w) w s.t. h (w) =0, i =1,...,l. i 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 7.5. lagrange duality (optional reading) 63 SomeofyoumayrecallhowthemethodofLagrangemultiplierscanbeusedto solveit.(Don\u2019tworryifyouhaven\u2019tseenitbefore.)Inthismethod,wedefinethe Lagrangiantobe l \u2211 L(w,\u03b2) = f(w)+ \u03b2 h (w) i i i=1 Here,the\u03b2 \u2019sarecalledtheLagrangemultipliers.Wewouldthenfindandset i L\u2019spartialderivativestozero: \u2202L \u2202L =0; =0, \u2202w \u2202\u03b2 i i andsolve",
      "chunk_id": 107,
      "start_pos": 102169,
      "end_pos": 103369,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ismethod,wedefinethe Lagrangiantobe l \u2211 L(w,\u03b2) = f(w)+ \u03b2 h (w) i i i=1 Here,the\u03b2 \u2019sarecalledtheLagrangemultipliers.Wewouldthenfindandset i L\u2019spartialderivativestozero: \u2202L \u2202L =0; =0, \u2202w \u2202\u03b2 i i andsolveforwand\u03b2. Inthissection,wewillgeneralizethistoconstrainedoptimizationproblems in which we may have inequality as well as equality constraints. Due to time constraints,wewon\u2019treallybeabletodothetheoryofLagrangedualityjusticein thisclass,2butwewillgivethemainideasandresults,whichwewillthenapply 2Readers interested in learning toouroptimalmarginclassifier\u2019soptimizationproblem. moreaboutthistopicareencour- agedtoread,e.g.,R.T.Rockarfeller Considerthefollowing,whichwe\u2019llcalltheprimaloptimizationproblem: (1970),ConvexAnalysis,Princeton UniversityPress. min f(w) w s.t. g(w) \u22640, i =1,...,k i h (w) =0, i =1,...,l. i Tosolveit,westartbydefiningthegeneralizedLagrangian k l \u2211 \u2211 L(w,\u03b1,\u03b2) = f(w)+ \u03b1 g(w)+ \u03b2 h (w). i i i i i=1 i=1 Here,the\u03b1\u2019sand\u03b2 \u2019saretheLagrangemultipliers.Considerthequantity i i \u03b8P (w) = max L(w,\u03b1,\u03b2).",
      "chunk_id": 108,
      "start_pos": 103169,
      "end_pos": 104184,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "Tosolveit,westartbydefiningthegeneralizedLagrangian k l \u2211 \u2211 L(w,\u03b1,\u03b2) = f(w)+ \u03b1 g(w)+ \u03b2 h (w). i i i i i=1 i=1 Here,the\u03b1\u2019sand\u03b2 \u2019saretheLagrangemultipliers.Considerthequantity i i \u03b8P (w) = max L(w,\u03b1,\u03b2). \u03b1,\u03b2:\u03b1i \u22650 Here,the\u2018\u2018P\u2019\u2019subscriptstandsfor\u2018\u2018primal.\u2019\u2019Letsomewbegiven.Ifwviolates anyoftheprimalconstraints(i.e.,ifeither g(w) > 0orh (w) (cid:54)= 0forsomei), i i thenyoushouldbeabletoverifythat k l \u2211 \u2211 \u03b8P (w) = max f(w)+ \u03b1 i g i (w)+ \u03b2 i h i (w) \u03b1,\u03b2:\u03b1i \u22650 i=1 i=1 = \u221e. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 64 chapter 7. support vector machines Conversely,iftheconstraintsareindeedsatisfiedforaparticularvalueofw,then \u03b8P (w) = f(w).Hence, \uf8f1 \uf8f2f(w) ifwsatisfiesprimalconstraints \u03b8P (w) = \uf8f3 \u221e otherwise. Thus,\u03b8P takesthesamevalueastheobjectiveinourproblemforallvaluesofw thatsatisfiestheprimalconstraints,andispositiveinfinityiftheconstraintsare violated.Hence,ifweconsidertheminimizationproblem min\u03b8P (w) =min max L(w,\u03b1,\u03b2), w w \u03b1,\u03b2:\u03b1i \u22650 weseethatitisthesameproblem(i.e.,andhasthesamesolutionsas)ouroriginal, primalproblem.Forlateruse,wealsodefinetheoptimalvalueoftheobjectiveto be p\u2217 =min w \u03b8P (w);wecallthisthevalueoftheprimalproblem.",
      "chunk_id": 109,
      "start_pos": 103984,
      "end_pos": 105153,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "0 weseethatitisthesameproblem(i.e.,andhasthesamesolutionsas)ouroriginal, primalproblem.Forlateruse,wealsodefinetheoptimalvalueoftheobjectiveto be p\u2217 =min w \u03b8P (w);wecallthisthevalueoftheprimalproblem. Now,let\u2019slookataslightlydifferentproblem.Wedefine \u03b8D (\u03b1,\u03b2) =minL(w,\u03b1,\u03b2). w Here,the\u2018\u2018D\u2019\u2019subscriptstandsfor\u2018\u2018dual.\u2019\u2019Notealsothatwhereasinthedefi- nitionof\u03b8P wewereoptimizing(maximizing)withrespectto\u03b1,\u03b2,hereweare minimizingwithrespecttow. Wecannowposethedualoptimizationproblem: max \u03b8D (\u03b1,\u03b2) = max minL(w,\u03b1,\u03b2). \u03b1,\u03b2:\u03b1i \u22650 \u03b1,\u03b2:\u03b1i \u22650 w Thisisexactlythesameasourprimalproblemshownabove,exceptthattheorder ofthe\u2018\u2018max\u2019\u2019andthe\u2018\u2018min\u2019\u2019arenowexchanged.Wealsodefinetheoptimalvalue ofthedualproblem\u2019sobjectivetobed\u2217 =max \u03b1,\u03b2:\u03b1i \u22650 \u03b8D (w). Howaretheprimalandthedualproblemsrelated?Itcaneasilybeshownthat d \u2217 = max minL(w,\u03b1,\u03b2) \u2264min max L(w,\u03b1,\u03b2) = p \u2217 . \u03b1,\u03b2:\u03b1i \u22650 w w \u03b1,\u03b2:\u03b1i \u22650 (You should convince yourself of this; this follows from the \u2018\u2018max min\u2019\u2019 of a function alwaysbeing less than or equal tothe \u2018\u2018min max.\u2019\u2019) However, under certainconditions,wewillhave d \u2217 = p \u2217 , 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 7.6.",
      "chunk_id": 110,
      "start_pos": 104953,
      "end_pos": 106085,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\u2019\u2019 of a function alwaysbeing less than or equal tothe \u2018\u2018min max.\u2019\u2019) However, under certainconditions,wewillhave d \u2217 = p \u2217 , 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 7.6. optimal margin classifiers 65 sothatwecansolvethedualprobleminlieuoftheprimalproblem.Let\u2019ssee whattheseconditionsare. Suppose f andtheg i \u2019sareconvex,3andtheh i \u2019sareaffine.4Supposefurther 3When f hasaHessian,thenitis thattheconstraintsg are(strictly)feasible;thismeansthatthereexistssomew convexifandonlyiftheHessianis i positivesemi-definite.Forinstance, sothatg i (w) <0foralli. f(w) = w(cid:62)wisconvex;similarly, Under our above assumptions, there must exist w\u2217,\u03b1\u2217,\u03b2\u2217 so that w\u2217 is the alllinear(andaffine)functionsare solutiontotheprimalproblem,\u03b1\u2217,\u03b2\u2217 arethesolutiontothedualproblem,and alsoconvex.(Afunction f canalso beconvexwithoutbeingdifferen- moreover p\u2217 = d\u2217 = L(w\u2217,\u03b1\u2217,\u03b2\u2217).Moreover,w\u2217,\u03b1\u2217 and \u03b2\u2217 satisfytheKarush- tiable, but we won\u2019t need those Kuhn-Tucker(KKT)conditions,whichareasfollows: moregeneraldefinitionsofconvex- ityhere.) \u2202 \u2202 w L(w \u2217 ,\u03b1 \u2217 ,\u03b2 \u2217) =0, i =1,...,d (7.1) 4 hi I ( .e w ., )= the a r i (cid:62) e w e + xi b st i.",
      "chunk_id": 111,
      "start_pos": 105885,
      "end_pos": 107033,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "need those Kuhn-Tucker(KKT)conditions,whichareasfollows: moregeneraldefinitionsofconvex- ityhere.) \u2202 \u2202 w L(w \u2217 ,\u03b1 \u2217 ,\u03b2 \u2217) =0, i =1,...,d (7.1) 4 hi I ( .e w ., )= the a r i (cid:62) e w e + xi b st i. s \u2018\u2018A ai ffi ,b n i, e\u2019 s \u2019 o me t a h n a s t i thesamethingaslinear,exceptthat \u2202 L(w \u2217 ,\u03b1 \u2217 ,\u03b2 \u2217) =0, i =1,...,l (7.2) wealsoallowtheextraintercept \u2202\u03b2 i termbi. \u03b1 \u2217 g(w \u2217) =0, i =1,...,k (7.3) i i g(w \u2217) \u22640, i =1,...,k (7.4) i \u03b1 \u2217 \u22650, i =1,...,k (7.5) Moreover,ifsomew\u2217,\u03b1\u2217,\u03b2\u2217 satisfytheKKTconditions,thenitisalsoasolution totheprimalanddualproblems. WedrawattentiontoEquation(7.3),whichiscalledtheKKTdualcomple- mentaritycondition.Specifically,itimpliesthatif\u03b1\u2217 >0,theng(w\u2217) =0.(I.e., i i the\u2018\u2018g(w) \u22640\u2019\u2019constraintisactive,meaningitholdswithequalityratherthan i withinequality.)Lateron,thiswillbekeyforshowingthattheSVMhasonlya smallnumberof\u2018\u2018supportvectors\u2019\u2019;theKKTdualcomplementaritycondition willalsogiveusourconvergencetestwhenwetalkabouttheSMOalgorithm. 7.6 Optimalmarginclassifiers Note:Theequivalenceofoptimizationproblem7.6andtheoptimizationproblem7.11, andtherelationshipbetweentheprimaryanddualvariablesinequation7.8arethemost importanttakehomemessagesofthissection.",
      "chunk_id": 112,
      "start_pos": 106833,
      "end_pos": 108011,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "classifiers Note:Theequivalenceofoptimizationproblem7.6andtheoptimizationproblem7.11, andtherelationshipbetweentheprimaryanddualvariablesinequation7.8arethemost importanttakehomemessagesofthissection. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 66 chapter 7. support vector machines Previously,weposedthefollowing(primal)optimizationproblemforfinding theoptimalmarginclassifier: 1 min (cid:107)w(cid:107)2 (7.6) w,b 2 s.t. y (i)(w (cid:62) x (i)+b) \u22651, i =1,...,n (7.7) Wecanwritetheconstraintsas g(w) = \u2212y (i)(w (cid:62) x (i)+b)+1\u22640. i Wehaveonesuchconstraintforeachtrainingexample.NotethatfromtheKKT dualcomplementaritycondition,wewillhave\u03b1 >0onlyforthetrainingexam- i plesthathavefunctionalmarginexactlyequaltoone(i.e.,theonescorresponding toconstraintsthatholdwithequality,g(w) =0).Considerthefigurebelow,in i whichamaximummarginseparatinghyperplaneisshownbythesolidline. Thepointswiththesmallestmarginsareexactlytheonesclosesttothedeci- sionboundary;here,thesearethethreepoints(onenegativeandtwopositive examples)thatlieonthedashedlinesparalleltothedecisionboundary.Thus, onlythreeofthe \u03b1\u2019s\u2014namely,theonescorrespondingtothesethreetraining i examples\u2014willbenon-zeroat",
      "chunk_id": 113,
      "start_pos": 107811,
      "end_pos": 109011,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "threepoints(onenegativeandtwopositive examples)thatlieonthedashedlinesparalleltothedecisionboundary.Thus, onlythreeofthe \u03b1\u2019s\u2014namely,theonescorrespondingtothesethreetraining i examples\u2014willbenon-zeroattheoptimalsolutiontoouroptimizationproblem. Thesethreepointsarecalledthesupportvectorsinthisproblem.Thefactthat thenumberofsupportvectorscanbemuchsmallerthanthesizethetrainingset willbeusefullater. Let\u2019smoveon.Lookingahead,aswedevelopthedualformoftheproblem, onekeyideatowatchoutforisthatwe\u2019lltrytowriteouralgorithmintermsof onlytheinnerproduct(cid:104)x(i),x(j)(cid:105)(thinkofthisas(x(i))(cid:62)x(j))betweenpointsin theinputfeaturespace.Thefactthatwecanexpressouralgorithmintermsof theseinnerproductswillbekeywhenweapplythekerneltrick. WhenweconstructtheLagrangianforouroptimizationproblemwehave: L(w,b,\u03b1) = 1 (cid:107)w(cid:107)2\u2212 \u2211 n \u03b1 (cid:104) y (i)(w (cid:62) x (i)+b)\u22121 (cid:105) . i 2 i=1 Notethatthere\u2019reonly\u2018\u2018\u03b1\u2019\u2019butno\u2018\u2018\u03b2 \u2019\u2019Lagrangemultipliers,sincetheproblem i i hasonlyinequalityconstraints. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 7.6.",
      "chunk_id": 114,
      "start_pos": 108811,
      "end_pos": 109893,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "id:105) . i 2 i=1 Notethatthere\u2019reonly\u2018\u2018\u03b1\u2019\u2019butno\u2018\u2018\u03b2 \u2019\u2019Lagrangemultipliers,sincetheproblem i i hasonlyinequalityconstraints. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 7.6. optimal margin classifiers 67 Let\u2019sfindthedualformoftheproblem.Todoso,weneedtofirstminimize L(w,b,\u03b1)withrespecttowandb(forfixed\u03b1),toget\u03b8D,whichwe\u2019lldobysetting thederivativesofLwithrespecttowandbtozero.Wehave: n \u2207 L(w,b,\u03b1) = w\u2212 \u2211 \u03b1 y (i) x (i) =0 w i i=1 Thisimpliesthat n w = \u2211 \u03b1 y (i) x (i) . (7.8) i i=1 Asforthederivativewithrespecttob,weobtain \u2202 L(w,b,\u03b1) = \u2211 n \u03b1 y (i) =0. (7.9) i \u2202b i=1 Ifwetakethedefinitionof w inEquation(7.8)andplugthatbackintothe Lagrangian(Section7.6),andsimplify,weget L(w,b,\u03b1) = \u2211 n \u03b1 \u2212 1 \u2211 n y (i) y (j) \u03b1 \u03b1 (x (i))(cid:62) x (j)\u2212b \u2211 n \u03b1 y (i) . (7.10) i i j i 2 i=1 i,j=1 i=1 ButfromEquation(7.9),thelasttermmustbezero,soweobtain L(w,b,\u03b1) = \u2211 n \u03b1 \u2212 1 \u2211 n y (i) y (j) \u03b1 \u03b1 (x (i))(cid:62) x (j) . i i j 2 i=1 i,j=1 RecallthatwegottotheequationabovebyminimizingLwithrespecttowand b.Puttingthistogetherwiththeconstraints \u03b1 \u2265 0(thatwealwayshad)and i theconstraintfromequation(7.9),weobtainthefollowingdualoptimization problem: max W(\u03b1) = \u2211 n \u03b1 \u2212 1 \u2211 n y (i) y (j) \u03b1 \u03b1 (cid",
      "chunk_id": 115,
      "start_pos": 109693,
      "end_pos": 110893,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ecttowand b.Puttingthistogetherwiththeconstraints \u03b1 \u2265 0(thatwealwayshad)and i theconstraintfromequation(7.9),weobtainthefollowingdualoptimization problem: max W(\u03b1) = \u2211 n \u03b1 \u2212 1 \u2211 n y (i) y (j) \u03b1 \u03b1 (cid:104)x (i) ,x (j)(cid:105). (7.11) i i j \u03b1 2 i=1 i,j=1 s.t. \u03b1 \u22650, i =1,...,n (7.12) i n \u2211 \u03b1 y (i) =0. (7.13) i i=1 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 68 chapter 7. support vector machines Youshouldalsobeabletoverifythattheconditionsrequiredfor p\u2217 = d\u2217 and theKKTconditions(Equations(7.1)to(7.5))toholdareindeedsatisfiedinour optimizationproblem.Hence,wecansolvethedualinlieuofsolvingtheprimal problem.Specifically,inthedualproblemabove,wehaveamaximizationproblem inwhichtheparametersarethe\u03b1\u2019s.We\u2019lltalklateraboutthespecificalgorithm i thatwe\u2019regoingtousetosolvethedualproblem,butifweareindeedableto solveit(i.e.,findthe\u03b1\u2019sthatmaximizeW(\u03b1)subjecttotheconstraints),thenwe canuseEquation(7.8)togobackandfindtheoptimalw\u2019sasafunctionofthe\u03b1\u2019s. Havingfoundw\u2217,byconsideringtheprimalproblem,itisalsostraightforward tofindtheoptimalvaluefortheintercepttermbas max w\u2217(cid:62)x(i)+min w\u2217(cid:62)x(i) b \u2217 = \u2212 i:y(i)=\u22121 i:y(i)=1 .",
      "chunk_id": 116,
      "start_pos": 110693,
      "end_pos": 111846,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "sasafunctionofthe\u03b1\u2019s. Havingfoundw\u2217,byconsideringtheprimalproblem,itisalsostraightforward tofindtheoptimalvaluefortheintercepttermbas max w\u2217(cid:62)x(i)+min w\u2217(cid:62)x(i) b \u2217 = \u2212 i:y(i)=\u22121 i:y(i)=1 . (7.14) 2 (Checkforyourselfthatthisiscorrect.) Beforemovingon,let\u2019salsotakeamorecarefullookatEquation(7.8),which givestheoptimalvalueofwintermsof(theoptimalvalueof)\u03b1.Supposewe\u2019ve fitourmodel\u2019sparameterstoatrainingset,andnowwishtomakeapredictionat anewpointinputx.Wewouldthencalculatew(cid:62)x+b,andpredicty =1ifand onlyifthisquantityisbiggerthanzero.Butusingequation(7.8),thisquantity canalsobewritten: (cid:32) (cid:33)(cid:62) n w (cid:62) x+b = \u2211 \u03b1 y (i) x (i) x+b (7.15) i i=1 n = \u2211 \u03b1 y (i)(cid:104)x (i) ,x(cid:105)+b. (7.16) i i=1 Hence,ifwe\u2019vefoundthe\u03b1\u2019s,inordertomakeaprediction,wehavetocalculate i aquantitythatdependsonlyontheinnerproductbetweenxandthepointsin thetrainingset.Moreover,wesawearlierthatthe\u03b1\u2019swillallbezeroexceptfor i thesupportvectors.Thus,manyofthetermsinthesumabovewillbezero,and wereallyneedtofindonlytheinnerproductsbetweenxandthesupportvectors (ofwhichthereisoftenonlyasmallnumber)inordercalculateequation(7.16) andmakeourprediction.",
      "chunk_id": 117,
      "start_pos": 111646,
      "end_pos": 112810,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "s,manyofthetermsinthesumabovewillbezero,and wereallyneedtofindonlytheinnerproductsbetweenxandthesupportvectors (ofwhichthereisoftenonlyasmallnumber)inordercalculateequation(7.16) andmakeourprediction. Byexaminingthedualformoftheoptimizationproblem,wegainedsignificant insightintothestructureoftheproblem,andwerealsoabletowritetheentire algorithmintermsofonlyinnerproductsbetweeninputfeaturevectors.Inthe 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 7.7. regularization and the non-separable case (optional reading) 69 nextsection,wewillexploitthispropertytoapplythekernelstoourclassifica- tionproblem.Theresultingalgorithm,supportvectormachines,willbeableto efficientlylearninveryhighdimensionalspaces. 7.7 Regularizationandthenon-separablecase(optionalreading) ThederivationoftheSVMaspresentedsofarassumedthatthedataislinearly separable.Whilemappingdatatoahighdimensionalfeaturespacevia\u03c6does generallyincreasethelikelihoodthatthedataisseparable,wecan\u2019tguarantee thatitalwayswillbeso.Also,insomecasesitisnotclearthatfindingaseparating hyperplaneisexactlywhatwe\u2019dwanttodo,sincethatmightbesusceptibleto outliers.Forinstance,theleftfigurebelowshowsanoptimalmarginclassifier, a",
      "chunk_id": 118,
      "start_pos": 112610,
      "end_pos": 113810,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "yswillbeso.Also,insomecasesitisnotclearthatfindingaseparating hyperplaneisexactlywhatwe\u2019dwanttodo,sincethatmightbesusceptibleto outliers.Forinstance,theleftfigurebelowshowsanoptimalmarginclassifier, andwhenasingleoutlierisaddedintheupper-leftregion(rightfigure),itcauses thedecisionboundarytomakeadramaticswing,andtheresultingclassifierhas amuchsmallermargin. Tomakethealgorithmworkfornon-linearlyseparabledatasetsaswellasbeless sensitivetooutliers,wereformulateouroptimization(using(cid:96) regularization) 1 asfollows: min 1 (cid:107)w(cid:107)2+C \u2211 n \u03be i \u03b3,w,b 2 i=1 s.t. y (i)(w (cid:62) x (i)+b) \u22651\u2212\u03be , i =1,...,n i \u03be \u22650, i =1,...,n. i Thus,examplesarenowpermittedtohave(functional)marginlessthan1,and ifanexamplehasfunctionalmargin1\u2212\u03be (with\u03be > 0),wewouldpayacost i oftheobjectivefunctionbeingincreasedbyC\u03be .TheparameterCcontrolsthe i relativeweightingbetweenthetwingoalsofmakingthe(cid:107)w(cid:107)2small(whichwe sawearliermakesthemarginlarge)andofensuringthatmostexampleshave functionalmarginatleast1. Asbefore,wecanformtheLagrangian: L(w,b,\u03be,\u03b1,r) = 1 w (cid:62) w+C \u2211 n \u03be \u2212 \u2211 n \u03b1 (cid:104) y (i)(x (cid:62) w+b)\u22121+\u03be (cid:105) \u2212 \u2211 n r \u03be .",
      "chunk_id": 119,
      "start_pos": 113610,
      "end_pos": 114757,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "rge)andofensuringthatmostexampleshave functionalmarginatleast1. Asbefore,wecanformtheLagrangian: L(w,b,\u03be,\u03b1,r) = 1 w (cid:62) w+C \u2211 n \u03be \u2212 \u2211 n \u03b1 (cid:104) y (i)(x (cid:62) w+b)\u22121+\u03be (cid:105) \u2212 \u2211 n r \u03be . i i i i i 2 i=1 i=1 i=1 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 70 chapter 7. support vector machines Here,the\u03b1 \u2019sandr\u2019sareourLagrangemultipliers(constrainedtobe\u22650).We i i won\u2019tgothroughthederivationofthedualagainindetail,butaftersettingthe derivativeswithrespecttowandbtozeroasbefore,substitutingthembackin, andsimplifying,weobtainthefollowingdualformoftheproblem: max W(\u03b1) = \u2211 n \u03b1 \u2212 1 \u2211 n y (i) y (j) \u03b1 \u03b1 (cid:104)x (i) ,x (j)(cid:105) i i j \u03b1 2 i=1 i,j=1 s.t. 0\u2264 \u03b1 \u2264C, i =1,...,n i n \u2211 \u03b1 y (i) =0. i i=1 Asbefore,wealsohavethatwcanbeexpressedintermsofthe\u03b1 \u2019sasgiven i inequation(7.8),sothataftersolvingthedualproblem,wecancontinuetouse equation(7.16)tomakeourpredictions.Notethat,somewhatsurprisingly,in adding(cid:96) regularization,theonlychangetothedualproblemisthatwhatwas 1 originallyaconstraintthat0\u2264 \u03b1 hasnowbecome0\u2264 \u03b1 \u2264C.Thecalculationfor i i b\u2217 alsohastobemodified(equation(7.14)isnolongervalid);seethecomments inthenextsection/Platt\u2019spaper.",
      "chunk_id": 120,
      "start_pos": 114557,
      "end_pos": 115743,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "hedualproblemisthatwhatwas 1 originallyaconstraintthat0\u2264 \u03b1 hasnowbecome0\u2264 \u03b1 \u2264C.Thecalculationfor i i b\u2217 alsohastobemodified(equation(7.14)isnolongervalid);seethecomments inthenextsection/Platt\u2019spaper. Also,theKKTdual-complementarityconditions(whichinthenextsection willbeusefulfortestingfortheconvergenceoftheSMOalgorithm)are: \u03b1 =0 =\u21d2 y (i)(w (cid:62) x (i)+b) \u22651 (7.17) i \u03b1 =C =\u21d2 y (i)(w (cid:62) x (i)+b) \u22641 (7.18) i 0< \u03b1 <C =\u21d2 y (i)(w (cid:62) x (i)+b) =1. (7.19) i Now, all that remains is to give an algorithm for actually solving the dual problem,whichwewilldointhenextsection. 7.8 TheSMOalgorithm(optionalreading) TheSMO(sequentialminimaloptimization)algorithm,duetoJohnPlatt,gives anefficientwayofsolvingthedualproblemarisingfromthederivationofthe SVM.PartlytomotivatetheSMOalgorithm,andpartlybecauseit\u2019sinterestingin itsownright,let\u2019sfirsttakeanotherdigressiontotalkaboutthecoordinateascent algorithm. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 7.9. smo 71 7.8.1 Coordinateascent Considertryingtosolvetheunconstrainedoptimizationproblem maxW(\u03b1 ,\u03b1 ,...,\u03b1 ).",
      "chunk_id": 121,
      "start_pos": 115543,
      "end_pos": 116637,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "scent algorithm. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 7.9. smo 71 7.8.1 Coordinateascent Considertryingtosolvetheunconstrainedoptimizationproblem maxW(\u03b1 ,\u03b1 ,...,\u03b1 ). 1 2 n \u03b1 Here,wethinkofW asjustsomefunctionoftheparameters\u03b1 \u2019s,andfornow i ignore any relationship between this problem and SVMs. We\u2019ve already seen twooptimizationalgorithms,gradientascentandNewton\u2019smethod.Thenew algorithmwe\u2019regoingtoconsiderhereiscalledcoordinateascent: repeat Algorithm7.1.Coordinateascent. fori =1,...,ndo \u03b1 i :=argmax \u03b1\u02c6i W(\u03b1 1 ,...,\u03b1 i\u22121 ,\u03b1\u02c6 i ,\u03b1 i+1 ,...,\u03b1 n ). endfor untilconvergence Thus,intheinnermostloopofthisalgorithm,wewillholdallthevariables exceptforsome \u03b1 fixed,andreoptimizeW withrespecttojusttheparameter i \u03b1.Intheversionofthismethodpresentedhere,theinner-loopreoptimizesthe i variablesinorder\u03b1 ,\u03b1 ,...,\u03b1 ,\u03b1 ,\u03b1 ,...(Amoresophisticatedversionmight 1 2 n 1 2 chooseotherorderings;forinstance,wemaychoosethenextvariabletoupdate according to which one we expect to allow us to make the largest increase in W(\u03b1).) WhenthefunctionW happenstobeofsuchaformthatthe\u2018\u2018argmax\u2019\u2019inthe innerloopcanbeperformedefficiently,thencoordinateascentcanbeafairly efficientalgorithm.Here\u2019",
      "chunk_id": 122,
      "start_pos": 116437,
      "end_pos": 117637,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "low us to make the largest increase in W(\u03b1).) WhenthefunctionW happenstobeofsuchaformthatthe\u2018\u2018argmax\u2019\u2019inthe innerloopcanbeperformedefficiently,thencoordinateascentcanbeafairly efficientalgorithm.Here\u2019sapictureofcoordinateascentinaction: Theellipsesinthefigurearethecontoursofaquadraticfunctionthatwewant tooptimize.Coordinateascentwasinitializedat(2,\u22122),andalsoplottedinthe figureisthepaththatittookonitswaytotheglobalmaximum.Noticethaton eachstep,coordinateascenttakesastepthat\u2019sparalleltooneoftheaxes,since onlyonevariableisbeingoptimizedatatime. 7.9 SMO We close off the discussion of SVMs by sketching the derivation of the SMO algorithm. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 72 chapter 7. support vector machines Here\u2019sthe(dual)optimizationproblemthatwewanttosolve: max W(\u03b1) = \u2211 n \u03b1 \u2212 1 \u2211 n y (i) y (j) \u03b1 \u03b1 (cid:104)x (i) ,x (j)(cid:105). (7.20) i i j \u03b1 2 i=1 i,j=1 s.t. 0\u2264 \u03b1 \u2264C, i =1,...,n (7.21) i n \u2211 \u03b1 y (i) =0. (7.22) i i=1 Let\u2019s say we have set of \u03b1\u2019s that satisfy the constraints in equations (7.21) i and(7.22).Now,supposewewanttohold\u03b1 ,...,\u03b1 fixed,andtakeacoordinate 2 n ascentstepandreoptimizetheobjectivewithrespectto \u03b1 .Canwemakeany 1 progress?Thea",
      "chunk_id": 123,
      "start_pos": 117437,
      "end_pos": 118637,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "at satisfy the constraints in equations (7.21) i and(7.22).Now,supposewewanttohold\u03b1 ,...,\u03b1 fixed,andtakeacoordinate 2 n ascentstepandreoptimizetheobjectivewithrespectto \u03b1 .Canwemakeany 1 progress?Theanswerisno,becausetheconstraint7.22ensuresthat n \u03b1 y (1) = \u2212 \u2211 \u03b1 y (i) . 1 i i=2 Or,bymultiplyingbothsidesbyy(1),weequivalentlyhave n \u03b1 = \u2212y (1) \u2211 \u03b1 y (i) . 1 i i=2 (Thisstepusedthefactthaty(1) \u2208 {\u22121,1},andhence(y(1))2 = 1.)Hence,\u03b1 1 isexactlydeterminedbytheother\u03b1\u2019s,andifweweretohold\u03b1 ,...,\u03b1 fixed, i 2 n thenwecan\u2019tmakeanychangeto\u03b1 withoutviolatingtheconstraint7.22inthe 1 optimizationproblem. Thus,ifwewanttoupdatesomesubjectofthe\u03b1\u2019s,wemustupdateatleasttwo i ofthemsimultaneouslyinordertokeepsatisfyingtheconstraints.Thismotivates theSMOalgorithm,whichsimplydoesthefollowing: To test for convergence of this algorithm, we can check whether the KKT conditions(equations(7.17)to(7.19))aresatisfiedtowithinsometol.Here,tolis theconvergencetoleranceparameter,andistypicallysettoaround0.01to0.001. (Seethepaperandpseudocodefordetails.) ThekeyreasonthatSMOisanefficientalgorithmisthattheupdateto\u03b1,\u03b1 can i j becomputedveryefficiently.Let\u2019snowbrieflysketchthemainideasforderiving theefficientupdate.",
      "chunk_id": 124,
      "start_pos": 118437,
      "end_pos": 119630,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ". (Seethepaperandpseudocodefordetails.) ThekeyreasonthatSMOisanefficientalgorithmisthattheupdateto\u03b1,\u03b1 can i j becomputedveryefficiently.Let\u2019snowbrieflysketchthemainideasforderiving theefficientupdate. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 7.9. smo 73 repeat Algorithm7.2.SMOalgorithm. 1. Selectsomepair\u03b1 and\u03b1 toupdatenext(usingaheuristicthattriesto i j pickthetwothatwillallowustomakethebiggestprogresstowards theglobalmaximum). 2. ReoptimizeW(\u03b1)withrespectto\u03b1 and\u03b1 ,whileholdingalltheother i j \u03b1 \u2019s(k (cid:54)=i,j)fixed. k untilconvergence Let\u2019ssaywecurrentlyhavesomesettingofthe\u03b1\u2019sthatsatisfytheconstraints i 7.21\u20137.22,andsupposewe\u2019vedecidedtohold\u03b1 ,...,\u03b1 fixed,andwanttore- 3 n optimizeW(\u03b1 ,\u03b1 ,...,\u03b1 )withrespectto\u03b1 and\u03b1 (subjecttotheconstraints). 1 2 n 1 2 Fromequation(7.22),werequirethat n \u03b1 y (1)+\u03b1 y (2) = \u2212 \u2211 \u03b1 y (i) . 1 2 i i=3 Sincetherighthandsideisfixed(aswe\u2019vefixed\u03b1 ,...\u03b1 ),wecanjustletitbe 3 n denotedbysomeconstant\u03b6: \u03b1 y (1)+\u03b1 y (2) = \u03b6. 1 2 Wecanthuspicturetheconstraintson\u03b1 and\u03b1 asfollows: 1 2 From the constraints 7.21, we know that \u03b1 and \u03b1 must lie within the box 1 2 [0,C]\u00d7[0,C]shown.Alsoplottedistheline\u03b1 y(1)+\u03b1 y(2) = \u03b6,onwhichwe 1 2 know\u03b1 an",
      "chunk_id": 125,
      "start_pos": 119430,
      "end_pos": 120630,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "uspicturetheconstraintson\u03b1 and\u03b1 asfollows: 1 2 From the constraints 7.21, we know that \u03b1 and \u03b1 must lie within the box 1 2 [0,C]\u00d7[0,C]shown.Alsoplottedistheline\u03b1 y(1)+\u03b1 y(2) = \u03b6,onwhichwe 1 2 know\u03b1 and\u03b1 mustlie.Notealsothat,fromtheseconstraints,weknow L \u2264 1 2 \u03b1 \u2264 H; otherwise, (\u03b1 ,\u03b1 ) can\u2019t simultaneously satisfy both the box and the 2 1 2 straightlineconstraint.Inthisexample,L =0.Butdependingonwhattheline \u03b1 y(1) +\u03b1 y(2) = \u03b6 looks like, this won\u2019t always necessarily be the case; but 1 2 more generally, there will be some lower-bound L and some upper-bound H on the permissible values for \u03b1 that will ensure that \u03b1 ,\u03b1 lie within the box 2 1 2 [0,C]\u00d7[0,C]. Usingsection7.9,wecanalsowrite\u03b1 asafunctionof\u03b1 : 1 2 \u03b1 = (\u03b6\u2212\u03b1 y (2))y (1) . 1 2 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 74 chapter 7. support vector machines (Checkthisderivationyourself;weagainusedthefactthaty(1) \u2208 {\u22121,1}sothat (y(1))2 =1.)Hence,theobjectiveW(\u03b1)canbewritten W(\u03b1 ,\u03b1 ,...,\u03b1 ) =W((\u03b6\u2212\u03b1 y (2))y (1) ,\u03b1 ,...,\u03b1 ).",
      "chunk_id": 126,
      "start_pos": 120430,
      "end_pos": 121447,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "hapter 7. support vector machines (Checkthisderivationyourself;weagainusedthefactthaty(1) \u2208 {\u22121,1}sothat (y(1))2 =1.)Hence,theobjectiveW(\u03b1)canbewritten W(\u03b1 ,\u03b1 ,...,\u03b1 ) =W((\u03b6\u2212\u03b1 y (2))y (1) ,\u03b1 ,...,\u03b1 ). 1 2 n 2 2 n Treating\u03b1 ,...,\u03b1 asconstants,youshouldbeabletoverifythatthisisjustsome 3 n quadraticfunctionin\u03b1 .I.e.,thiscanalsobeexpressedintheforma\u03b12+b\u03b1 +c 2 2 2 for some appropriate a, b, and c. If we ignore the \u2018\u2018box\u2019\u2019 constraints 7.21 (or, equivalently, that L \u2264 \u03b1 \u2264 H), then we can easily maximize this quadratic 2 new,unclipped functionbysettingitsderivativetozeroandsolving.We\u2019lllet\u03b1 denote 2 theresultingvalueof\u03b1 .Youshouldalsobeabletoconvinceyourselfthatifwehad 2 insteadwantedtomaximizeWwithrespectto\u03b1 butsubjecttotheboxconstraint, 2 new,unclipped thenwecanfindtheresultingvalueoptimalsimplybytaking\u03b1 and 2 \u2018\u2018clipping\u2019\u2019ittolieinthe[L,H]interval,toget \uf8f1 \uf8f4\uf8f4\uf8f2 H if\u03b1 n 2 ew,unclipped > H \u03b1new = \u03b1 new,unclipped ifL \u2264 \u03b1 new,unclipped \u2264 H 2 2 2 \uf8f4\uf8f4\uf8f3L if\u03b1 new,unclipped < L 2 Finally,havingfoundthe\u03b1new,wecanusesection7.9togobackandfindthe 2 optimalvalueof\u03b1new.",
      "chunk_id": 127,
      "start_pos": 121247,
      "end_pos": 122309,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\uf8f4\uf8f4\uf8f2 H if\u03b1 n 2 ew,unclipped > H \u03b1new = \u03b1 new,unclipped ifL \u2264 \u03b1 new,unclipped \u2264 H 2 2 2 \uf8f4\uf8f4\uf8f3L if\u03b1 new,unclipped < L 2 Finally,havingfoundthe\u03b1new,wecanusesection7.9togobackandfindthe 2 optimalvalueof\u03b1new. 1 There\u2019reacouplemoredetailsthatarequiteeasybutthatwe\u2019llleaveyouto readaboutyourselfinPlatt\u2019spaper:Oneisthechoiceoftheheuristicsusedto selectthenext\u03b1,\u03b1 toupdate;theotherishowtoupdatebastheSMOalgorithm i j isrun. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc Part V: Deep Learning FromCS229Fall2020,TengyuMa, Wenowbeginourstudyofdeeplearning.Inthissetofnotes,wegiveanoverview AnandAvati,KianKatanforoosh, ofneuralnetworks,discussvectorizationanddiscusstrainingneuralnetworks Andrew Ng, Moses Charikar, & ChristopherR\u00e9,StanfordUniver- withbackpropagation. sity. 8 Supervised Learning with Non-Linear Models Inthesupervisedlearningsetting(predictingyfromtheinputx),supposeour model/hypothesis is h (x). In the past lectures, we have considered the cases \u03b8 whenh (x) = \u03b8(cid:62)x(inlinearregressionorlogisticregression)orh (x) = \u03b8(cid:62)\u03c6(x) \u03b8 \u03b8 (where\u03c6(x)isthefeaturemap).Acommonalityofthesetwomodelsisthatthey arelinearintheparameters\u03b8.Nextwewillconsiderlearninggeneralfamil",
      "chunk_id": 128,
      "start_pos": 122109,
      "end_pos": 123309,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ")x(inlinearregressionorlogisticregression)orh (x) = \u03b8(cid:62)\u03c6(x) \u03b8 \u03b8 (where\u03c6(x)isthefeaturemap).Acommonalityofthesetwomodelsisthatthey arelinearintheparameters\u03b8.Nextwewillconsiderlearninggeneralfamilyof modelsthatarenon-linearinboththeparameters\u03b8andtheinputsx.Themost commonnon-linearmodelsareneuralnetworks,whichwewilldefinestaringfrom thenextsection.Forthissection,itsufficestothinkh (x)asanabstractnon-linear \u03b8 model.1 1Ifaconcreteexampleishelpful, Suppose{(x(i),y(i))}n arethetrainingexamples.Forsimplicity,westartwith perhaps think about the model thecasewherey(i) \u2208R i a = n 1 dh (x) \u2208R. h \u03b8 (x) = \u03b8 1 2x 1 2+\u03b8 2 2x 2 2+\u00b7\u00b7\u00b7+\u03b8 d 2x d 2 \u03b8 inthissubsection,eventhoughit\u2019s Cost/lossfunction. Wedefinetheleastsquarecostfunctionforthei-thexample notaneuralnetwork. (x(i),y(i))as 1(cid:16) (cid:17)2 J (i)(\u03b8) = h (x (i))\u2212y (i) (8.1) \u03b8 2 anddefinethemean-squarecostfunctionforthedatasetas J(\u03b8) = 1 \u2211 n J (i)(\u03b8) (8.2) n i=1 whichissameasinlinearregressionexceptthatweintroduceaconstant1/nin frontofthecostfunctiontobeconsistentwiththeconvention.Notethatmulti- plyingthecostfunctionwithascalarwillnotchangethelocalminimaorglobal minimaofthecostfunction.Alsonotethattheunderlyingparameterizationfor 76 ch",
      "chunk_id": 129,
      "start_pos": 123109,
      "end_pos": 124309,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ostfunctiontobeconsistentwiththeconvention.Notethatmulti- plyingthecostfunctionwithascalarwillnotchangethelocalminimaorglobal minimaofthecostfunction.Alsonotethattheunderlyingparameterizationfor 76 chapter 8. supervised learning with non-linear models h (x)isdifferentfromthecaseoflinearregression,eventhoughtheformofthe \u03b8 costfunctionisthesamemean-squaredloss.Throughoutthenotes,weusethe words\u2018\u2018loss\u2019\u2019and\u2018\u2018cost\u2019\u2019interchangeably. 2Recallthat,asdefinedinthepre- viouslecturenotes,weusetheno- tation\u2018\u2018a:=b\u2019\u2019todenoteanoper- Optimizers(SGD). Commonly,peopleusegradientdescent(GD),stochastic ation(inacomputerprogram)in gradient(SGD),ortheirvariantstooptimizethelossfunction J(\u03b8).GD\u2019supdate whichwesetthevalueofavariable atobeequaltothevalueofb.In rulecanbewrittenas2 otherwords,thisoperationover- \u03b8 := \u03b8\u2212\u03b1\u2207 J(\u03b8) (8.3) writesawiththevalueofb.Incon- \u03b8 trast,wewillwrite\u2018\u2018a=b\u2019\u2019when where\u03b1 >0isoftenreferredtoasthelearningrateorstepsize.Next,weintroduce weareassertingastatementoffact, thatthevalueofaisequaltothe aversionoftheSGD(algorithm8.1),whichisslightlydifferentfromthatinthe valueofb. firstlecturenotes.",
      "chunk_id": 130,
      "start_pos": 124109,
      "end_pos": 125212,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "thelearningrateorstepsize.Next,weintroduce weareassertingastatementoffact, thatthevalueofaisequaltothe aversionoftheSGD(algorithm8.1),whichisslightlydifferentfromthatinthe valueofb. firstlecturenotes. OftentimescomputingthegradientofBexamplessimultane- Hyperparameter:learningrate\u03b1,numberoftotaliterationn . Algorithm8.1.Stochasticgradient iter descent. Initialize\u03b8randomly. fori =1ton do iter Samplejuniformlyfrom1,...,n,andupdate\u03b8by \u03b8 := \u03b8\u2212\u03b1\u2207 J (j)(\u03b8) \u03b8 endfor ouslyfortheparameter\u03b8 canbefasterthancomputingBgradientsseparately duetohardwareparallelization.Therefore,amini-batchversionofSGDismost commonlyusedindeeplearning,asshowninalgorithm8.2.Therearealsoother variantsoftheSGDormini-batchSGDwithslightlydifferentsamplingschemes. Withthesegenericalgorithms,atypicaldeeplearningmodelislearnedwith thefollowingsteps: 1. Defineaneuralnetworkparametrizationh (x),whichwewillintroducein \u03b8 chapter9. 2. Write the backpropagation algorithm to compute the gradient of the loss function J(j)(\u03b8)efficiently,whichwillbecoveredinchapter10. 3. RunSGDormini-batchSGD(orothergradient-basedoptimizers)withthe lossfunction J(\u03b8).",
      "chunk_id": 131,
      "start_pos": 125012,
      "end_pos": 126128,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "propagation algorithm to compute the gradient of the loss function J(j)(\u03b8)efficiently,whichwillbecoveredinchapter10. 3. RunSGDormini-batchSGD(orothergradient-basedoptimizers)withthe lossfunction J(\u03b8). 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 77 Hyperparameter:learningrate\u03b1,batchsizeB,#iterationn . Algorithm8.2.Mini-batchstochas- iter ticgradientdescent Initialize\u03b8randomly. fori =1ton do iter Samplejuniformlyfrom1,...,n,andupdate\u03b8by SampleBexamplesj ,...,j (withoutreplacement)uniformlyfrom 1 B {1,...,n},andupdate\u03b8by \u03b8 := \u03b8\u2212 \u03b1 \u2211 B \u2207 J (jk )(\u03b8) B \u03b8 k=1 endfor toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 9 Neural Networks Neuralnetworksrefertobroadtypeofnon-linearmodels/parametrizationsh (x) \u03b8 that involve combinations of matrix multiplications and other entrywise non- linearoperations.Wewillstartsmallandslowlybuildupaneuralnetwork,step bystep. A neural network with a single neuron. Recall the housing price prediction problemfrombefore:giventhesizeofthehouse,wewanttopredicttheprice. Wewilluseitasarunningexampleinthissubsection.",
      "chunk_id": 132,
      "start_pos": 125928,
      "end_pos": 127025,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ",step bystep. A neural network with a single neuron. Recall the housing price prediction problemfrombefore:giventhesizeofthehouse,wewanttopredicttheprice. Wewilluseitasarunningexampleinthissubsection. Previously,wefitastraightlinetothegraphofsizevs.housingprice.Now, insteadoffittingastraightline,wewishtopreventnegativehousingpricesby settingtheabsoluteminimumpriceaszero.Thisproducesa\u2018\u2018kink\u2019\u2019inthegraph asshowninfigure9.1.Howdowerepresentsuchafunctionwithasinglekinkas h (x)withunknownparameter?(Afterdoingso,wecaninvokethemachinery \u03b8 inpartV.) Wedefineaparameterizedfunctionh (x)withinputx,parameterizedby\u03b8, \u03b8 whichoutputsthepriceofthehousey.Formally,h : x (cid:55)\u2192 y.Perhapsoneofthe \u03b8 simplestparametrizationwouldbe h (x) =max(wx+b,0), where\u03b8 = (w,b) \u2208R2 (9.1) \u03b8 Hereh (x)returnsasinglevalue:(wx+b)orzero,whicheverisgreater.Inthe \u03b8 contextofneuralnetworks,thefunctionmax{t,0}iscalledaReLU(pro-nounced \u2018\u2018ray-lu\u2019\u2019),orrectifiedlinearunit,andoftendenotedbyReLU(t)(cid:44) max{t,0}. Generally,aone-dimensionalnon-linearfunctionthatmapsRtoRsuchas ReLUisoftenreferredtoasanactivationfunction.Themodelh (x)issaidto \u03b8 haveasingleneuronpartlybecauseithasasinglenon-linearactivationfunction.",
      "chunk_id": 133,
      "start_pos": 126825,
      "end_pos": 128010,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "nerally,aone-dimensionalnon-linearfunctionthatmapsRtoRsuchas ReLUisoftenreferredtoasanactivationfunction.Themodelh (x)issaidto \u03b8 haveasingleneuronpartlybecauseithasasinglenon-linearactivationfunction. (Wewilldiscussmoreaboutwhyanon-linearactivationiscalledneuron.) When the input x \u2208 Rd has multiple dimensions, a neural network with a singleneuroncanbewrittenas h (x) =ReLU(w (cid:62) x+b), wherew \u2208Rd,b \u2208R, and\u03b8 = (w,b) (9.2) \u03b8 79 2,000 1,500 1,000 500 0 0 1,000 2,000 3,000 4,000 5,000 squarefeet )0001$ni(ecirp housingprices Figure9.1. Housingpriceswitha \u2018\u2018kink\u2019\u2019inthegraph. Thetermbisoftenreferredtoasthe\u2018\u2018bias\u2019\u2019,andthevectorwisreferredto astheweightvector.Suchaneuralnetworkhas1layer.(Wewilldefinewhat multiplelayersmeaninthesequel.) Stackingneurons. Amorecomplexneuralnetworkmaytakethesingleneuron describedaboveand\u2018\u2018stack\u2019\u2019themtogethersuchthatoneneuronpassesitsoutput asinputintothenextneuron,resultinginamorecomplexfunction. Letusnowdeepenthehousingpredictionexample.Inadditiontothesize of the house, suppose that you know the number of bedrooms, the zip code andthewealthoftheneighborhood.Buildingneuralnetworksisanalogousto Legobricks:youtakeindividualbricksandstackthemtogethertobuildcomp",
      "chunk_id": 134,
      "start_pos": 127810,
      "end_pos": 129010,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "he house, suppose that you know the number of bedrooms, the zip code andthewealthoftheneighborhood.Buildingneuralnetworksisanalogousto Legobricks:youtakeindividualbricksandstackthemtogethertobuildcomplex structures. The same applies to neural networks: we take individual neurons andstackthemtogethertocreatecomplexneuralnetworks.Giventhesefeatures (size,numberofbedrooms,zipcode,andwealth),wemightthendecidethat thepriceofthehousedependsonthemaximumfamilysizeitcanaccommodate. Suppose the family size is a function of the size of the house and number of bedrooms(seefigure9.2).Thezipcodemayprovideadditionalinformationsuch ashowwalkabletheneighborhoodis(i.e.,canyouwalktothegrocerystoreor toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 80 chapter 9. neural networks doyouneedtodriveeverywhere).Combiningthezipcodewiththewealthofthe neighborhoodmaypredictthequalityofthelocalelementaryschool.Giventhese threederivedfeatures(familysize,walkable,schoolquality),wemayconclude thatthepriceofthehomeultimatelydependsonthesethreefeatures. size Figure9.2. Diagramofasmallneu- #bedrooms familysize r p a r l ic n e e s t .",
      "chunk_id": 135,
      "start_pos": 128810,
      "end_pos": 129949,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "dfeatures(familysize,walkable,schoolquality),wemayconclude thatthepriceofthehomeultimatelydependsonthesethreefeatures. size Figure9.2. Diagramofasmallneu- #bedrooms familysize r p a r l ic n e e s t . workforpredictinghousing pricey zipcode walkable schoolquality wealth Formally,theinputtoaneuralnetworkisasetofinputfeaturesx ,x ,x ,x . 1 2 3 4 Wedenotetheintermediatevariablesfor\u2018\u2018familysize\u2019\u2019,\u2018\u2018walk-able\u2019\u2019,and\u2018\u2018school quality\u2019\u2019bya ,a ,a (thesea\u2019sareoftenreferredtoas\u2018\u2018hiddenunits\u2019\u2019or\u2018\u2018hid- 1 2 3 i denneurons\u2019\u2019).Werepresenteachofthea\u2019sasaneuralnetworkwithasingle i neuronwithasubsetofx ,...,x asinputs.Thenasinfigure9.1,wewillhave 1 4 theparameterization: a =ReLU(\u03b8 x +\u03b8 x +\u03b8 ) 1 1 1 2 2 3 a =ReLU(\u03b8 x +\u03b8 ) 2 4 3 5 a =ReLU(\u03b8 x +\u03b8 x +\u03b8 ) 3 6 3 7 4 8 where(\u03b8 ,...,\u03b8 )areparameters.Nowwerepresentthefinaloutput h (x)as 1 8 \u03b8 anotherlinearfunctionwitha 1 ,a 2 ,a 3 asinputs,andweget1 1Typically, for multi-layer neural network,attheend,neartheout- h (x) = \u03b8 a +\u03b8 a +\u03b8 a +\u03b8 (9.3) put, we don\u2019t apply ReLU, espe- \u03b8 9 1 10 2 11 3 12 ciallywhentheoutputisnotnec- where\u03b8containsalltheparameters(\u03b8 ,...,\u03b8 ). essarilyapositivenumber.",
      "chunk_id": 136,
      "start_pos": 129749,
      "end_pos": 130875,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ",attheend,neartheout- h (x) = \u03b8 a +\u03b8 a +\u03b8 a +\u03b8 (9.3) put, we don\u2019t apply ReLU, espe- \u03b8 9 1 10 2 11 3 12 ciallywhentheoutputisnotnec- where\u03b8containsalltheparameters(\u03b8 ,...,\u03b8 ). essarilyapositivenumber. 1 12 Nowwerepresenttheoutputasaquitecomplexfunctionofxwithparameters \u03b8.Thenyoucanusethisparametrizationh withthemachineryofpartVtolearn \u03b8 theparameters\u03b8. Inspirationfrombiologicalneuralnetworks. Asthenamesuggests,artificial neuralnetworkswereinspiredbybiologicalneuralnetworks.Thehiddenunits a ,...,a correspond to the neurons in a biological neural network, and the 1 m 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 81 parameters\u03b8\u2019scorrespondtothesynapses.However,it\u2019sunclearhowsimilarthe i moderndeepartificialneuralnetworksaretothebiologicalones.Forexample, perhapsnotmanyneuroscientiststhinkbiologicalneuralnetworkscouldhave 1000layers,whilesomemodernartificialneuralnetworksdo(wewillelaborate moreonthenotionoflayers.)Moreover,it\u2019sanopenquestionwhetherhuman brainsupdatetheirneuralnetworksinawaysimilartothewaythatcomputer scientistslearnartificialneuralnetworks(usingbackpropagation,whichwewill introduceinthenextsection.). Two-layer fully-connected neural networks.",
      "chunk_id": 137,
      "start_pos": 130675,
      "end_pos": 131874,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "atetheirneuralnetworksinawaysimilartothewaythatcomputer scientistslearnartificialneuralnetworks(usingbackpropagation,whichwewill introduceinthenextsection.). Two-layer fully-connected neural networks. We constructed the neural net- work in equation (9.3) using a significant amount of prior knowledge/belief abouthowthe\u2018\u2018familysize\u2019\u2019,\u2018\u2018walkable\u2019\u2019,and\u2018\u2018schoolquality\u2019\u2019aredeterminedby theinputs.Weimplicitlyassumedthatweknowthefamilysizeisanimportant quantitytolookatandthatitcanbedeterminedbyonlythe\u2018\u2018size\u2019\u2019and\u2018\u2018#bed- rooms\u2019\u2019.Suchapriorknowledgemightnotbeavailableforotherapplications.It wouldbemoreflexibleandgeneraltohaveagenericparameterization.Asimple waywouldbetowritetheintermediatevariablea asafunctionofallx ,...,x : 1 1 4 a =ReLU(w (cid:62) x+b ), wherew \u2208R4andb \u2208R 1 1 1 1 1 a =ReLU(w (cid:62) x+b ), wherew \u2208R4andb \u2208R 2 2 2 2 2 a =ReLU(w (cid:62) x+b ), wherew \u2208R4andb \u2208R 3 3 3 3 3 Westilldefineh (x)usingequation(9.3)witha ,a ,a beingdefinedasabove. \u03b8 1 2 3 Thuswehaveaso-calledfully-connectedneuralnetworkasvisualizedinthe dependencygraphinfigure9.3becausealltheintermediatevariablesa\u2019sdepend i onalltheinputsx\u2019s. i a Figure9.3. Diagramofatwo-layer 1 fully connected neural network.",
      "chunk_id": 138,
      "start_pos": 131674,
      "end_pos": 132868,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "-connectedneuralnetworkasvisualizedinthe dependencygraphinfigure9.3becausealltheintermediatevariablesa\u2019sdepend i onalltheinputsx\u2019s. i a Figure9.3. Diagramofatwo-layer 1 fully connected neural network. x 1 Eachedgefromnode xi tonode a 2 ajindicatesthatajdependsonxi. x 2 a h \u03b8 (x) w Th it e h ed th g e e w fro e m igh x t i ( to w a [ j 1 j ]) is i a w s h so ic c h iat d e e d - 3 notesthei-thcoordinateofthevec- x 3 tor w [ j 1] . The activation aj can be a computedbytakingtheReLUof 4 theweightedsumofxi\u2019swiththe weightsbeingtheweightsassoci- atedwiththeincomingedges,that is,aj =ReLU (cid:16) \u2211 i d =1 (w [ j 1]) ixi (cid:17) . toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 82 chapter 9. neural networks Forfullgenerality,atwo-layerfully-connectedneuralnetworkwithmhidden unitsandddimensionalinputx \u2208Rd isdefinedas \u2200j \u2208 [1,...,m], z = w [1](cid:62) x+b [1] wherew [1] \u2208Rd,b [1] \u2208R (9.4) j j j j j a =ReLU(z ) (9.5) j j a = [a ,...,a ](cid:62) \u2208Rm (9.6) 1 m h (x) = w [2](cid:62) a+b [2] wherew [2] \u2208Rm,b [2] \u2208R (9.7) \u03b8 NotethatbydefaultthevectorsinRd areviewedascolumnvectors,andin particularaisacolumnvectorwithcomponentsa ,a ,...,a .Theindices[1]and 1 2 m [2] areused",
      "chunk_id": 139,
      "start_pos": 132668,
      "end_pos": 133868,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "](cid:62) a+b [2] wherew [2] \u2208Rm,b [2] \u2208R (9.7) \u03b8 NotethatbydefaultthevectorsinRd areviewedascolumnvectors,andin particularaisacolumnvectorwithcomponentsa ,a ,...,a .Theindices[1]and 1 2 m [2] areusedtodistinguishtwosetsofparameters:thew [1] \u2019s(eachofwhichisa j vectorinRd)andw[2] (whichisavectorinRm).Wewillhavemoreoftheselater. Vectorization. Beforeweintroduceneuralnetworkswithmorelayersandmore complexstructures,wewillsimplifytheexpressionsforneuralnetworkswith morematrixandvectornotations.Anotherimportantmotivationofvectorization isthespeedperspectiveintheimplementation.Inordertoimplementaneural networkefficiently,onemustbecarefulwhenusingforloops.Themostnatural waytoimplementequation(9.4)incodeisperhapstouseaforloop.Inpractice, thedimensionalitiesoftheinputsandhiddenunitsarehigh.Asaresult,code willrunveryslowlyifyouuseforloops.LeveragingtheparallelisminGPUs is/wascrucialfortheprogressofdeeplearning. Thisgaverisetovectorization.Insteadofusingforloops,vectorizationtakes advantageofmatrixalgebraandhighlyoptimizednumericallinearalgebrapack- ages(e.g.,BLAS)tomakeneuralnetworkcomputationsrunquickly.Beforethe deeplearningera,aforloopmayhavebeensufficientonsmallerdatasets,but moderndeepn",
      "chunk_id": 140,
      "start_pos": 133668,
      "end_pos": 134868,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ixalgebraandhighlyoptimizednumericallinearalgebrapack- ages(e.g.,BLAS)tomakeneuralnetworkcomputationsrunquickly.Beforethe deeplearningera,aforloopmayhavebeensufficientonsmallerdatasets,but moderndeepnetworksandstate-of-the-artdatasetswillbeinfeasibletorunwith forloops. Wevectorizethetwo-layerfully-connectedneuralnetworkasbelow.Wedefine aweightmatrixW[1] inRm\u00d7d astheconcatenationofallthevectorsw [1] \u2019sinthe j 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 83 followingway: \uf8ee [1](cid:62) \uf8f9 w 1 \uf8ef [1](cid:62) \uf8fa \uf8ef w \uf8fa W [1] = \uf8ef 2 \uf8fa \u2208Rm\u00d7d \uf8ef . \uf8fa \uf8ef . . \uf8fa \uf8f0 \uf8fb [1](cid:62) w m Now by the definition of matrix vector multiplication, we can write z = [z ,...,z ](cid:62) \u2208Rm as: 1 m \uf8ee z 1 \uf8f9 \uf8ee w 1 [1](cid:62) \uf8f9 \uf8ee x 1 \uf8f9 \uf8ee b 1 [1] \uf8f9 \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 . . . . . . \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb = \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 w 2 [ . . . 1](cid:62) \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 x . . . 2 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb + \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 x 2 [ . . . 1]\uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb z m w m [1](cid:62) z d b m [1] z (cid:124) \u2208R (cid:123)(cid:122) m\u00d7 (cid:125) 1 (cid:124) W[1]\u2208 (cid:123)(cid:122) Rm\u00d7d (cid:125) x (cid:124) \u2208 (cid:123) R (cid:122) d\u00d7 (cid:125) 1 b[ (cid:124) 1]\u2208 (cid:123) R (cid:122) m\u00d7 (cid:125) 1 Orsuccinctly, z =W [1] x+b [1] (9.8) Weremarkagainthatavecto",
      "chunk_id": 141,
      "start_pos": 134668,
      "end_pos": 135868,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "(cid:123)(cid:122) Rm\u00d7d (cid:125) x (cid:124) \u2208 (cid:123) R (cid:122) d\u00d7 (cid:125) 1 b[ (cid:124) 1]\u2208 (cid:123) R (cid:122) m\u00d7 (cid:125) 1 Orsuccinctly, z =W [1] x+b [1] (9.8) WeremarkagainthatavectorinRd inthesenotes,followingtheconventions previouslyestablished,isautomaticallyviewedasacolumnvector,andcanalso beviewedasad\u00d71dimensionalmatrix.(Notethatthisisdifferentfromnumpy whereavectorisviewedasarowvectorinbroadcasting.) Computing the activations a \u2208 Rm from z \u2208 Rm involves an element-wise non-linearapplicationoftheReLUfunction,whichcanbecomputedinparallel efficiently.OverloadingReLUforelement-wiseapplicationofReLU(meaning,for avectort \u2208Rd,ReLU(t)isavectorsuchthatReLU(t) =ReLU(t )),wehave: i i a =ReLU(z) (9.9) DefineW[2] = [w[2](cid:62) ] \u2208R1\u00d7m similarly.Then,themodelinequation(9.7)can besummarizedas: a =ReLU(W [1] x+b [1]) (9.10) h (x) =W [2] a+b [2] (9.11) \u03b8 Here\u03b8consistsofW[1],W[2](oftenreferredtoastheweightmatrices)andb[1],b[2] (referredtoasthebiases).ThecollectionofW[1],b[1]isreferredtoasthefirstlayer, andW[2],b[2] thesecondlayer.Theactivationaisreferredtoasthehiddenlayer. Atwo-layerneuralnetworkisalsocalledone-hidden-layerneuralnetwork.",
      "chunk_id": 142,
      "start_pos": 135668,
      "end_pos": 136831,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "hebiases).ThecollectionofW[1],b[1]isreferredtoasthefirstlayer, andW[2],b[2] thesecondlayer.Theactivationaisreferredtoasthehiddenlayer. Atwo-layerneuralnetworkisalsocalledone-hidden-layerneuralnetwork. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 84 chapter 9. neural networks Multi-layerfully-connectedneuralnetworks. Withthissuccinctnotations,we canstackmorelayerstogetadeeperfully-connectedneuralnetwork.Letrbethe numberoflayers(weightmatrices).LetW[1],...,W[r],b[1],...,b[r] betheweight matricesandbiasesofallthelayers.Thenamulti-layerneuralnetworkcanbe writtenas: a [1] =ReLU(W [1] x+b [1]) (9.12) a [2] =ReLU(W [2] a [1]+b [2]) (9.13) \u00b7\u00b7\u00b7 (9.14) a [r\u22121] =ReLU(W [r\u22121] a [r\u22122]+b [r\u22121]) (9.15) h (x) =W [r] a [r\u22121]+b [r] (9.16) \u03b8 Wenotethattheweightmatricesandbiasesneedtohavecompatibledimen- sionsfortheequationsabovetomakesense.Ifa[k] hasdimensionm ,thenthe k weightmatrixW[k] shouldbeofdimensionm k \u00d7m k\u22121 ,andthebiasb[k] \u2208 Rmk. Moreover,W[1] \u2208Rm1 \u00d7d andW[r] \u2208R1\u00d7mr\u22121. Thetotalnumberofneuronsinthenetworkis m +\u00b7\u00b7\u00b7+m ,andthetotal 1 r numberofparametersinthisnetworkis(d+1)m 1 +(m 1 +1)m 2 +\u00b7\u00b7\u00b7+(m r\u22121 + 1)m .",
      "chunk_id": 143,
      "start_pos": 136631,
      "end_pos": 137771,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "andthebiasb[k] \u2208 Rmk. Moreover,W[1] \u2208Rm1 \u00d7d andW[r] \u2208R1\u00d7mr\u22121. Thetotalnumberofneuronsinthenetworkis m +\u00b7\u00b7\u00b7+m ,andthetotal 1 r numberofparametersinthisnetworkis(d+1)m 1 +(m 1 +1)m 2 +\u00b7\u00b7\u00b7+(m r\u22121 + 1)m . r Sometimesfornotationalconsistencywealsowritea[0] = x,anda[r] = h (x). \u03b8 Thenwehavesimplerecursionthat a [k] =ReLU(W [k] a [k\u22121]+b [k]), \u2200k =1,...,r\u22121 (9.17) Notethatthiswouldhavebetruefork =riftherewereanadditionalReLUin equation(9.16),butoftenpeopleliketomakethelastlayerlinear(akawithouta ReLU)sothatnegativeoutputsarepossibleandit\u2019seasiertointerpretthelast layerasalinearmodel.(Moreontheinterpretabilityatthe\u2018\u2018connectiontokernel method\u2019\u2019paragraphofthissection.) Otheractivationfunctions. TheactivationfunctionReLUcanbereplacedby manyothernon-linearfunction\u03c3(\u00b7)thatmapsRtoRsuchas: 1 \u03c3(z) = (sigmoid) 1+e\u2212z ez\u2212e\u2212z \u03c3(z) = (tanh) ez+e\u2212z 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 85 Whydowenotusetheidentityfunctionfor\u03c3(z)? Thatis,whynotuse\u03c3(z) = z? Assumeforsakeofargumentthatb[1] andb[2] arezeros.Suppose\u03c3(z) = z,then fortwo-layerneuralnetwork,wehavethat h (x) =W [2] a [1] \u03b8 =W [2] \u03c3(z [1]) (bydefinition) =W [2] z [1] (since\u03c3(z) = z) =W [2] W [1] x (fromchapter9) =",
      "chunk_id": 144,
      "start_pos": 137571,
      "end_pos": 138771,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "mentthatb[1] andb[2] arezeros.Suppose\u03c3(z) = z,then fortwo-layerneuralnetwork,wehavethat h (x) =W [2] a [1] \u03b8 =W [2] \u03c3(z [1]) (bydefinition) =W [2] z [1] (since\u03c3(z) = z) =W [2] W [1] x (fromchapter9) =W\u02dc x (whereW\u02dc =W[2]W[1]) NoticehowW[2]W[1] collapsedintoW\u02dc . Thisisbecauseapplyingalinearfunctiontoanotherlinearfunctionwillresult in a linear function over the original input (i.e., you can construct a W\u02dc such that W\u02dc x = W[2]W[1]x). This loses much of the representational power of the neuralnetworkasoftentimestheoutputwearetryingtopredicthasanon-linear relationshipwiththeinputs.Withoutnon-linearactivationfunctions,theneural networkwillsimplyperformlinearregression. Connection to the Kernel Method. In the previous lectures, we covered the conceptoffeaturemaps.Recallthatthemainmotivationforfeaturemapsisto representfunctionsthatarenon-linearintheinputxby\u03b8(cid:62)\u03c6(x),where\u03b8arethe parametersand\u03c6(x),thefeaturemap,isahandcraftedfunctionnon-linearinthe rawinputx.Theperformanceofthelearningalgorithmscansignificantlydepends onthechoiceofthefeaturemap\u03c6(x).Oftentimespeopleusedomainknowledge todesignthefeaturemap\u03c6(x)thatsuitstheparticularapplications.Theprocess ofchoosingthefeaturemapsisoftenref",
      "chunk_id": 145,
      "start_pos": 138571,
      "end_pos": 139771,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ithmscansignificantlydepends onthechoiceofthefeaturemap\u03c6(x).Oftentimespeopleusedomainknowledge todesignthefeaturemap\u03c6(x)thatsuitstheparticularapplications.Theprocess ofchoosingthefeaturemapsisoftenreferredtoasfeatureengineering. Wecanviewdeeplearningasawaytoautomaticallylearntherightfeature map(sometimesalsoreferredtoas\u2018\u2018therepresentation\u2019\u2019)asfollows.Supposewe denoteby\u03b2thecollectionoftheparametersinafully-connectedneuralnetworks (equation(9.16))exceptthoseinthelastlayer.Thenwecanabstractrighta[r\u22121] asafunctionoftheinputxandtheparametersin\u03b2 : a[r\u22121] = \u03c6 (x).Nowwecan \u03b2 writethemodelas: h (x) =W [r] \u03c6 (x)+b [r] (9.18) \u03b8 \u03b2 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 86 chapter 9. neural networks When\u03b2isfixed,then\u03c6 (\u00b7)canviewedasafeaturemap,andthereforeh (x)isjust \u03b2 \u03b8 alinearmodeloverthefeatures\u03c6 (x).However,wewilltraintheneuralnetworks, \u03b2 boththeparametersin\u03b2andtheparametersW[r],b[r]areoptimized,andtherefore wearenotlearningalinearmodelinthefeaturespace,butalsolearningagood featuremap \u03c6 (\u00b7) itselfsothatit\u2019spossibletopredictaccuratelywithalinear \u03b2 modelontopofthefeaturemap.Therefore,deeplearningtendstodependless onthedomainknowledgeoftheparticularapplications",
      "chunk_id": 146,
      "start_pos": 139571,
      "end_pos": 140771,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "earningagood featuremap \u03c6 (\u00b7) itselfsothatit\u2019spossibletopredictaccuratelywithalinear \u03b2 modelontopofthefeaturemap.Therefore,deeplearningtendstodependless onthedomainknowledgeoftheparticularapplicationsandrequiresoftenless featureengineering.Thepenultimatelayera[r\u22121] isoften(informally)referredto asthelearnedfeaturesorrepresentationsinthecontextofdeeplearning. Intheexampleofhousepriceprediction,afully-connectedneuralnetwork doesnotneedustospecifytheintermediatequantitysuch\u2018\u2018familysize\u2019\u2019,and mayautomaticallydiscoversomeusefulfeaturesinthelastpenultimatelayer (theactivationa[r\u22121]),andusethemtolinearlypredictthehousingprice.Often thefeaturemap/representationobtainedfromonedatasets(thatis,thefunction \u03c6 (\u00b7)canbealsousefulforotherdatasets,whichindicatestheycontainessential \u03b2 informationaboutthedata.However,oftentimes,theneuralnetworkwilldiscover complex features which are very useful for predicting the output but may be difficultforahumantounderstandorinterpret.Thisiswhysomepeoplereferto neuralnetworksasablackbox,asitcanbedifficulttounderstandthefeaturesit hasdiscovered.",
      "chunk_id": 147,
      "start_pos": 140571,
      "end_pos": 141650,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ry useful for predicting the output but may be difficultforahumantounderstandorinterpret.Thisiswhysomepeoplereferto neuralnetworksasablackbox,asitcanbedifficulttounderstandthefeaturesit hasdiscovered. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 10 Backpropagation Inthissection,weintroducebackpropgationorauto-differentiation,whichcomputes thegradientoftheloss\u2207J(j)(\u03b8)efficiently.Wewillstartwithaninformaltheorem thatstatesthataslongasareal-valuedfunction f canbeefficientlycomputed/eval- uatedbyadifferentiablenetworkorcircuit,thenitsgradientcanbeefficiently computed in a similar time. We will then show how to do this concretely for fully-connectedneuralnetworks. Becausetheformalityofthegeneraltheoremisnotthemainfocushere,we willintroducethetermswithinformaldefinitions.Byadifferentiablecircuitor adifferentiablenetwork,wemeanacompositionofasequenceofdifferentiable arithmeticoperations(additions,subtraction,multiplication,divisions,etc)and elementarydifferentiablefunctions(ReLU,exp,log,sin,cos,etc.).Letthesizeof thecircuitbethetotalnumberofsuchoperationsandelementaryfunctions.We assumethateachoftheoperationsandfunctions,andtheirderivativesorpartial derivativesc",
      "chunk_id": 148,
      "start_pos": 141450,
      "end_pos": 142650,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "unctions(ReLU,exp,log,sin,cos,etc.).Letthesizeof thecircuitbethetotalnumberofsuchoperationsandelementaryfunctions.We assumethateachoftheoperationsandfunctions,andtheirderivativesorpartial derivativescanbecomputedinO(1)timeinthecomputer. Theorem1(backpropagationorauto-differentiation,informallystated). Suppose adifferentiablecircuitofsizeNcomputesareal-valuedfunction f : R(cid:96) (cid:55)\u2192 R.Then,the gradient\u2207f canbecomputedintimeO(N),byacircuitofsizeO(N).1 1Wenoteiftheoutputofthefunc- tion f doesnotdependonsome Wenotethatthelossfunction J(j)(\u03b8)forthej-thexamplecanbeindeedcom- oftheinputcoordinates,thenwe set by default the gradient w.r.t putedbyasequenceofoperationsandfunctionsinvolvingadditions,subtraction, thatcoordinatetozero.Settingto multiplications,andnon-linearactivations.Thusthetheoremsuggeststhatwe zerodoesnotcounttowardstheto- shouldbeabletocompute\u2207J(j)(\u03b8)inasimilartimetothatforcomputing J(j)(\u03b8) talruntimehereinouraccounting scheme.ThisiswhywhenN\u2264(cid:96), itself.Thisdoesnotonlyapplytothefully-connectedneuralnetworkintroduced we can compute the gradient in inchapter9,butalsomanyothertypesofneuralnetworks. O(N)time,whichmightbepoten- tiallyevenlessthan(cid:96).",
      "chunk_id": 149,
      "start_pos": 142450,
      "end_pos": 143639,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "Thisdoesnotonlyapplytothefully-connectedneuralnetworkintroduced we can compute the gradient in inchapter9,butalsomanyothertypesofneuralnetworks. O(N)time,whichmightbepoten- tiallyevenlessthan(cid:96). Intherestofthesection,wewillshowcasehowtocomputethegradientof thelossefficientlyforfully-connectedneuralnetworksusingbackpropagation. Eventhoughauto-differentiationorbackpropagationisimplementedinallthe deeplearningpackagessuchasTensorFlowandPyTorch,understandingitis veryhelpfulforgaininginsightsintotheworkingsofdeeplearning. 88 chapter 10. backpropagation 10.1 Preliminary:chainrule Wefirstrecallthechainruleincalculus.Supposethevariable Jdependsonthe variables\u03b8 ,...,\u03b8 viatheintermediatevariablesg ,...,g : 1 p 1 k g = g (\u03b8 ,...,\u03b8 ),\u2200j \u2208 {1,...,k}J = J(g ,...,g ) (10.1) j j 1 p 1 k Hereweoverloadthemeaningofg \u2019s:theydenoteboththeintermediatevariables j butalsothefunctionsusedtocomputetheintermediatevariables.Then,bythe chainrule,wehavethat\u2200i: \u2202J \u2211 k \u2202J \u2202gj = (10.2) \u2202\u03b8 \u2202gj \u2202\u03b8 i j=1 i Fortheeaseofinvokingthechainruleinthefollowingsubsectionsinvarious ways,wewillcall J theoutputvariable, g ,...,g intermediatevariables,and 1 k \u03b8 ,...,\u03b8 theinputvariablesinthechainrule.",
      "chunk_id": 150,
      "start_pos": 143439,
      "end_pos": 144616,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\u2202gj \u2202\u03b8 i j=1 i Fortheeaseofinvokingthechainruleinthefollowingsubsectionsinvarious ways,wewillcall J theoutputvariable, g ,...,g intermediatevariables,and 1 k \u03b8 ,...,\u03b8 theinputvariablesinthechainrule. 1 p 10.2 Backpropagationfortwo-layerneuralnetworks Nowweconsiderthetwo-layerneuralnetworkdefinedinequation(9.11).Our generalapproachistofirstunpackthevectorizednotationtoscalarformtoapply thechainrule,butassoonaswefinishthederivation,wewillpackthescalar equationsbacktoavectorizedformtokeepthenotationssuccinct. Recallthefollowingequationsareusedforthecomputationoftheloss J: z =W [1] x+b [1] (10.3) a =ReLU(z) (10.4) h (x)(cid:44) o =W [2] a+b [2] (10.5) \u03b8 1 J = (y\u2212o)2 (10.6) 2 Recall that W[1] \u2208 Rm\u00d7d, W[2] \u2208 R1\u00d7m, and b[1],z,a \u2208 Rm, and o,y,b[2] \u2208 R. RecallthatavectorinRd isautomaticallyinterpretedasacolumnvector(likea matrixinRd\u00d71)ifneedbe.2 2We also note that even though thisistheconventioninmath,it\u2019s different from the convention in numpywhereanonedimensional arraywillbeautomaticallyinter- pretedasarowvector. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 10.2.",
      "chunk_id": 151,
      "start_pos": 144416,
      "end_pos": 145516,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "inmath,it\u2019s different from the convention in numpywhereanonedimensional arraywillbeautomaticallyinter- pretedasarowvector. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 10.2. backpropagation for two-layer neural networks 89 10.2.1 Computing \u2202J \u2202W[2] SupposeW[2] = [W [2] ,...,W [2] ].Westartbycomputing \u2202J usingthechainrule 1 m \u2202W [2] i (equation(10.2))withoastheintermediatevariable. \u2202J \u2202J \u2202o = \u00b7 \u2202W[2] \u2202o \u2202W[2] i i \u2202o = (o\u2212y)\u00b7 \u2202W[2] i = (o\u2212y)\u00b7a (becauseo = \u2211m W[2] a +b[2]) i i=1 i i Vectorizednotation. Theequationaboveinvectorizednotationbecomes: \u2202J = (o\u2212y)\u00b7a (cid:62) \u2208R1\u00d7m (10.7) \u2202W[2] Similarly,weleavethereadertoverifythat: \u2202J = (o\u2212y) \u2208R (10.8) \u2202b[2] Clarificationforthedimensionalityofthepartialderivativenotation. Wewill usethenotation \u2202J frequentlyintherestofthelecturenotes.Wenotethatherewe \u2202A onlyusethisnotationforthecasewhen Jisareal-valuedvariable,3but Acanbe 3Thereisanextensionofthisno- avectororamatrix.Moreover, \u2202J hasthesamedimensionalityasA.Forexample, tationtovectorormatrixvariable \u2202A J.However,inpractice,it\u2019soften whenAisamatrix,the(i,j)-thentryof \u2202J isequalto \u2202J .Ifyouarefamiliarwith \u2202A \u2202Aij impracticaltocomputethederiva- thenotionoftotalderivat",
      "chunk_id": 152,
      "start_pos": 145316,
      "end_pos": 146516,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "tationtovectorormatrixvariable \u2202A J.However,inpractice,it\u2019soften whenAisamatrix,the(i,j)-thentryof \u2202J isequalto \u2202J .Ifyouarefamiliarwith \u2202A \u2202Aij impracticaltocomputethederiva- thenotionoftotalderivatives,wenotethattheconventionfordimensionality tivesofhigh-dimensionaloutputs. Thus,wewillavoidusingtheno- hereisdifferentfromthatfortotalderivatives. tation \u2202J for J thatisnotareal- \u2202A valuedvariable. 10.2.2 Computing \u2202J \u2202W[1] Nextwecompute \u2202J .Wefirstunpackthevectorizednotation:letW [1] denote \u2202W[1] ij the (i,j)-theentryofW[1],wherei \u2208 [m] and j \u2208 [d].Wecompute \u2202J using \u2202W [1] ij chainrule(equation(10.2))withz astheintermediatevariable: i \u2202J \u2202J \u2202z = \u00b7 i [1] \u2202z [1] \u2202W i \u2202W ij ij = \u2202J \u00b7x (becausez = \u2211d W [1] x +b [1] ) \u2202z j i k=1 ik k i i toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 90 chapter 10. backpropagation Vectorizednotation. Theequationabovecanbewrittencompactlyas: \u2202J \u2202J = \u00b7x (cid:62) (10.9) \u2202W[1] \u2202z Wecanverifythatthedimensionsmatch: \u2202J \u2208 Rm\u00d7d, \u2202J \u2208 Rm\u00d71 and x(cid:62) \u2208 \u2202W[1] \u2202z R1\u00d7d. Abstraction. Forfutureusage,thecomputationsfor \u2202J and \u2202J abovecan \u2202W[1] \u2202W[2] beabstractifiedintothefollowingclaim: Claim1.",
      "chunk_id": 153,
      "start_pos": 146316,
      "end_pos": 147470,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ythatthedimensionsmatch: \u2202J \u2208 Rm\u00d7d, \u2202J \u2208 Rm\u00d71 and x(cid:62) \u2208 \u2202W[1] \u2202z R1\u00d7d. Abstraction. Forfutureusage,thecomputationsfor \u2202J and \u2202J abovecan \u2202W[1] \u2202W[2] beabstractifiedintothefollowingclaim: Claim1. Suppose Jisareal-valuedoutputvariable,z \u2208Rm istheintermediate variable,andW \u2208 Rm\u00d7d,u \u2208 Rd,b \u2208 Rm aretheinputvariables,andsuppose theysatisfythefollowing: z =Wu+b J = J(z) Then \u2202J and \u2202J satisfy: \u2202W \u2202b \u2202J \u2202J = \u00b7u (cid:62) \u2202W \u2202z \u2202J \u2202J = \u2202b \u2202z 10.2.3 Computing \u2202J \u2202z Equation(10.9)tellsusthattocompute \u2202J ,itsufficestocompute \u2202J ,whichis \u2202W[1] \u2202z thegoalofthenextfewderivations. Weinvokethechainrulewith Jastheoutputvariable,a astheintermediate i variable,andz astheinputvariable: i \u2202J \u2202J \u2202a = i \u2202z \u2202a \u2202z i i i \u2202J = \u00b71{z \u22650} i \u2202a i Vectorization and abstraction. The computation above can be summarized into: 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 10.2. backpropagation for two-layer neural networks 91 Claim2. Supposethereal-valuedoutputvariable Jandvectorsz,a \u2208Rm satisfy thefollowing: a = \u03c3(z) (where\u03c3isanelement-wiseactivation,z,a \u2208Rm) J = J(a) Then,wehavethat \u2202J \u2202J = (cid:12)\u03c3 (cid:48)(z) \u2202z \u2202a where \u03c3(cid:48)(\u00b7) istheelement-wisederivativeoftheactivationfunctio",
      "chunk_id": 154,
      "start_pos": 147270,
      "end_pos": 148470,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "isfy thefollowing: a = \u03c3(z) (where\u03c3isanelement-wiseactivation,z,a \u2208Rm) J = J(a) Then,wehavethat \u2202J \u2202J = (cid:12)\u03c3 (cid:48)(z) \u2202z \u2202a where \u03c3(cid:48)(\u00b7) istheelement-wisederivativeoftheactivationfunction \u03c3,and (cid:12) denotestheelement-wiseproductoftwovectorsofthesamedimensionality. 10.2.4 Computing \u2202J \u2202a Now it suffices to compute \u2202J . We invoke the chain rule with J as the output \u2202a variable,oastheintermediatevariable,anda astheinputvariable: i \u2202J \u2202J \u2202o = \u2202a \u2202o\u2202a i i = (o\u2212y)\u00b7W [2] (becauseo = \u2211m W [2] a +b[2]) i i=1 i i Vectorization. Invectorizednotation,wehave: \u2202J =W [2](cid:62) \u00b7(o\u2212y) (10.10) \u2202a Abstraction. Wenowpresentamoregeneralformofthecomputationabove. Claim3. Suppose Jisareal-valuedoutputvariable,v \u2208Rm istheintermediate variable,andW \u2208 Rm\u00d7d,u \u2208 Rd,b \u2208 Rm aretheinputvariables,andsuppose theysatisfythefollowing: v =Wu+b J = J(v) Then, \u2202J \u2202J =W (cid:62) . (10.11) \u2202u \u2202v toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 92 chapter 10. backpropagation 10.2.5 Summaryfortwo-layerneuralnetworks Nowcombiningtheequationsabove,wearriveatalgorithm10.1whichcomputes thegradientsfortwo-layerneuralnetworks.",
      "chunk_id": 155,
      "start_pos": 148270,
      "end_pos": 149410,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "mossr@cs.stanford.edu 92 chapter 10. backpropagation 10.2.5 Summaryfortwo-layerneuralnetworks Nowcombiningtheequationsabove,wearriveatalgorithm10.1whichcomputes thegradientsfortwo-layerneuralnetworks. Computethevaluesofz \u2208Rm,a \u2208Rm,ando \u2208R Algorithm10.1.Back-propogation fortwo-layerneuralnetworks. Compute: \u2202J \u03b4 [2] (cid:44) = (o\u2212y) \u2208R \u2202o \u03b4 [1] (cid:44) \u2202J = (W [2](cid:62) (o\u2212y))(cid:12)1{z \u22650} \u2208Rm\u00d71 (byclaim2and10.10) \u2202z Compute: \u2202J = \u03b4 [2] a (cid:62) \u2208R1\u00d7m (byequation(10.7)) \u2202W[2] \u2202J = \u03b4 [2] \u2208R (byequation(10.8)) \u2202b[2] \u2202J = \u03b4 [1] x (cid:62) \u2208Rm\u00d7d (byequation(10.9)) \u2202W[1] \u2202J = \u03b4 [1] \u2208Rm (asanexercise) \u2202b[1] 10.3 Multi-layerneuralnetworks In this section, we will derive the backpropagation algorithms for the model definedinequation(9.16).Withthenotationa[0] = x,recallthatwehave: a [1] =ReLU(W [1] a [0]+b [1]) a [2] =ReLU(W [2] a [1]+b [2]) \u00b7\u00b7\u00b7 a [r\u22121] =ReLU(W [r\u22121] a [r\u22122]+b [r\u22121]) a [r] = z [r] =W [r] a [r\u22121]+b [r] 1(cid:16) (cid:17)2 J = a [r]\u2212y 2 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 10.3. multi-layer neural networks 93 Herewedefinebotha[r] andz[r] ash (x)fornotationalsimplicity.",
      "chunk_id": 156,
      "start_pos": 149210,
      "end_pos": 150341,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "id:16) (cid:17)2 J = a [r]\u2212y 2 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 10.3. multi-layer neural networks 93 Herewedefinebotha[r] andz[r] ash (x)fornotationalsimplicity. \u03b8 First,wenotethatwehavethefollowinglocalabstractionfork \u2208 {1,...,r}: z [k] =W [k] a [k\u22121]+b [k] J = J(z [k]) InvokingClaim1,wehavethat \u2202J = \u2202J \u00b7a [k\u22121](cid:62) (10.12) \u2202W[k] \u2202z[k] \u2202J \u2202J = (10.13) \u2202b[k] \u2202z[k] Therefore,itsufficestocompute \u2202J .Forsimplicity,let\u2019sdefine\u03b4[k] (cid:44) \u2202J .We \u2202z[k] \u2202z[k] compute\u03b4[k] fromk =rto1inductively.Firstwehavethat: \u2202J \u03b4 [r] (cid:44) = (z [r]\u2212y) (10.14) \u2202z[r] Nextfork \u2264r\u22121,supposewehavecomputedthevalueof\u03b4[k+1],thenwewill compute\u03b4[k].First,usingclaim2,wehavethat: \u2202J \u2202J \u03b4 [k] (cid:44) = (cid:12)ReLU (cid:48)(z [k]) (10.15) \u2202z[k] \u2202a[k] Thenwenotethattherelationshipbetweena[k]andz[k+1]canbeabstractlywritten as: z [k+1] =W [k+1] a [k]+b [k+1] (10.16) J = J(z [k+1]) (10.17) Thereforebyclaim3wehavethat: \u2202J =W [k+1](cid:62) \u2202J (10.18) \u2202a[k] \u2202z[k+1] Itfollowsthat: (cid:18) (cid:19) \u03b4 [k] = W [k+1](cid:62) \u2202J (cid:12)ReLU (cid:48)(z [k]) \u2202z[k+1] (cid:16) (cid:17) = W [k+1](cid:62) \u03b4 [k+1] (cid:12)ReLU (cid:48)(z [k]) toc 2021-05-2300:18:27-07:00,draft: sendcom",
      "chunk_id": 157,
      "start_pos": 150141,
      "end_pos": 151341,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ": (cid:18) (cid:19) \u03b4 [k] = W [k+1](cid:62) \u2202J (cid:12)ReLU (cid:48)(z [k]) \u2202z[k+1] (cid:16) (cid:17) = W [k+1](cid:62) \u03b4 [k+1] (cid:12)ReLU (cid:48)(z [k]) toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 94 chapter 10. backpropagation Computeandstorethevaluesofa[k]\u2019sandz[k]\u2019sfork =1,...,r,and J. Algorithm10.2.Back-propagation formulti-layerneuralnetworks. (cid:46)(Thisisoftencalledthe\u2018\u2018forwardpass\u2019\u2019.) fork =rto1do (cid:46)(Thisisoftencalledthe\u2018\u2018backwardpass\u2019\u2019) ifk =rthen Compute\u03b4[r] (cid:44) \u2202J \u2202z[r] else Compute: \u03b4 [k] (cid:44) \u2202J = (cid:16) W [k+1](cid:62) \u03b4 [k+1] (cid:17) (cid:12)ReLU (cid:48)(z [k]) \u2202z[k] Compute: \u2202J = \u03b4 [k] a [k\u22121](cid:62) \u2202W[k] \u2202J = \u03b4 [k] \u2202b[k] endfor 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 11 Vectorization Over Training Examples Aswediscussedinchapter9,intheimplementationofneuralnetworks,wewill leveragetheparallelismacrossmultipleexamples.Thismeansthatwewillneedto writetheforwardpass(theevaluationoftheoutputs)oftheneuralnetworkand thebackwardpass(backpropagation)formultipletrainingexamplesinmatrix notation. Thebasicidea.",
      "chunk_id": 158,
      "start_pos": 151141,
      "end_pos": 152258,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ltipleexamples.Thismeansthatwewillneedto writetheforwardpass(theevaluationoftheoutputs)oftheneuralnetworkand thebackwardpass(backpropagation)formultipletrainingexamplesinmatrix notation. Thebasicidea. Thebasicideaissimple.Supposeyouhaveatrainingsetwith threeexamplesx(1),x(2),x(3).Thefirst-layeractivationsforeachexampleareas follows: z [1](1) =W [1] x (1)+b [1] z [1](2) =W [1] x (2)+b [1] z [1](3) =W [1] x (3)+b [1] Notethedifferencebetweensquarebrackets[\u00b7],whichrefertothelayernumber, and parenthesis (\u00b7), which refer to the training example number. Intuitively, onewouldimplementthisusingaforloop.Itturnsout,wecanvectorizethese operationsaswell.First,define: \uf8ee \uf8f9 X = \uf8efx(1) x(2) x(3)\uf8fa \u2208Rd\u00d73 (11.1) \uf8f0 \uf8fb Notethatwearestackingtrainingexamplesincolumnsandnotrows.Wecan thencombinethisintoasingleunifiedformulation: \uf8ee \uf8f9 Z [1] = \uf8efz[1](1) z[1](2) z[1](3)\uf8fa =W [1] X+b [1] (11.2) \uf8f0 \uf8fb 96 chapter 11. vectorization over training examples Youmaynoticethatweareattemptingtoaddb[1] \u2208R4\u00d71toW[1]X \u2208R4\u00d73.Strictly followingtherulesoflinearalgebra,thisisnotallowed.Inpracticehowever,this additionisperformedusingbroadcasting.Wecreateanintermediateb\u02dc[1] \u2208R4\u00d73: \uf8ee \uf8f9 b\u02dc[1] = \uf8efb[1] b[1] b[1]\uf8fa (11.3) \uf8f0 \uf8fb We can then per",
      "chunk_id": 159,
      "start_pos": 152058,
      "end_pos": 153258,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "followingtherulesoflinearalgebra,thisisnotallowed.Inpracticehowever,this additionisperformedusingbroadcasting.Wecreateanintermediateb\u02dc[1] \u2208R4\u00d73: \uf8ee \uf8f9 b\u02dc[1] = \uf8efb[1] b[1] b[1]\uf8fa (11.3) \uf8f0 \uf8fb We can then perform the computation: Z[1] = W[1]X+b\u02dc[1]. Often times, it is notnecessarytoexplicitlyconstructb\u02dc[1].Byinspectingthedimensionsinequa- tion(11.1),youcanassumeb[1] \u2208R4\u00d71iscorrectlybroadcasttoW[1]X \u2208R4\u00d73. Thematricizationapproachasabovecaneasilygeneralizetomultiplelayers, withonesubtletythough,asdiscussedbelow. Complications/subtletyintheimplementation. Allthedeeplearningpackages orimplementationsputthedatapointsintherowsofadatamatrix.(Ifthedata pointitselfisamatrixortensor,thenthedataareconcentratedalongthezero-th dimension.)However,mostofthedeeplearningpapersuseasimilarnotation to these notes where the data points are treated as column vectors.1 There is 1Theinstructorsuspectsthatthis asimpleconversiontodealwiththemismatch:intheimplementation,allthe ismostlybecauseinmathematics wenaturallymultiplyamatrixtoa columnsbecomerowvectors,rowvectorsbecomecolumnvectors,allthematrices vectoronthelefthandside.",
      "chunk_id": 160,
      "start_pos": 153058,
      "end_pos": 154168,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "odealwiththemismatch:intheimplementation,allthe ismostlybecauseinmathematics wenaturallymultiplyamatrixtoa columnsbecomerowvectors,rowvectorsbecomecolumnvectors,allthematrices vectoronthelefthandside. aretransposed,andtheordersofthematrixmultiplicationsareflipped.Inthe exampleabove,usingtherowmajorconvention,thedatamatrixisX \u2208R3\u00d7d,the firstlayerweightmatrixhasdimensionalityd\u00d7m(insteadofm\u00d7dasinthetwo layerneuralnetsection),andthebiasvectorb[1] \u2208R1\u00d7m.Thecomputationfor thehiddenactivationbecomes: Z [1] = XW [1]+b [1] \u2208R3\u00d7m (11.4) 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 97 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu Part VI: Regularization and Model Selection FromCS229Spring2021,Andrew Supposewearetryingselectamongseveraldifferentmodelsforalearning Ng,MosesCharikar,Christopher problem.Forinstance,wemightbeusingapolynomialregressionmodelh (x) = R\u00e9&YoannLeCalonnec,Stanford \u03b8 g(\u03b8 +\u03b8 x+\u03b8 x2+\u00b7\u00b7\u00b7+\u03b8 xk), and wish to decide if k should be 0,1,..., or University. 0 1 2 k 10.",
      "chunk_id": 161,
      "start_pos": 153968,
      "end_pos": 155004,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "stopher problem.Forinstance,wemightbeusingapolynomialregressionmodelh (x) = R\u00e9&YoannLeCalonnec,Stanford \u03b8 g(\u03b8 +\u03b8 x+\u03b8 x2+\u00b7\u00b7\u00b7+\u03b8 xk), and wish to decide if k should be 0,1,..., or University. 0 1 2 k 10. How can we automatically select a model that represents a good tradeoff betweenthetwinevilsofbiasandvariance?2Alternatively,supposewewantto 2Giventhatwesaidintheprevi- automaticallychoosethebandwidthparameter\u03c4forlocallyweightedregression, oussetofnotesthatbiasandvari- ancearetwoverydifferentbeasts, ortheparameterCforour(cid:96) -regularizedSVM.Howcanwedothat? 1 somereadersmaybewonderingif Forthesakeofconcreteness,inthesenotesweassumewehavesomefinite weshouldbecallingthem\u2018\u2018twin\u2019\u2019 evils here. Perhaps it\u2019d be better setofmodelsM = {M ,...,M }thatwe\u2019retryingtoselectamong.Forinstance, 1 d tothinkofthemasnon-identical in our first example above, the model M i would be an i-th order polynomial twins. The phrase \u2018\u2018the fraternal regressionmodel.(ThegeneralizationtoinfiniteMisnothard.3)Alternatively, twin evils of bias and variance\u2019\u2019 doesn\u2019t have the same ring to it, ifwearetryingtodecidebetweenusinganSVM,aneuralnetworkorlogistic though. regression,thenMmaycontainthesemodels.",
      "chunk_id": 162,
      "start_pos": 154804,
      "end_pos": 155985,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "hard.3)Alternatively, twin evils of bias and variance\u2019\u2019 doesn\u2019t have the same ring to it, ifwearetryingtodecidebetweenusinganSVM,aneuralnetworkorlogistic though. regression,thenMmaycontainthesemodels. 3Ifwearetryingtochoosefrom aninfinitesetofmodels,saycor- respondingtothepossiblevalues ofthebandwidth\u03c4\u2208R+,wemay 12 Cross validation discretize\u03c4andconsideronlyafi- nitenumberofpossiblevaluesfor it.Moregenerally,mostoftheal- gorithmsdescribedherecanallbe viewedasperformingoptimization searchinthespaceofmodels,and Letssupposeweare,asusual,givenatrainingsetS.Givenwhatweknowabout wecanperformthissearchoverin- empiricalriskminimization,here\u2019swhatmightinitiallyseemlikeaalgorithm, finitemodelclassesaswell. resultingfromusingempiricalriskminimizationformodelselection: 1. Traineachmodel M onS,togetsomehypothesish. i i 2. Pickthehypotheseswiththesmallesttrainingerror. Thisalgorithmdoesnotwork.Considerchoosingtheorderofapolynomial. Thehighertheorderofthepolynomial,thebetteritwillfitthetrainingset S, andthusthelowerthetrainingerror.Hence,thismethodwillalwaysselecta high-variance,high-degreepolynomialmodel,whichwesawpreviouslyisoften poorchoice.",
      "chunk_id": 163,
      "start_pos": 155785,
      "end_pos": 156931,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "erofthepolynomial,thebetteritwillfitthetrainingset S, andthusthelowerthetrainingerror.Hence,thismethodwillalwaysselecta high-variance,high-degreepolynomialmodel,whichwesawpreviouslyisoften poorchoice. Here\u2019sanalgorithmthatworksbetter.Inhold-outcrossvalidation(alsocalled simplecrossvalidation),wedothefollowing: 99 1. RandomlysplitSintoS (say,70%ofthedata)andS (theremaining30%). train cv Here,S iscalledthehold-outcrossvalidationset. cv 2. Traineachmodel M onS only,togetsomehypothesish. i train i 3. Selectandoutputthehypothesish thathadthesmallesterror\u03b5\u02c6 (h )onthe i Scv i holdoutcrossvalidationset.(Recall,\u03b5\u02c6 (h)denotestheempiricalerrorofh Scv onthesetofexamplesinS .) cv BytestingonasetofexamplesS thatthemodelswerenottrainedon,we cv obtain a better estimate of each hypothesis h\u2019s true generalization error, and i canthenpicktheonewiththesmallestestimatedgeneralizationerror.Usually, somewherebetween1/4\u22121/3ofthedataisusedintheholdoutcrossvalidation set,and30%isatypicalchoice. Optionally,step3inthealgorithmmayalsobereplacedwithselectingthe model M according to argmin \u03b5\u02c6 (h ), and then retraining M on the entire i i Scv i i training set S.",
      "chunk_id": 164,
      "start_pos": 156731,
      "end_pos": 157879,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "idation set,and30%isatypicalchoice. Optionally,step3inthealgorithmmayalsobereplacedwithselectingthe model M according to argmin \u03b5\u02c6 (h ), and then retraining M on the entire i i Scv i i training set S. (This is often a good idea, with one exception being learning algorithmsthatarebeverysensitivetoperturbationsoftheinitialconditions and/ordata.Forthesemethods,M doingwellonS doesnotnecessarilymean i train itwillalsodowellonS ,anditmightbebettertoforgothisretrainingstep.) cv Thedisadvantageofusingholdoutcrossvalidationisthatit\u2018\u2018wastes\u2019\u2019about 30%ofthedata.Evenifweweretotaketheoptionalstepofretrainingthemodel ontheentiretrainingset,it\u2019sstillasifwe\u2019retryingtofindagoodmodelfora learningprobleminwhichwehad0.7mtrainingexamples,ratherthanntraining examples,sincewe\u2019retestingmodelsthatweretrainedononly0.7mexamples eachtime.Whilethisisfineifdataisabundantand/orcheap,inlearningproblems inwhichdataisscarce(consideraproblemwithm = 20,say),we\u2019dliketodo somethingbetter. Hereisamethod,calledk-foldcrossvalidation,thatholdsoutlessdataeach time: 1. RandomlysplitSintokdisjointsubsetsofm/ktrainingexampleseach.Lets callthesesubsetsS ,...,S . 1 k 2.",
      "chunk_id": 165,
      "start_pos": 157679,
      "end_pos": 158819,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "liketodo somethingbetter. Hereisamethod,calledk-foldcrossvalidation,thatholdsoutlessdataeach time: 1. RandomlysplitSintokdisjointsubsetsofm/ktrainingexampleseach.Lets callthesesubsetsS ,...,S . 1 k 2. Foreachmodel M,weevaluateitasfollows: i \u2022 Forj =1,...,k: \u2013 TrainthemodelM i onS 1 \u222a\u00b7\u00b7\u00b7\u222aS j\u22121 \u222aS j+1 \u222a\u00b7\u00b7\u00b7S k (i.e.,trainonallthe dataexceptS )togetsomehypothesish . j ij toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu \u2013 Testthehypothesish onS ,toget\u03b5\u02c6 (h ). ij j Sj ij \u2022 Theestimatedgeneralizationerrorofmodel M isthencalculatedasthe i averageofthe\u03b5\u02c6 (h )\u2019s(averagedoverj). Sj ij 3. Pickthemodel M withthelowestestimatedgeneralizationerror,andretrain i thatmodelontheentiretrainingsetS.Theresultinghypothesisisthenoutput asourfinalanswer. Atypicalchoiceforthenumberoffoldstouseherewouldbek =10.While thefractionofdataheldouteachtimeisnow1/k\u2014muchsmallerthanbefore\u2014 thisproceduremayalsobemorecomputationallyexpensivethanhold-outcross validation,sincewenowneedtraintoeachmodelktimes. Whilek =10isacommonlyusedchoice,inproblemsinwhichdataisreally scarce,sometimeswewillusetheextremechoiceofk = minordertoleaveout as little data as possible each time.",
      "chunk_id": 166,
      "start_pos": 158619,
      "end_pos": 159789,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "cewenowneedtraintoeachmodelktimes. Whilek =10isacommonlyusedchoice,inproblemsinwhichdataisreally scarce,sometimeswewillusetheextremechoiceofk = minordertoleaveout as little data as possible each time. In this setting, we would repeatedly train onallbutoneofthetrainingexamplesinS,andtestonthatheld-outexample. Theresultingm = kerrorsarethenaveragedtogethertoobtainourestimateof thegeneralizationerrorofamodel.Thismethodhasitsownname;sincewe\u2019re holdingoutonetrainingexampleatatime,thismethodiscalledleave-one-out crossvalidation. Finally,eventhoughwehavedescribedthedifferentversionsofcrossvalidation asmethodsforselectingamodel,theycanalsobeusedmoresimplytoevaluatea singlemodeloralgorithm.Forexample,ifyouhaveimplementedsomelearning algorithmandwanttoestimatehowwellitperformsforyourapplication(orif youhaveinventedanovellearningalgorithmandwanttoreportinatechnical paperhowwellitperformsonvarioustestsets),crossvalidationwouldgivea reasonablewayofdoingso. 13 Feature Selection Onespecialandimportantcaseofmodelselectioniscalledfeatureselection.",
      "chunk_id": 167,
      "start_pos": 159589,
      "end_pos": 160635,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "toreportinatechnical paperhowwellitperformsonvarioustestsets),crossvalidationwouldgivea reasonablewayofdoingso. 13 Feature Selection Onespecialandimportantcaseofmodelselectioniscalledfeatureselection. Tomotivatethis,imaginethatyouhaveasupervisedlearningproblemwherethe numberoffeaturesdisverylarge(perhapsd (cid:29) n),butyoususpectthatthereis onlyasmallnumberoffeaturesthatare\u2018\u2018relevant\u2019\u2019tothelearningtask.Evenif 101 youusetheasimplelinearclassifier(suchastheperceptron)overthedinput features,theVCdimensionofyourhypothesisclasswouldstillbeO(n),andthus overfittingwouldbeapotentialproblemunlessthetrainingsetisfairlylarge. Insuchasetting,youcanapplyafeatureselectionalgorithmtoreducethe numberoffeatures.Givendfeatures,thereare2d possiblefeaturesubsets(since eachofthedfeaturescaneitherbeincludedorexcludedfromthesubset),and thusfeatureselectioncanbeposedasamodelselectionproblemover2d possible models.Forlargevaluesofd,it\u2019susuallytooexpensivetoexplicitlyenumerate overandcompareall2dmodels,andsotypicallysomeheuristicsearchprocedure isusedtofindagoodfeaturesubset.Thefollowingsearchprocedureiscalled forwardsearch: InitializeF = \u2205. Algorithm13.1.Forwardsearch.",
      "chunk_id": 168,
      "start_pos": 160435,
      "end_pos": 161597,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ate overandcompareall2dmodels,andsotypicallysomeheuristicsearchprocedure isusedtofindagoodfeaturesubset.Thefollowingsearchprocedureiscalled forwardsearch: InitializeF = \u2205. Algorithm13.1.Forwardsearch. repeat fori =1,...,ddo ifi (cid:54)\u2208 F then F = F \u222a{i} i UsesomeversionofcrossvalidationtoevaluatefeaturesF. i (i.e.,trainyourlearningalgorithmusingonlythefeaturesinF, i andestimateitsgeneralizationerror.) endfor SetF tobethebestfeaturesubsetfoundinthepreviousstep. untilconvergence Selectandoutputthebestfeaturesubsetthatwasevaluatedduringthe entiresearchprocedure. TheouterloopofthealgorithmcanbeterminatedeitherwhenF = {1,...,d} isthesetofallfeatures,orwhen|F|exceedssomepre-setthreshold(correspond- ingtothemaximumnumberoffeaturesthatyouwantthealgorithmtoconsider using). Thisalgorithmdescribedaboveoneinstantiationofwrappermodelfeature selection,sinceitisaprocedurethat\u2018\u2018wraps\u2019\u2019aroundyourlearningalgorithm, and repeatedly makes calls to the learning algorithm to evaluate how well it doesusingdifferentfeaturesubsets.Asidefromforwardsearch,othersearch procedurescanalsobeused.Forexample,backwardsearchstartsoffwithF = toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 102",
      "chunk_id": 169,
      "start_pos": 161397,
      "end_pos": 162597,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ifferentfeaturesubsets.Asidefromforwardsearch,othersearch procedurescanalsobeused.Forexample,backwardsearchstartsoffwithF = toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 102 chapter 13. feature selection {1,...,d}asthesetofallfeatures,andrepeatedlydeletesfeaturesoneatatime (evaluatingsingle-featuredeletionsinasimilarmannertohowforwardsearch evaluatessingle-featureadditions)untilF = \u2205. Wrapperfeatureselectionalgorithmsoftenworkquitewell,butcanbecompu- tationallyexpensivegivenhowthattheyneedtomakemanycallstothelearning algorithm.Indeed,completeforwardsearch(terminatingwhenF = {1,...,d}) wouldtakeaboutO(n2)callstothelearningalgorithm. Filter feature selection methods give heuristic, but computationally much cheaper,waysofchoosingafeaturesubset.Theideahereistocomputesome simplescoreS(i)thatmeasureshowinformativeeachfeaturex isabouttheclass i labelsy.Then,wesimplypickthekfeatureswiththelargestscoresS(i). OnepossiblechoiceofthescorewouldbedefineS(i)tobe(theabsolutevalue of)thecorrelationbetweenx andy,asmeasuredonthetrainingdata.Thiswould i result in our choosing the features that are the most strongly correlated with theclasslabels.Inpractice,itismorecommon(part",
      "chunk_id": 170,
      "start_pos": 162397,
      "end_pos": 163597,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "alue of)thecorrelationbetweenx andy,asmeasuredonthetrainingdata.Thiswould i result in our choosing the features that are the most strongly correlated with theclasslabels.Inpractice,itismorecommon(particularlyfordiscrete-valued featuresx)tochooseS(i)tobethemutualinformationMI(x,y)betweenx and i i i y: MI(x,y) = \u2211 \u2211 p(x,y)log p(x i ,y) (13.1) i i p(x )p(y) xi \u2208{0,1}y\u2208{0,1} i (Theequationaboveassumesthat x andyarebinary-valued;moregenerally i thesummationswouldbeoverthedomainsofthevariables.)Theprobabilities above p(x,y), p(x )and p(y)canallbeestimatedaccordingtotheirempirical i i distributionsonthetrainingset. Togainintuitionaboutwhatthisscoredoes,notethatthemutualinformation canalsobeexpressedasaKullback-Leibler(KL)divergence: MI(x,y) =KL(p(x,y) || p(x )p(y)) (13.2) i i i You\u2019llgettoplaymorewithKL-divergenceintheproblemsets,butinformally, thisgivesameasureofhowdifferenttheprobabilitydistributions p(x,y)and i p(x )p(y)are.Ifx andyareindependentrandomvariables,thenwewouldhave i i p(x,y) = p(x )p(y),andtheKL-divergencebetweenthetwodistributionswill i i bezero.Thisisconsistentwiththeideaif x andyareindependent,then x is i i clearlyvery\u2018\u2018non-informative\u2019\u2019abouty,andthusthescoreS(i)shouldb",
      "chunk_id": 171,
      "start_pos": 163397,
      "end_pos": 164597,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "p(x )p(y),andtheKL-divergencebetweenthetwodistributionswill i i bezero.Thisisconsistentwiththeideaif x andyareindependent,then x is i i clearlyvery\u2018\u2018non-informative\u2019\u2019abouty,andthusthescoreS(i)shouldbesmall. Conversely,if x isvery\u2018\u2018informative\u2019\u2019abouty,thentheirmutualinformation i MI(x,y)wouldbelarge. i 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc Onefinaldetail:Nowthatyou\u2019verankedthefeaturesaccordingtotheirscores S(i),howdoyoudecidehowmanyfeaturesktochoose?Well,onestandardway todosoistousecrossvalidationtoselectamongthepossiblevaluesofk.For example,whenapplyingnaiveBayestotextclassification\u2014aproblemwhered, thevocabularysize,isusuallyverylarge\u2014usingthismethodtoselectafeature subsetoftenresultsinincreasedclassifieraccuracy. 14 Bayesian statistics and regularization Inthissection,wewilltalkaboutonemoretoolinourarsenalforourbattle againstoverfitting. Atthebeginningofthequarter,wetalkedaboutparameterfittingusingmaxi- mumlikelihoodestimation(MLE),andchoseourparametersaccordingto n \u03b8 =argmax \u220f p(y (i) | x (i) ;\u03b8).",
      "chunk_id": 172,
      "start_pos": 164397,
      "end_pos": 165446,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "enalforourbattle againstoverfitting. Atthebeginningofthequarter,wetalkedaboutparameterfittingusingmaxi- mumlikelihoodestimation(MLE),andchoseourparametersaccordingto n \u03b8 =argmax \u220f p(y (i) | x (i) ;\u03b8). (14.1) MLE \u03b8 i=1 Throughoutoursubsequentdiscussions,weviewed\u03b8asanunknownparameter oftheworld.Thisviewofthe\u03b8asbeingconstant-valuedbutunknownistakenin frequentiststatistics.Inthefrequentistthisviewoftheworld,\u03b8isnotrandom\u2014it justhappenstobeunknown\u2014andit\u2019sourjobtocomeupwithstatisticalproce- dures(suchasmaximumlikelihood)totrytoestimatethisparameter. Analternativewaytoapproachourparameterestimationproblemsistotake theBayesianviewoftheworld,andthinkof\u03b8asbeingarandomvariablewhose valueisunknown.Inthisapproach,wewouldspecifyapriordistribution p(\u03b8) on\u03b8thatexpressesour\u2018\u2018priorbeliefs\u2019\u2019abouttheparameters.Givenatrainingset S = {(x(i),y(i))}n ,whenweareaskedtomakeapredictiononanewvalueof i=1 x,wecanthencomputetheposteriordistributionontheparameters: p(S | \u03b8)p(\u03b8) p(\u03b8 | S) = (14.2) p(S) (cid:16) (cid:17) \u220fn p(y(i) | x(i),\u03b8) p(\u03b8) i=1 = (cid:82) (cid:0)\u220fn p(y(i) | x(i),\u03b8)p(\u03b8) (cid:1) d\u03b8 (14.3) \u03b8 i=1 Intheequationabove,p(y(i) | x(i),\u03b8)comesfromwhatevermodelyou\u2019reusingfor yourlearningproblem.Forexample,i",
      "chunk_id": 173,
      "start_pos": 165246,
      "end_pos": 166446,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\u220fn p(y(i) | x(i),\u03b8) p(\u03b8) i=1 = (cid:82) (cid:0)\u220fn p(y(i) | x(i),\u03b8)p(\u03b8) (cid:1) d\u03b8 (14.3) \u03b8 i=1 Intheequationabove,p(y(i) | x(i),\u03b8)comesfromwhatevermodelyou\u2019reusingfor yourlearningproblem.Forexample,ifyouareusingBayesianlogisticregression, thenyoumightchoose p(y(i) | x(i),\u03b8) = h (x(i))y(i) (1\u2212h (x(i)))(1\u2212y(i) ),where \u03b8 \u03b8 h \u03b8 (x(i)) =1/(1+exp(\u2212\u03b8(cid:62)x(i))).1 1Since we are now viewing \u03b8 as Whenwearegivenanewtestexamplexandaskedtomakeapredictiononit, a random variable, it is okay to conditiononitsvalue,andwrite wecancomputeourposteriordistributionontheclasslabelusingtheposterior \u2018\u2018p(y|x,\u03b8)\u2019\u2019insteadof\u2018\u2018p(y|x;\u03b8).\u2019\u2019 distributionon\u03b8: (cid:90) p(y | x,S) = p(y | x,\u03b8)p(\u03b8 | S)d\u03b8 (14.4) \u03b8 Intheequationabove, p(\u03b8 | S)comesfromequation(14.2).Thus,forexample,if thegoalistothepredicttheexpectedvalueofygivenx,thenwewouldoutput:2 2Theintegralbelowwouldbere- placed by a summation if y is (cid:90) E[y | x,S] = yp(y | x,S)dy (14.5) discrete-valued. y The procedure that we\u2019ve outlined here can be thought of as doing \u2018\u2018fully Bayesian\u2019\u2019prediction,whereourpredictioniscomputedbytakinganaverage withrespecttotheposterior p(\u03b8 | S)over\u03b8.Unfortunately,ingeneralitiscom- putationallyverydifficulttocomputethisp",
      "chunk_id": 174,
      "start_pos": 166246,
      "end_pos": 167446,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "t of as doing \u2018\u2018fully Bayesian\u2019\u2019prediction,whereourpredictioniscomputedbytakinganaverage withrespecttotheposterior p(\u03b8 | S)over\u03b8.Unfortunately,ingeneralitiscom- putationallyverydifficulttocomputethisposteriordistribution.Thisisbecause it requires taking integrals over the (usually high-dimensional) \u03b8 as in equa- tion(14.2),andthistypicallycannotbedoneinclosed-form. Thus,inpracticewewillinsteadapproximatetheposteriordistributionfor\u03b8. Onecommonapproximationistoreplaceourposteriordistributionfor\u03b8(asin equation(14.4))withasinglepointestimate.TheMAP(maximumaposteriori) estimatefor\u03b8isgivenby: n \u03b8 =argmax \u220f p(y (i) | x (i) ,\u03b8)p(\u03b8) (14.6) MAP \u03b8 i=1 NotethatthisisthesameformulasasfortheMLE(maximumlikelihood)estimate for\u03b8,exceptfortheprior p(\u03b8)termattheend. In practical applications, a common choice for the prior p(\u03b8) is to assume that\u03b8 \u223c N(0,\u03c42I).Usingthischoiceofprior,thefittedparameters\u03b8MAPwill havesmallernormthanthatselectedbymaximumlikelihood.Inpractice,this causestheBayesianMAPestimatetobelesssusceptibletooverfittingthantheML estimateoftheparameters.Forexample,Bayesianlogisticregressionturnsoutto beaneffectivealgorithmfortextclassification,eventhoughintextclassification weusuallyhaved",
      "chunk_id": 175,
      "start_pos": 167246,
      "end_pos": 168446,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "lesssusceptibletooverfittingthantheML estimateoftheparameters.Forexample,Bayesianlogisticregressionturnsoutto beaneffectivealgorithmfortextclassification,eventhoughintextclassification weusuallyhaved (cid:29) n. 105 15 Some calculations from bias variance FromCS229Fall2020,Christopher Thissectioncontainsarepriseoftheeigenvalueargumentstounderstandhow R\u00e9,StanfordUniversity. varianceisreducedbyregularization.Wealsodescribedifferentwaysregulariza- tioncanoccurincludingfromthealgorithmorinitialization.Thisnotecontains someadditionalcalculationsfromthelectureandPiazza,justsothatwehave typesetversionsofthem.Theycontainnonewinformationoverthelecture,but theydosupplementtheprevioussections. RecallwehaveadesignmatrixX \u2208Rn\u00d7dandlabelsy \u2208Rn.Weareinterested intheunderdeterminedcasen < dsothatrank(X) \u2264 n < d.Weconsiderthe followingoptimizationproblemforleastsquareswitharegularizationparameter \u03bb \u22650: 1 \u03bb (cid:96)(\u03b8;\u03bb) = min (cid:107)X\u03b8\u2212y(cid:107)2+ (cid:107)\u03b8(cid:107)2 (15.1) \u03b8\u2208Rd 2 2 Normalequations. Computingderivativesaswedidforthenormalequations, weseethat: \u2207 (cid:96)(\u03b8;\u03bb) = X (cid:62)(X\u03b8\u2212y)+\u03bb\u03b8 = (X (cid:62) X+\u03bbI)\u03b8\u2212X (cid:62) y (15.2) \u03b8 Bysetting\u2207 (cid:96)(\u03b8,\u03bb) =0wecansolveforthe\u03b8\u02c6thatminimize",
      "chunk_id": 176,
      "start_pos": 168246,
      "end_pos": 169446,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "Computingderivativesaswedidforthenormalequations, weseethat: \u2207 (cid:96)(\u03b8;\u03bb) = X (cid:62)(X\u03b8\u2212y)+\u03bb\u03b8 = (X (cid:62) X+\u03bbI)\u03b8\u2212X (cid:62) y (15.2) \u03b8 Bysetting\u2207 (cid:96)(\u03b8,\u03bb) =0wecansolveforthe\u03b8\u02c6thatminimizestheaboveproblem. \u03b8 Explicitly,wehave: \u03b8\u02c6 = (X (cid:62) X+\u03bbI)\u22121X (cid:62) y (15.3) Toseethattheinverseinequation(15.3)exists,weobservethatX(cid:62)Xisasym- metric,reald\u00d7dmatrixsoithasdeigenvalues(somemaybe0).Moreover,itis positivesemidefinite,andwecapturethisbywritingeig(X(cid:62)X) = {\u03c32,...,\u03c32}. 1 d Now,inspiredbytheregularizedproblem,weexamine: (cid:110) (cid:111) eig(X (cid:62) X+\u03bbI) = \u03c32+\u03bb,...,\u03c32+\u03bb (15.4) 1 d Since\u03c32 \u2265 0foralli \u2208 [d],ifweset\u03bb > 0thenX(cid:62)X+\u03bbI isfullrank,andthe i inverseof(X(cid:62)X+\u03bbI)exists.Inturn,thismeansthereisauniquesuch\u03b8\u02c6. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 106 chapter 15. some calculations from bias variance Variance. Recallthatinbias-variance,weareconcernedwiththevarianceof\u03b8\u02c6as wesamplethetrainingset.Wewanttoarguethatastheregularizationparameter \u03bb increases,thevarianceinthefitted \u03b8\u02c6 decreases.Wewon\u2019tcarryoutthefull formalargument,butitsufficestomakeoneobservationthatisimmediatefrom equation(15.3):thevarianceof\u03b8\u02c6isp",
      "chunk_id": 177,
      "start_pos": 169246,
      "end_pos": 170446,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "stheregularizationparameter \u03bb increases,thevarianceinthefitted \u03b8\u02c6 decreases.Wewon\u2019tcarryoutthefull formalargument,butitsufficestomakeoneobservationthatisimmediatefrom equation(15.3):thevarianceof\u03b8\u02c6isproportionaltotheeigenvaluesof(X(cid:62)X+\u03bbI)\u22121. Toseethis,observethattheeigenvaluesofaninversearejusttheinverseofthe eigenvalues: (cid:40) (cid:41) (cid:16) (cid:17) 1 1 eig (X (cid:62) X+\u03bbI)\u22121 = ,..., +\u03bb (15.5) \u03c32+\u03bb \u03c32 1 d Now,conditiononthepointswedraw,namelyX.Then,recallthatrandomness isinthelabelnoise(recallthelinearregressionmodely \u223c X\u03b8\u2217+N(0,\u03c42I) = N(X\u03b8\u2217,\u03c42I)). Recallafactaboutthemultivariatenormaldistribution: ify \u223c N(\u00b5,\u03a3)then Ay \u223c N(A\u00b5,A\u03a3A (cid:62)) (15.6) Usinglinearity,wecanverifythattheexpectationof\u03b8\u02c6is: (cid:104) (cid:105) E[\u03b8\u02c6] =E (X (cid:62) X+\u03bbI)\u22121X (cid:62) y (15.7) (cid:104) (cid:105) =E (X (cid:62) X+\u03bbI)\u22121X (cid:62)(X\u03b8 \u2217+N(0,\u03c42I)) (15.8) (cid:104) (cid:105) =E (X (cid:62) X+\u03bbI)\u22121X (cid:62)(X\u03b8 \u2217) (15.9) = (X (cid:62) X+\u03bbI)\u22121(X (cid:62) X)\u03b8 \u2217 (essentiallya\u2018\u2018shrunk\u2019\u2019\u03b8\u2217) Thelastlineabovesuggeststhatthemoreregularizationweadd(largerthe\u03bb), themoretheestimated\u03b8\u02c6willbeshrunktowards0.Inotherwords,regularization addsbias(towardszerointhiscase).Thoughwepaidthecostofhigherbias,we",
      "chunk_id": 178,
      "start_pos": 170246,
      "end_pos": 171446,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "stlineabovesuggeststhatthemoreregularizationweadd(largerthe\u03bb), themoretheestimated\u03b8\u02c6willbeshrunktowards0.Inotherwords,regularization addsbias(towardszerointhiscase).Thoughwepaidthecostofhigherbias,we gainbyreducingthevarianceof\u03b8\u02c6.Toseethisbias-variancetradeoffconcretely, observethecovariancematrixof\u03b8\u02c6: C :=Cov[\u03b8\u02c6] (15.10) (cid:16) (cid:17) (cid:16) (cid:17) = (X (cid:62) X+\u03bbI)\u22121X (cid:62) (\u03c42I) X(X (cid:62) X+\u03bbI)\u22121 (15.11) and (cid:40) (cid:41) \u03c42\u03c32 \u03c42\u03c32 eig(C) = 1 ,..., d (15.12) (\u03c32+\u03bb)2 (\u03c32+\u03bb)2 1 d 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc Noticethattheentirespectrumofthecovarianceisadecreasingfunctionof\u03bb.By decomposingintheeigenvaluebasis,wecanseethatactuallyE(cid:2) (cid:107)\u03b8\u02c6\u2212\u03b8\u2217(cid:107)2 (cid:3) isa decreasingfunctionof\u03bb,asdesired. Gradientdescent. Weshowthatyoucaninitializegradientdescentinawaythat effectivelyregularizesundeterminedleastsquares\u2014evenwithnoregularization penalty(\u03bb =0).Ourfirstobservationisthatanypointx \u2208Rdcanbedecomposed intotwoorthogonalcomponentsx ,x suchthat: 0 1 x = x +x andx \u2208Null(X)andx \u2208Range(X (cid:62)) (15.13) 0 1 0 1 RecallthatNull(X)andRange(X(cid:62))areorthogonalsubspacesbythefundamental theoryoflinearalgebra.Wewrite",
      "chunk_id": 179,
      "start_pos": 171246,
      "end_pos": 172446,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "gonalcomponentsx ,x suchthat: 0 1 x = x +x andx \u2208Null(X)andx \u2208Range(X (cid:62)) (15.13) 0 1 0 1 RecallthatNull(X)andRange(X(cid:62))areorthogonalsubspacesbythefundamental theoryoflinearalgebra.WewriteP fortheprojectiononthenullandP forthe 0 1 projectionontherange,thenx = P (x)andx = P (x). 0 0 1 1 Ifoneinitializesatapoint\u03b8then,weobservethatthegradientisorthogonal tothenullspace.Thatis,ifg(\u03b8) = X(cid:62)(X\u03b8\u2212y)theng(cid:62)P (v) =0foranyv \u2208Rd. 0 But,then: P (\u03b8 (t+1)) = P (\u03b8t\u2212\u03b1g(\u03b8 (t))) = P (\u03b8t)\u2212\u03b1P g(\u03b8 (t)) = P (\u03b8 (t)) (15.14) 0 0 0 0 0 Thatis,nolearninghappensinthenull.Whateverportionisinthenullthatwe initializestaystherethroughoutexecution. AkeypropertyoftheMoore-Penrosepseudoinverse,isthatif\u03b8\u02c6 = (X(cid:62)X)+ X(cid:62)ythenP (\u03b8\u02c6) =0.Hence,thegradientdescentsolutioninitializedat\u03b8 canbe 0 0 written\u03b8\u02c6+P (\u03b8 ).Twoimmediateobservations: 0 0 \u2022 UsingtheMoore-Penroseinverseactsasregularization,becauseitselectsthe solution\u03b8\u02c6. \u2022 Sodoesgradientdescent\u2014providedthatweinitializeat\u03b8 =0.Thisispartic- 0 ularlyinteresting,asmanymodernmachinelearningtechniquesoperatein theseunderdeterminedregimes.",
      "chunk_id": 180,
      "start_pos": 172246,
      "end_pos": 173340,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ion,becauseitselectsthe solution\u03b8\u02c6. \u2022 Sodoesgradientdescent\u2014providedthatweinitializeat\u03b8 =0.Thisispartic- 0 ularlyinteresting,asmanymodernmachinelearningtechniquesoperatein theseunderdeterminedregimes. We\u2019ve argued that there are many ways to find equivalent solutions, and that this allows us to understand the effect on the model fitting procedure as regularization.Thus,therearemanywaystofindthatequivalentsolution.Many modernmethodsofmachinelearningincludingdropoutanddataaugmentation arenotpenalty,buttheireffectisunderstoodasregularization.Onecontrastwith theabovemethodsisthattheyoftendependonsomepropertyofthedataorfor 108 chapter 16. bias-variance and error analysis howmuchtheyeffectivelyregularization.Insomesense,theyadapttothedata. Afinalcommentisthatinthesamesenseabove,addingmoredataregularizes themodelaswell! 16 Bias-variance and error analysis FromCS229Fall2017,YoannLe 16.1 Thebias-variancetradeoff Calonnec,StanfordUniversity. Assumeyouaregivenawellfittedmachinelearningmodel f\u02c6thatyouwantto applyonsometestdataset.Forinstance,themodelcouldbealinearregression whoseparameterswerecomputedusingsometrainingsetdifferentfromyour testset.Foreachpointxinyourtestset,youwanttopredicttheas",
      "chunk_id": 181,
      "start_pos": 173140,
      "end_pos": 174340,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "tyouwantto applyonsometestdataset.Forinstance,themodelcouldbealinearregression whoseparameterswerecomputedusingsometrainingsetdifferentfromyour testset.Foreachpointxinyourtestset,youwanttopredicttheassociatedtarget y \u2208R,andcomputethemeansquarederror(MSE): (cid:104) (cid:105) E |f\u02c6(x)\u2212y|2 (16.1) (x,y)\u223ctestset YounowrealizethatthisMSEistoohigh,andtrytofindanexplanationtothis result: \u2022 Overfitting:themodelistoocloselyrelatedtotheexamplesinthetrainingset anddoesn\u2019tgeneralizewelltootherexamples. \u2022 Underfitting:themodeldidn\u2019tgatherenoughinformationfromthetrainingset, anddoesn\u2019tcapturethelinkbetweenthefeaturesxandthetargety. \u2022 Thedataissimplynoisy,thatisthemodelisneitheroverfittingorunderfitting, andthehighMSEissimplyduetotheamountofnoiseinthedataset. Ourintuitioncanbeformalizedbythebias-variancetradeoff. Assumethatthepointsinyourtraining/testsetarealltakenfromasimilar distribution,with y = f(x )+(cid:101), wherethenoise(cid:101) satisfies E((cid:101)) =0,Var((cid:101)) = \u03c32 (16.2) i i i i i i 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 16.1. the bias-variance tradeoff 109 and your goal is to compute f.",
      "chunk_id": 182,
      "start_pos": 174140,
      "end_pos": 175281,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "sfies E((cid:101)) =0,Var((cid:101)) = \u03c32 (16.2) i i i i i i 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 16.1. the bias-variance tradeoff 109 and your goal is to compute f. By looking at your training set, you obtain an estimate f\u02c6.Nowusethisestimatewithyourtestset,meaningthatforeachexample j inthetestset,yourpredictionfor y = f(x )+(cid:101) is f\u02c6(x ).Here, x isafixed j j j j j realnumber(orvectorifthefeaturespaceismulti-dimensional)thus f(x )is j fixed,and(cid:101) isarealrandomvariablewithmean0andvariance\u03c32.Thecrucial j observationisthat f\u02c6(x )israndomsinceitdependsonthevalues(cid:101) fromthe j i trainingset.That\u2019swhytalkingaboutthebiasE[f\u02c6(x)\u2212 f(x)]andthevarianceof f\u02c6makessense. We can now compute our MSE on the test set by computing the following expectationwithrespecttothepossibletrainingsets(since f\u02c6isarandomvariable functionofthechoiceofthetraningset): (cid:104) (cid:105) testMSE=E (y\u2212 f\u02c6(x))2 (16.3) (cid:104) (cid:105) =E (((cid:101)+ f(x)\u2212 f\u02c6(x))2 (16.4) (cid:104) (cid:105) =E[(cid:101)2]+E (f(x)\u2212 f\u02c6(x))2 (16.5) (cid:16) (cid:17)2 (cid:16) (cid:17) = \u03c32+ E[f(x)\u2212 f\u02c6(x)] +Var f(x)\u2212 f\u02c6(x) (16.6) (cid:16) (cid:17)2 (cid:16) (cid:17) = \u03c32+ Bias f\u02c6(",
      "chunk_id": 183,
      "start_pos": 175081,
      "end_pos": 176281,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "(16.4) (cid:104) (cid:105) =E[(cid:101)2]+E (f(x)\u2212 f\u02c6(x))2 (16.5) (cid:16) (cid:17)2 (cid:16) (cid:17) = \u03c32+ E[f(x)\u2212 f\u02c6(x)] +Var f(x)\u2212 f\u02c6(x) (16.6) (cid:16) (cid:17)2 (cid:16) (cid:17) = \u03c32+ Bias f\u02c6(x) +Var f\u02c6(x) (16.7) Thereisnothingwecandoaboutthefirstterm\u03c32 aswecannotpredictthe noise(cid:101)bydefinition.Thebiastermisduetounderfitting,meaningthatonaverage, f\u02c6doesnotpredict f.Thelasttermiscloselyrelatedtooverfitting,theprediction f\u02c6 istooclosefromthevaluesytrainandvariesalotwiththechoiceofourtraining set. Tosumup,wecanunderstandourMSEasfollows: HighBias \u2190\u2192 Underfitting HighVariance \u2190\u2192 Overfitting Large\u03c32 \u2190\u2192 Noisydata Hence,whenanalyzingtheperformanceofamachinelearningalgorithm,we mustalwaysaskourselveshowtoreducethebiaswithoutincreasingthevariance, andrespectivelyhowtoreducethevariancewithoutincreasingthebias.Most ofthetime,reducingonewillincreasetheother,andthereisatradeoffbetween biasandvariance. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 110 chapter 16.",
      "chunk_id": 184,
      "start_pos": 176081,
      "end_pos": 177083,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "houtincreasingthebias.Most ofthetime,reducingonewillincreasetheother,andthereisatradeoffbetween biasandvariance. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 110 chapter 16. bias-variance and error analysis 16.2 Erroranalysis Eventhoughunderstandingwhetherourpoortesterrorisduetohighbiasorhigh varianceisimportant,knowingwhichpartsofthemachinelearningalgorithm leadtothiserrororscoreiscrucial.Considerthemachinelearningpipelineon??. Thealgorithmsisdividedintoseveralsteps: 1. Theinputsaretakenfromacameraimage 2. Preprocessingtoremovethebackgroundontheimage.Forinstance,ifthe imagearetakenfromasecuritycamera,thebackgroundisalwaysthesame, andwecouldremoveiteasilybykeepingthepixelsthatchangedontheimage. 3. Detectthepositionoftheface. 4. Detecttheeyes-Detectthenose-Detectthemouth 5. Finallogisticregressionsteptopredictthelabel Ifyoubiuldacomplicatedsystemlikethisone,youmightwanttofigureout howmucherrorisattributabletoeachofthecomponents,howgoodiseachof thesegreenboxes.Indeed,ifoneoftheseboxesisreallyproblematic,youmight wanttospendmoretimetryingtoimprovetheperformanceofthatonegreen box.Howdoyoudecidewhatparttofocuson?",
      "chunk_id": 185,
      "start_pos": 176883,
      "end_pos": 178035,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ofthecomponents,howgoodiseachof thesegreenboxes.Indeed,ifoneoftheseboxesisreallyproblematic,youmight wanttospendmoretimetryingtoimprovetheperformanceofthatonegreen box.Howdoyoudecidewhatparttofocuson? One thing we can do is plug in the ground-truth for each component, and seehowaccuracychanges.Let\u2019ssaytheoverallaccuracyofthesystemis85% (prettybad).Youcannowtakeyourdevelopmentsetandmanuallygiveitthe perfectbackgroundremoval,thatis,insteadofusingyourbackgroundremoval algorithm, manually specify the perfect background removal yourself (using photoshopforinstance),andlookathowmuchthataffecttheperformanceof theoverallsystem. Nowlet\u2019ssaytheaccuracyonlyimprovesby0.1%.Thisgivesusanupperbound, thatisevenifweworkedforyearsonbackgroundremoval,itwouldn\u2019thelpour systembymorethan0.1%. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 16.3. ablative analysis 111 Component Accuracy Table 16.1. Accuracy when pro- vidingthesystemwiththeperfect Overallsystem 85% component. Preprocess(removebackground) 85.1% Facedetection 91% Eyessegmentation 95% Nosesegmentation 96% Mouthsegmentation 97% Logisticregression 100% Nowlet\u2019sgivethepipelinetheperfectfacedetectionbyspecifyingthepositio",
      "chunk_id": 186,
      "start_pos": 177835,
      "end_pos": 179035,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "s(removebackground) 85.1% Facedetection 91% Eyessegmentation 95% Nosesegmentation 96% Mouthsegmentation 97% Logisticregression 100% Nowlet\u2019sgivethepipelinetheperfectfacedetectionbyspecifyingtheposition ofthefacemanually,seehowmuchweimprovetheperformance,andsoon.The resultsarespecifiedinthetable16.1. Lookingatthetable,weknowthatworkingonthebackgroundremovalwon\u2019t helpmuch.Italsotellsuswherethebiggestjumpsare.Wenoticethathavingan accuratefacedetectionmechanismreallyimprovestheperformance,andsimilarly, theeyesreallyhelpmakingthepredictionmoreaccurate. Erroranalysisisalsousefulwhenpublishingapaper,sinceit\u2019saconvenient way to analyze the error of an algorithm and explain which parts should be improved. 16.3 Ablativeanalysis Whileerroranalysistriestoexplainthedifferencebetweencurrentperformance andperfectperformance,ablativeanalysistriestoexplainthedifferencebetween somebaseline(muchpoorer)performanceandcurrentperformance. Forinstance,supposeyouhavebuiltagoodanti-spamclassifierbyaddinglots ofcleverfeaturestologisticregression \u2022 Spellingcorrection \u2022 Senderhostfeatures \u2022 Emailheaderfeatures \u2022 Emailtextparserfeatures \u2022 Javascriptparser toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@",
      "chunk_id": 187,
      "start_pos": 178835,
      "end_pos": 180035,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "cleverfeaturestologisticregression \u2022 Spellingcorrection \u2022 Senderhostfeatures \u2022 Emailheaderfeatures \u2022 Emailtextparserfeatures \u2022 Javascriptparser toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 112 chapter 16. bias-variance and error analysis \u2022 Featuresfromembeddedimages andyourquestionis:Howmuchdideachofthesecomponentsreallyhelp? In this example, let\u2019s say that simple logistic regression without any clever featuresgets94%performance,butwhenaddingthesecleverfeatures,weget 99.9%performance.Inabaltiveanalysis,whatwedoisstartfromthecurrentlevel ofperformance99.9%,andslowlytakeawayallofthesefeaturestoseehowit affectsperformance.Theresultsareprovidedintable16.2. Component Accuracy Table16.2. Accuracywhenremov- ing feature from logistic regres- Overallsystem 99.9% sion. Spellingcorrection 99.0% Senderhostfeatures 98.9% Emailheaderfeatures 98.9% Emailtextparserfeatures 95% Javascriptparser 94.5% Featuresfromimages 94.0% Whenpresentingtheresultsinapaper,ablativeanalysisreallyhelpsanalyzing thefeaturesthathelpeddecreasingthemisclassificationrate.Insteadofsimply givingtheloss/errorrateofthealgorithm,wecanprovideevidencethatsome specificfeaturesareactuallymoreimportantth",
      "chunk_id": 188,
      "start_pos": 179835,
      "end_pos": 181035,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "allyhelpsanalyzing thefeaturesthathelpeddecreasingthemisclassificationrate.Insteadofsimply givingtheloss/errorrateofthealgorithm,wecanprovideevidencethatsome specificfeaturesareactuallymoreimportantthanothers. 16.3.1 Analyzeyourmistakes Assume you are given a dataset with pictures of animals, and your goal is to identify pictures of cats that you would eventually send to the members of a communityofcatlovers.Younoticethattherearemanypicturesofdogsinthe originaldataset,andwonderswhetheryoushouldbuildaspecialalgorithmto identifythepicturesofdogsandavoidsendingdogspicturestocatloversornot. Onethingyoucandoistakea100examplesfromyourdevelopmentsetthat aremisclassified,andcountuphowmanyofthese100mistakesaredogs.If5% ofthemaredogs,thenevenifyoucomeupwithasolutiontoidentidyyourdogs, yourerrorwouldonlygodownby5%,thatisyouraccuracywouldgoupfrom 90%to90.5%.However,if50ofthese100errorsaredogs,thenyoucouldimprove youraccuracytoreach95%. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 16.3.",
      "chunk_id": 189,
      "start_pos": 180835,
      "end_pos": 181850,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "thatisyouraccuracywouldgoupfrom 90%to90.5%.However,if50ofthese100errorsaredogs,thenyoucouldimprove youraccuracytoreach95%. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 16.3. ablative analysis 113 Byanalyzingyourmistakes,youcanfocusonwhat\u2019sreallyimportant.Ifyou noticethat80outofyour100mistakesareblurryimages,thenworkhardon classifyingcorrectlytheseblurryimages.Ifyounoticethat70outofthe100errors aregreatcats,thenfocusonthisspecifictaskofidentifyinggreatcats. Inbrief,donotwasteyourtimeimprovingpartsofyouralgorithmthatwon\u2019t reallyhelpdecreasingyourerrorrate,andfocusonwhatreallymatters. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu Part VII: Unsupervised Learning FromCS229Spring2021,Andrew Ng,MosesCharikar,Christopher R\u00e9&TengyuMa,StanfordUniver- 17 The k-means Clustering Algorithm sity. Intheclusteringproblem,wearegivenatrainingset{x(1),...,x(n)},andwant togroupthedataintoafewcohesive\u2018\u2018clusters.\u2019\u2019Here,x(i) \u2208Rd asusual;butno labelsy(i) aregiven.So,thisisanunsupervisedlearningproblem.Thek-means clusteringalgorithmisasfollows: 1. Initializeclustercentroids\u00b5 ,\u00b5 ,...,\u00b5 \u2208Rd randomly. 1 2 k 2.",
      "chunk_id": 190,
      "start_pos": 181650,
      "end_pos": 182801,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ters.\u2019\u2019Here,x(i) \u2208Rd asusual;butno labelsy(i) aregiven.So,thisisanunsupervisedlearningproblem.Thek-means clusteringalgorithmisasfollows: 1. Initializeclustercentroids\u00b5 ,\u00b5 ,...,\u00b5 \u2208Rd randomly. 1 2 k 2. Repeatuntilconvergence: \u2022 Foreveryi,set: c (i) :=argmin(cid:107)x (i)\u2212\u00b5 (cid:107)2 j j \u2022 Foreachj,set: \u2211n 1{c(i) = j}x(i) \u00b5 := i=1 j \u2211n 1{c(i) = j} i=1 In the algorithm above, k (a parameter of the algorithm) is the number of clusterswewanttofind;andtheclustercentroids\u00b5 representourcurrentguesses j forthepositionsofthecentersoftheclusters.Toinitializetheclustercentroids (instep1ofthealgorithmabove),wecouldchoosektrainingexamplesrandomly, andsettheclustercentroidstobeequaltothevaluesofthesekexamples.(Other initializationmethodsarealsopossible.) Theinner-loopofthealgorithmrepeatedlycarriesouttwosteps:(i)\u2018\u2018Assign- ing\u2019\u2019eachtrainingexamplex(i) totheclosestclustercentroid\u00b5 ,and(ii)Moving j eachclustercentroid\u00b5 tothemeanofthepointsassignedtoit.Figure1shows j anillustrationofrunningk-means. Isthek-meansalgorithmguaranteedtoconverge?Yesitis,inacertainsense.",
      "chunk_id": 191,
      "start_pos": 182601,
      "end_pos": 183663,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "lustercentroid\u00b5 ,and(ii)Moving j eachclustercentroid\u00b5 tothemeanofthepointsassignedtoit.Figure1shows j anillustrationofrunningk-means. Isthek-meansalgorithmguaranteedtoconverge?Yesitis,inacertainsense. Inparticular,letusdefinethedistortionfunctiontobe: n J(c,\u00b5) = \u2211 (cid:107)x (i)\u2212\u00b5 (cid:107)2 c(i) i=1 Thus, Jmeasuresthesumofsquareddistancesbetweeneachtrainingexample x(i) andtheclustercentroid\u00b5 towhichithasbeenassigned.Itcanbeshown c(i) that k-meansisexactlycoordinatedescenton J.Specifically,theinner-loopof k-meansrepeatedlyminimizes Jwithrespecttocwhileholding\u00b5fixed,andthen minimizes Jwithrespectto\u00b5whileholdingcfixed.Thus, Jmustmonotonically decrease,andthevalueof J mustconverge.(Usually,thisimpliesthatcand\u00b5 willconvergetoo.Intheory,itispossiblefork-meanstooscillatebetweenafew differentclusterings\u2014i.e.,afewdifferentvaluesforcand/or\u00b5\u2014thathaveexactly thesamevalueof J,butthisalmostneverhappensinpractice.) ThedistortionfunctionJisanon-convexfunction,andsocoordinatedescenton Jisnotguaranteedtoconvergetotheglobalminimum.Inotherwords,k-means canbesusceptibletolocaloptima.Veryoftenk-meanswillworkfineandcome upwithverygoodclusteringsdespitethis.Butifyouareworriedaboutgetting stuckinbadlocalm",
      "chunk_id": 192,
      "start_pos": 183463,
      "end_pos": 184663,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "toconvergetotheglobalminimum.Inotherwords,k-means canbesusceptibletolocaloptima.Veryoftenk-meanswillworkfineandcome upwithverygoodclusteringsdespitethis.Butifyouareworriedaboutgetting stuckinbadlocalminima,onecommonthingtodoisrunk-meansmanytimes (usingdifferentrandominitialvaluesfortheclustercentroids\u00b5 ).Then,outof j allthedifferentclusteringsfound,picktheonethatgivesthelowestdistortion J(c,\u00b5). 18 Mixtures of Gaussians and the EM Algorithm Inthischapter,wediscusstheEM(Expectation-Maximization)algorithmfor densityestimation. Supposethatwearegivenatrainingset{x(1),...,x(n)}asusual.Sinceweare intheunsupervisedlearningsetting,thesepointsdonotcomewithanylabels. Wewishtomodelthedatabyspecifyingajointdistributionp(x(i),z(i)) = p(x(i) | z(i))p(z(i)). Here, z(i) \u223c Multinomial(\u03c6) (where \u03c6 \u2265 0,\u2211k \u03c6 = 1, and the j j=1 j parameter\u03c6 gives p(z(i) = j)),andx(i) | z(i) = j \u223c N(\u00b5 ,\u03a3 ).Weletkdenote j j j thenumberofvaluesthatthez(i)\u2019scantakeon.Thus,ourmodelpositsthateach 116 chapter 18. mixtures of gaussians and the em algorithm x(i) wasgeneratedbyrandomlychoosingz(i) from{1,...,k},andthenx(i) was drawnfromoneofkGaussiansdependingonz(i).Thisiscalledthemixtureof Gaussiansmodel.Also,notethatthez(i)\u2019sar",
      "chunk_id": 193,
      "start_pos": 184463,
      "end_pos": 185663,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ians and the em algorithm x(i) wasgeneratedbyrandomlychoosingz(i) from{1,...,k},andthenx(i) was drawnfromoneofkGaussiansdependingonz(i).Thisiscalledthemixtureof Gaussiansmodel.Also,notethatthez(i)\u2019sarelatentrandomvariables,meaning thatthey\u2019rehidden/unobserved.Thisiswhatwillmakeourestimationproblem difficult. Theparametersofourmodelarethus\u03c6,\u00b5and\u03a3.Toestimatethem,wecan writedownthelikelihoodofourdata: n (cid:96)(\u03c6,\u00b5,\u03a3) = \u2211 logp(x (i) ;\u03c6,\u00b5,\u03a3) i=1 n k = \u2211 log \u2211 p(x (i) | z (i) ;\u00b5,\u03a3)p(z (i) ;\u03c6) i=1 z(i)=1 However, if we set to zero the derivatives of this formula with respect to the parametersandtrytosolve,we\u2019llfindthatitisnotpossibletofindthemaximum likelihoodestimatesoftheparametersinclosedform.(Trythisyourselfathome.) Therandomvariablesz(i)indicatewhichofthekGaussianseachx(i)hadcome from.Notethatifweknewwhatthez(i)\u2019swere,themaximumlikelihoodproblem wouldhavebeeneasy.Specifically,wecouldthenwritedownthelikelihoodas: n (cid:96)(\u03c6,\u00b5,\u03a3) = \u2211 logp(x (i) | z (i) ;\u00b5,\u03a3)+logp(z (i) ;\u03c6) i=1 Maximizingthiswithrespectto\u03c6,\u00b5and\u03a3givestheparameters: \u03c6 = 1 \u2211 n 1{z (i) = j} j n i=1 \u2211n 1{z(i) = j}x(i) \u00b5 = i=1 j \u2211n 1{z(i) = j} i=1 \u2211n 1{z(i) = j}(x(i)\u2212\u00b5 )(x(i)\u2212\u00b5 )(cid:62) \u03a3 = i=1 j j j \u2211n 1{z(i) = j} i=1 I",
      "chunk_id": 194,
      "start_pos": 185463,
      "end_pos": 186663,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ngthiswithrespectto\u03c6,\u00b5and\u03a3givestheparameters: \u03c6 = 1 \u2211 n 1{z (i) = j} j n i=1 \u2211n 1{z(i) = j}x(i) \u00b5 = i=1 j \u2211n 1{z(i) = j} i=1 \u2211n 1{z(i) = j}(x(i)\u2212\u00b5 )(x(i)\u2212\u00b5 )(cid:62) \u03a3 = i=1 j j j \u2211n 1{z(i) = j} i=1 Indeed,weseethatifthez(i) \u2019swereknown,thenmaximumlikelihoodesti- mationbecomesnearlyidenticaltowhatwehadwhenestimatingtheparameters oftheGaussiandiscriminantanalysismodel,exceptthatherethez(i)\u2019splaying theroleoftheclasslabels.1 1Thereareotherminordifferences However,inourdensityestimationproblem,thez(i) \u2019sarenotknown.What in the formulas here from what we\u2019dobtainedinPS1withGaus- canwedo?TheEMalgorithmisaniterativealgorithmthathastwomainsteps. siandiscriminantanalysis,firstbe- causewe\u2019vegeneralizedthez(i)\u2019s to be multinomial rather than 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc Bernoulli,andsecondbecausehere weareusingadifferent\u03a3 jforeach Gaussian. 117 Appliedtoourproblem,intheE-step,ittriesto\u2018\u2018guess\u2019\u2019thevaluesofthez(i) \u2019s. IntheM-step,itupdatestheparametersofourmodelbasedonourguesses.Since intheM-stepwearepretendingthattheguessesinthefirstpartwerecorrect,the maximizationbecomeseasy.Here\u2019sthealgorithm: \u2022 Repeatuntilconvergence: \u2013 (E-step)Foreachi,j,set: w (i",
      "chunk_id": 195,
      "start_pos": 186463,
      "end_pos": 187663,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "rmodelbasedonourguesses.Since intheM-stepwearepretendingthattheguessesinthefirstpartwerecorrect,the maximizationbecomeseasy.Here\u2019sthealgorithm: \u2022 Repeatuntilconvergence: \u2013 (E-step)Foreachi,j,set: w (i) := p(z (i) = j | x (i) ;\u03c6,\u00b5,\u03a3) j \u2013 (M-step)Updatetheparameters: \u03c6 = 1 \u2211 n w (i) j n j i=1 \u2211n w (i) x(i) i=1 j \u00b5 = j \u2211n w (i) i=1 j \u2211n w (i) (x(i)\u2212\u00b5 )(x(i)\u2212\u00b5 )(cid:62) \u03a3 = i=1 j j j j \u2211n w (i) i=1 j In the E-step, we calculate the posterior probability of our parameters, the z(i)\u2019s,giventhex(i) andusingthecurrentsettingofourparameters.I.e.,using Bayesrule,weobtain: p(x(i) | z(i) = j;\u00b5,\u03a3)p(z(i) = j;\u03c6) p(z (i) = j | x (i) ;\u03c6,\u00b5,\u03a3) = \u2211k p(x(i) | z(i) = l;\u00b5,\u03a3)p(z(i) = l;\u03c6) l=1 Here, p(x(i) | z(i) = j;\u00b5,\u03a3)isgivenbyevaluatingthedensityofaGaussianwith mean\u00b5 andcovariance\u03a3 at x(i) ; p(z(i) = j;\u03c6)isgivenby\u03c6,andsoon.The j j j valuesw (i) calculatedintheE-steprepresentour\u2018\u2018soft\u2019\u2019guesses2forthevalues 2The term \u2018\u2018soft\u2019\u2019 refers to our j ofz(i). guessesbeingprobabilitiesandtak- ingvaluesin [0,1];incontrast,a Also,youshouldcontrasttheupdatesintheM-stepwiththeformulaswehad \u2018\u2018hard\u2019\u2019guessisonethatrepresents whenthez(i)\u2019swereknownexactly.Theyareidentical,exceptthatinsteadofthe asinglebestguess(suchastakin",
      "chunk_id": 196,
      "start_pos": 187463,
      "end_pos": 188663,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "st,a Also,youshouldcontrasttheupdatesintheM-stepwiththeformulaswehad \u2018\u2018hard\u2019\u2019guessisonethatrepresents whenthez(i)\u2019swereknownexactly.Theyareidentical,exceptthatinsteadofthe asinglebestguess(suchastaking valuesin{0,1}or{1,...,k}). indicatorfunctions\u2018\u20181{z(i) = j}\u2019\u2019indicatingfromwhichGaussianeachdatapoint (i) hadcome,wenowinsteadhavethew \u2019s. j TheEM-algorithmisalsoreminiscentoftheK-meansclusteringalgorithm, exceptthatinsteadofthe\u2018\u2018hard\u2019\u2019clusterassignmentsc(i),weinsteadhavethe (i) \u2018\u2018soft\u2019\u2019assignmentsw .SimilartoK-means,itisalsosusceptibletolocaloptima, j soreinitializingatseveraldifferentinitialparametersmaybeagoodidea. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 118 chapter 18. mixtures of gaussians and the em algorithm It\u2019sclearthattheEMalgorithmhasaverynaturalinterpretationofrepeatedly tryingtoguesstheunknownz(i)\u2019s;buthowdiditcomeabout,andcanwemake any guarantees about it, such as regarding its convergence? In the next set of notes,wewilldescribeamoregeneralviewofEM,onethatwillallowustoeasily applyittootherestimationproblemsinwhichtherearealsolatentvariables,and whichwillallowustogiveaconvergenceguarantee.",
      "chunk_id": 197,
      "start_pos": 188463,
      "end_pos": 189612,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "the next set of notes,wewilldescribeamoregeneralviewofEM,onethatwillallowustoeasily applyittootherestimationproblemsinwhichtherearealsolatentvariables,and whichwillallowustogiveaconvergenceguarantee. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc Part VIII: The EM Algorithm FromCS229Spring2021,Andrew In the previous set of notes, we talked about the EM algorithm as applied to Ng,MosesCharikar,Christopher fittingamixtureofGaussians.Inthissetofnotes,wegiveabroaderviewofthe R\u00e9&TengyuMa,StanfordUniver- sity. EMalgorithm,andshowhowitcanbeappliedtoalargefamilyofestimation problemswithlatentvariables.Webeginourdiscussionwithaveryusefulresult calledJensen\u2019sinequality. 19 Jensen\u2019s inequality Let f be a function whose domain is the set of real numbers. Recall that f is a convex function if f(cid:48)(cid:48)(x) \u2265 0 (for all x \u2208 R). In the case of f taking vector- valuedinputs,thisisgeneralizedtotheconditionthatitshessian Hispositive semi-definite(H \u22650).If f(cid:48)(cid:48)(x) >0forallx,thenwesay f isstrictlyconvex(in thevector-valuedcase,thecorrespondingstatementisthatHmustbepositive definite,writtenH >0).Jensen\u2019sinequalitycanthenbestatedasfollows: Theorem.",
      "chunk_id": 198,
      "start_pos": 189412,
      "end_pos": 190602,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ":48)(cid:48)(x) >0forallx,thenwesay f isstrictlyconvex(in thevector-valuedcase,thecorrespondingstatementisthatHmustbepositive definite,writtenH >0).Jensen\u2019sinequalitycanthenbestatedasfollows: Theorem. Let f beaconvexfunction,andletXbearandomvariable.Then: E[f(X)] \u2265 f(E[X]). (19.1) Moreover,if f isstrictlyconvex,thenE[f(X)] = f(E[X])holdstrueifandonlyif X =E[X]withprobability1(i.e.,ifXisaconstant). Recallourconventionofoccasionallydroppingtheparentheseswhenwriting expectations,sointhetheoremabove, f(EX) = f(E[X]). Foraninterpretationofthetheorem,considerthefigurebelow. Here, f is a convex function shown by the solid line. Also, X is a random variablethathasa0.5chanceoftakingthevaluea,anda0.5chanceoftakingthe valueb(indicatedonthex-axis).Thus,theexpectedvalueofXisgivenbythe midpointbetweenaandb. Wealsoseethevalues f(a), f(b)and f(E[X])indicatedonthey-axis.Moreover, the value E[f(X)] is now the midpoint on the y-axis between f(a) and f(b). From our example, we see that because f is convex, it must be the case that E[f(X)] \u2265 f(EX). Incidentally,quitealotofpeoplehavetroublerememberingwhichwaythe inequalitygoes,andrememberingapicturelikethisisagoodwaytoquickly figureouttheanswer.",
      "chunk_id": 199,
      "start_pos": 190402,
      "end_pos": 191594,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "s convex, it must be the case that E[f(X)] \u2265 f(EX). Incidentally,quitealotofpeoplehavetroublerememberingwhichwaythe inequalitygoes,andrememberingapicturelikethisisagoodwaytoquickly figureouttheanswer. Remark. Recallthat f is[strictly]concaveifandonlyif\u2212f is[strictly]convex (i.e., f(cid:48)(cid:48)(x) \u22640or H \u22640).Jensen\u2019sinequalityalsoholdsforconcavefunctions f, butwiththedirectionofalltheinequalitiesreversed(E[f(X)] \u2264 f(EX),etc.). 20 The EM algorithm Suppose we have an estimation problem in which we have a training set {x(1),...,x(n)}consistingofnindependentexamples.Wehavealatentvariable model p(x,z;\u03b8)withzbeingthelatentvariable(whichforsimplicityisassumed totakefinitenumberofvalues).Thedensityforxcanbeobtainedbymarginalized overthelatentvariablez: \u2211 p(x;\u03b8) = p(x,z;\u03b8) (20.1) z Wewishtofittheparameters\u03b8bymaximizingthelog-likelihoodofthedata, definedby: n (cid:96)(\u03b8) = \u2211 logp(x (i) ;\u03b8) (20.2) i=1 Wecanrewritetheobjectiveintermsofthejointdensity p(x,z;\u03b8)by: n (cid:96)(\u03b8) = \u2211 logp(x (i) ;\u03b8) (20.3) i=1 n = \u2211 log \u2211 p(x (i) ,z (i) ;\u03b8) (20.4) i=1 z(i) 121 Butexplicitlyfindingthemaximumlikelihoodestimatesoftheparameters\u03b8may behardsinceitwillresultindifficultnon-convexoptimizationproblems.1 H",
      "chunk_id": 200,
      "start_pos": 191394,
      "end_pos": 192594,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "(20.3) i=1 n = \u2211 log \u2211 p(x (i) ,z (i) ;\u03b8) (20.4) i=1 z(i) 121 Butexplicitlyfindingthemaximumlikelihoodestimatesoftheparameters\u03b8may behardsinceitwillresultindifficultnon-convexoptimizationproblems.1 Here, 1It\u2019smostlyanempiricalobserva- thez(i)\u2019sarethelatentrandomvariables;anditisoftenthecasethatifthez(i)\u2019s tionthattheoptimizationproblem isdifficulttooptimize. wereobserved,thenmaximumlikelihoodestimationwouldbeeasy. Insuchasetting,theEMalgorithmgivesanefficientmethodformaximum likelihood estimation. Maximizing (cid:96)(\u03b8) explicitly might be difficult, and our strategywillbetoinsteadrepeatedlyconstructalower-boundon(cid:96)(E-step),and thenoptimizethatlower-bound(M-step).2 2Empirically, the E-step and M- It turns out that the summation \u2211n is not essential here, and towards a stepcanoftenbecomputedmore i=1 efficiently than optimizing the simplerexpositionoftheEMalgorithm,wewillfirstconsideroptimizingthe function(cid:96)(\u00b7)directly.However,it thelikelihoodlogp(x)forasingleexamplex.Afterwederivethealgorithmfor doesn\u2019t necessarily mean that al- ternatingthetwostepscanalways optimizinglogp(x),wewillconvertittoanalgorithmthatworksfornexamples converge to the global optimum byaddingbackthe",
      "chunk_id": 201,
      "start_pos": 192394,
      "end_pos": 193594,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "derivethealgorithmfor doesn\u2019t necessarily mean that al- ternatingthetwostepscanalways optimizinglogp(x),wewillconvertittoanalgorithmthatworksfornexamples converge to the global optimum byaddingbackthesumtoeachoftherelevantequations.Thus,nowweaimto of(cid:96)(\u00b7).EvenformixtureofGaus- optimizelogp(x;\u03b8)whichcanberewrittenas: sians,theEMalgorithmcaneither convergetoaglobaloptimumor \u2211 getstuck,dependingontheprop- logp(x;\u03b8) =log p(x,z;\u03b8) (20.5) ertiesofthetrainingdata.Empiri- z cally,forreal-worlddata,oftenEM LetQbeadistributionoverthepossiblevaluesofz.Thatis,\u2211 Q(z) =1,Q(z) \u2265 canconvergetoasolutionwithrel- z ativelyhighlikelihood(ifnotthe 0. optimum),andthetheorybehind Considerthefollowing:3 itisstilllargelynotunderstood. 3If z were continuous, then Q \u2211 wouldbeadensity,andthesum- logp(x;\u03b8) =log p(x,z;\u03b8) (20.6) mationsoverzinourdiscussion z arereplacedwithintegralsoverz. \u2211 p(x,z;\u03b8) =log Q(z) (20.7) Q(z) z \u2211 p(x,z;\u03b8) \u2265 Q(z)log (20.8) Q(z) z ThelaststepofthisderivationusedJensen\u2019sinequality.Specifically, f(x) = logx isaconcavefunction,since f(cid:48)(cid:48)(x) = \u22121/x2 < 0overitsdomain x \u2208 R+.",
      "chunk_id": 202,
      "start_pos": 193394,
      "end_pos": 194495,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ".7) Q(z) z \u2211 p(x,z;\u03b8) \u2265 Q(z)log (20.8) Q(z) z ThelaststepofthisderivationusedJensen\u2019sinequality.Specifically, f(x) = logx isaconcavefunction,since f(cid:48)(cid:48)(x) = \u22121/x2 < 0overitsdomain x \u2208 R+. Also,theterm \u2211 (cid:20) p(x,z;\u03b8)(cid:21) Q(z) Q(z) z in the summation is just an expectation of the quantity [p(x,z;\u03b8)/Q(z)] with respecttozdrawnaccordingtothedistributiongivenbyQ.4ByJensen\u2019sinequality, 4We note that the notion p(x,z;\u03b8) Q(z) wehave only makes sense if Q(z) (cid:54)= 0 (cid:18) (cid:20) p(x,z;\u03b8)(cid:21)(cid:19) (cid:20) (cid:18) p(x,z;\u03b8)(cid:19)(cid:21) wheneverp(x,z;\u03b8) (cid:54)= 0.Herewe f E z\u223cQ Q(z) \u2265E z\u223cQ f Q(z) , implicitlyassumethatweonlycon- siderthoseQwithsuchaproperty. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 122 chapter 20. the em algorithm wherethe\u2018\u2018z \u223c Q\u2019\u2019subscriptsaboveindicatethattheexpectationsarewithrespect tozdrawnfromQ.Thisallowedustogofromequation(20.7)toequation(20.8). Now,foranydistributionQ,theformula20.8givesalower-boundonlogp(x;\u03b8).",
      "chunk_id": 203,
      "start_pos": 194295,
      "end_pos": 195307,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\u2018\u2018z \u223c Q\u2019\u2019subscriptsaboveindicatethattheexpectationsarewithrespect tozdrawnfromQ.Thisallowedustogofromequation(20.7)toequation(20.8). Now,foranydistributionQ,theformula20.8givesalower-boundonlogp(x;\u03b8). TherearemanypossiblechoicesfortheQ\u2019s.Whichshouldwechoose?Well,ifwe havesomecurrentguess\u03b8oftheparameters,itseemsnaturaltotrytomakethe lower-boundtightatthatvalueof\u03b8.I.e.,wewillmaketheinequalityabovehold withequalityatourparticularvalueof\u03b8.Tomaketheboundtightforaparticular valueof\u03b8,weneedforthestepinvolvingJensen\u2019sinequalityinourderivation abovetoholdwithequality.Forthistobetrue,weknowitissufficientthatthe expectationbetakenovera\u2018\u2018constant\u2019\u2019-valuedrandomvariable.I.e.,werequire that p(x,z;\u03b8) = c Q(z) forsomeconstantcthatdoesnotdependonz.Thisiseasilyaccomplishedby choosing Q(z) \u221d p(x,z;\u03b8). Actually,sinceweknow\u2211 Q(z) = 1(becauseitisadistribution),thisfurther z tellsusthat p(x,z;\u03b8) Q(z) = (20.9) \u2211 p(x,z;\u03b8) z p(x,z;\u03b8) = (20.10) p(x;\u03b8) = p(z | x;\u03b8) (20.11) Thus,wesimplysettheQ\u2019stobetheposteriordistributionofthez\u2019sgivenxand thesettingoftheparameters\u03b8.",
      "chunk_id": 204,
      "start_pos": 195107,
      "end_pos": 196162,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "further z tellsusthat p(x,z;\u03b8) Q(z) = (20.9) \u2211 p(x,z;\u03b8) z p(x,z;\u03b8) = (20.10) p(x;\u03b8) = p(z | x;\u03b8) (20.11) Thus,wesimplysettheQ\u2019stobetheposteriordistributionofthez\u2019sgivenxand thesettingoftheparameters\u03b8. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 123 Indeed, we can directly verify that when Q(z) = p(z | x;\u03b8), then equa- tion(20.8)isanequalitybecause: \u2211 p(x,z;\u03b8) \u2211 p(x,z;\u03b8) Q(z)log = p(z | x;\u03b8)log Q(z) p(z | x;\u03b8) z z \u2211 p(z | x;\u03b8)p(x;\u03b8) = p(z | x;\u03b8)log p(z | x;\u03b8) z \u2211 = p(z | x;\u03b8)logp(x;\u03b8) z \u2211 =logp(x;\u03b8) p(z | x;\u03b8) z =logp(x;\u03b8) (because\u2211 p(z | x;\u03b8) =1) z Forconvenience,wecalltheexpressioninequation(20.8)theevidencelower bound(ELBO)andwedenoteitby: \u2211 p(x,z;\u03b8) ELBO(x;Q,\u03b8) = Q(z)log (20.12) Q(z) z Withthisequation,wecanre-writeequation(20.8)as: \u2200Q,\u03b8,x, logp(x;\u03b8) \u2265 ELBO(x;Q,\u03b8) (20.13) Intuitively,theEMalgorithmalternativelyupdatesQand\u03b8bya)settingQ(z) = p(z | x;\u03b8) followingequation(20.11)sothatELBO(x;Q,\u03b8) = logp(x;\u03b8) for x andthecurrent\u03b8,andb)maximizingELBO(x;Q,\u03b8)w.r.t\u03b8whilefixingthechoice ofQ.",
      "chunk_id": 205,
      "start_pos": 195962,
      "end_pos": 196988,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "theEMalgorithmalternativelyupdatesQand\u03b8bya)settingQ(z) = p(z | x;\u03b8) followingequation(20.11)sothatELBO(x;Q,\u03b8) = logp(x;\u03b8) for x andthecurrent\u03b8,andb)maximizingELBO(x;Q,\u03b8)w.r.t\u03b8whilefixingthechoice ofQ. Recallthatallthediscussionabovewasundertheassumptionthatweaimto optimizethelog-likelihoodlogp(x;\u03b8)forasingleexample x.Itturnsoutthat withmultipletrainingexamples,thebasicideaisthesameandweonlyneedto takeasumoverexamplesatrelevantplaces.Next,wewillbuildtheevidence lowerboundformultipletrainingexamplesandmaketheEMalgorithmformal. Recallwehaveatrainingset{x(1),...,x(n)}.NotethattheoptimalchoiceofQ is p(z | x;\u03b8),anditdependsontheparticularexamplex.Thereforeherewewill introducendistributionsQ ,...,Q ,oneforeachexamplex(i).Foreachexample 1 n x(i),wecanbuildtheevidencelowerbound: logp(x (i) ;\u03b8) \u2265ELBO(x (i) ;Q,\u03b8) = \u2211 Q (z (i))log p(x(i),z(i);\u03b8) i i Q (z(i)) z(i) i toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 124 chapter 20. the em algorithm Takingsumoveralltheexamples,weobtainalowerboundforthelog-likelihood: (cid:96)(\u03b8) \u2265 \u2211 ELBO(x (i) ;Q,\u03b8) (20.14) i i = \u2211\u2211 Q (z (i))log p(x(i),z(i);\u03b8) (20.15) i Q (z(i)) i z(i) i ForanysetofdistributionsQ ,...,Q ,theformula20.14gives",
      "chunk_id": 206,
      "start_pos": 196788,
      "end_pos": 197988,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "eobtainalowerboundforthelog-likelihood: (cid:96)(\u03b8) \u2265 \u2211 ELBO(x (i) ;Q,\u03b8) (20.14) i i = \u2211\u2211 Q (z (i))log p(x(i),z(i);\u03b8) (20.15) i Q (z(i)) i z(i) i ForanysetofdistributionsQ ,...,Q ,theformula20.14givesalower-bound 1 n on (cid:96)(\u03b8), and analogous to the argument around equation (20.11), the Q that i attainsequalitysatisfies: Q (z (i)) = p(z (i) | x (i) ;\u03b8) i Thus,wesimplysettheQ\u2019stobetheposteriordistributionofthez(i)\u2019sgivenx(i) i withthecurrentsettingoftheparameters\u03b8. Now,forthischoiceoftheQ\u2019s,equation(20.14)givesalower-boundonthe i log-likelihood(cid:96)thatwe\u2019retryingtomaximize.ThisistheE-step.IntheM-stepof thealgorithm,wethenmaximizeourformulainequation(20.14)withrespectto theparameterstoobtainanewsettingofthe\u03b8\u2019s.Repeatedlycarryingoutthese twostepsgivesustheEMalgorithm,whichisasfollows: \u2022 Repeatuntilconvergence: \u2013 (E-step)Foreachi,set: Q (z (i)) := p(z (i) | x (i) ;\u03b8) i \u2013 (M-step)Set: n \u03b8 :=argmax \u2211 ELBO(x (i) ;Q,\u03b8) (20.16) i \u03b8 i=1 =argmax \u2211\u2211 Q (z (i))log p(x(i),z(i);\u03b8) . (20.17) i Q (z(i)) \u03b8 i z(i) i Howdoweknowifthisalgorithmwillconverge?Well,suppose\u03b8(t)and\u03b8(t+1) are the parameters from two successive iterations of EM.",
      "chunk_id": 207,
      "start_pos": 197788,
      "end_pos": 198929,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "i \u03b8 i=1 =argmax \u2211\u2211 Q (z (i))log p(x(i),z(i);\u03b8) . (20.17) i Q (z(i)) \u03b8 i z(i) i Howdoweknowifthisalgorithmwillconverge?Well,suppose\u03b8(t)and\u03b8(t+1) are the parameters from two successive iterations of EM. We will now prove that(cid:96)(\u03b8(t)) \u2264 (cid:96)(\u03b8(t+1)),whichshowsEMalwaysmonotonicallyimprovesthelog- likelihood.ThekeytoshowingthisresultliesinourchoiceoftheQ\u2019s.Specifically, i ontheiterationofEMinwhichtheparametershadstartedoutas\u03b8(t),wewould 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 125 havechosenQ (t) (z(i)) := p(z(i) | x(i);\u03b8(t)).Wesawearlierthatthischoiceensures i thatJensen\u2019sinequality,asappliedtogetequation(20.14),holdswithequality, andhence: n (cid:96)(\u03b8 (t)) = \u2211 ELBO(x (i) ;Q (t) ,\u03b8 (t)) (20.18) i i=1 Theparameters\u03b8(t+1) arethenobtainedbymaximizingtherighthandsideof theequationabove.Thus, n (cid:96)(\u03b8 (t+1)) \u2265 \u2211 ELBO(x (i) ;Q (t) ,\u03b8 (t+1)) i i=1 (becauseinequality20.14holdsforallQand\u03b8) n \u2265 \u2211 ELBO(x (i) ;Q (t) ,\u03b8 (t)) (seereasonbelow) i i=1 = (cid:96)(\u03b8 (t)) (byequation(20.18)) wherethelastinequalityfollowsfromthat\u03b8(t+1) ischosenexplicitlytobe: n argmax \u2211 ELBO(x (i) ;Q (t) ,\u03b8) i \u03b8 i=1 Hence,EMcausesthelikelihoodtoconvergemonotonically.Inourdescri",
      "chunk_id": 208,
      "start_pos": 198729,
      "end_pos": 199929,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ")(\u03b8 (t)) (byequation(20.18)) wherethelastinequalityfollowsfromthat\u03b8(t+1) ischosenexplicitlytobe: n argmax \u2211 ELBO(x (i) ;Q (t) ,\u03b8) i \u03b8 i=1 Hence,EMcausesthelikelihoodtoconvergemonotonically.Inourdescription oftheEMalgorithm,wesaidwe\u2019drunituntilconvergence.Giventheresultthat wejustshowed,onereasonableconvergencetestwouldbetocheckiftheincrease in(cid:96)(\u03b8)betweensuccessiveiterationsissmallerthansometoleranceparameter, andtodeclareconvergenceifEMisimproving(cid:96)(\u03b8)tooslowly. Remark. Ifwedefine(byoverloadingELBO(\u00b7)) ELBO(Q,\u03b8) = \u2211 n ELBO(x (i) ;Q,\u03b8) = \u2211\u2211 Q (z (i))log p(x(i),z(i);\u03b8) (20.19) i i Q (z(i)) i=1 i z(i) i then we know (cid:96)(\u03b8) \u2265 ELBO(Q,\u03b8) from our previousderivation.The EM can alsobeviewedanalternatingmaximizationalgorithmonELBO(Q,\u03b8),inwhich theE-stepmaximizesitwithrespecttoQ(checkthisyourself),andtheM-step maximizesitwithrespectto\u03b8. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 20.1 OtherinterpretationofELBO LetELBO(x;Q,\u03b8) = \u2211 Q(z)log p(x,z;\u03b8) bedefinedasinequation(20.12).There z Q(z) areseveralotherformsofELBO.First,wecanrewrite ELBO(x;Q,\u03b8) =E z\u223cQ [logp(x,z;\u03b8)]\u2212E z\u223cQ [logQ(z)] (20.20) =E z\u223cQ [logp(x | z;\u03b8)]\u2212D KL (Q || p z ) (20.21) whereweuse",
      "chunk_id": 209,
      "start_pos": 199729,
      "end_pos": 200929,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "finedasinequation(20.12).There z Q(z) areseveralotherformsofELBO.First,wecanrewrite ELBO(x;Q,\u03b8) =E z\u223cQ [logp(x,z;\u03b8)]\u2212E z\u223cQ [logQ(z)] (20.20) =E z\u223cQ [logp(x | z;\u03b8)]\u2212D KL (Q || p z ) (20.21) whereweuse p todenotethemarginaldistributionofz(underthedistribution z p(x,z;\u03b8)),andD ()denotestheKLdivergence: KL \u2211 Q(z) D (Q || p ) = Q(z)log (20.22) KL z p(z) z Inmanycases,themarginaldistributionofzdoesnotdependontheparameter\u03b8. Inthiscase,wecanseethatmaximizingELBOover\u03b8isequivalenttomaximizing thefirsttermin20.21.Thiscorrespondstomaximizingtheconditionallikelihood ofxconditionedonz,whichisoftenasimplerquestionthantheoriginalquestion. AnotherformofELBO(\u00b7)is(pleaseverifyyourself): ELBO(x;Q,\u03b8) =logp(x)\u2212D (Q || p ) (20.23) KL z|x where p istheconditionaldistributionofzgivenxundertheparameter\u03b8.This z|x formsshowsthatthemaximizerofELBO(Q,\u03b8)overQisobtainedwhenQ = p , z|x whichwasshowninequation(20.11)before. 21 Mixture of Gaussians revisited ArmedwithourgeneraldefinitionoftheEMalgorithm,let\u2019sgobacktoour oldexampleoffittingtheparameters\u03c6,\u00b5and\u03a3inamixtureofGaussians.For thesakeofbrevity,wecarryoutthederivationsfortheM-stepupdatesonlyfor\u03c6 and\u00b5 ,andleavetheupdatesfor\u03a3 asanexerciseforthereader.",
      "chunk_id": 210,
      "start_pos": 200729,
      "end_pos": 201918,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "gobacktoour oldexampleoffittingtheparameters\u03c6,\u00b5and\u03a3inamixtureofGaussians.For thesakeofbrevity,wecarryoutthederivationsfortheM-stepupdatesonlyfor\u03c6 and\u00b5 ,andleavetheupdatesfor\u03a3 asanexerciseforthereader. j j TheE-stepiseasy.Followingouralgorithmderivationabove,wesimplycalcu- late: w (i) = Q (z (i) = j) = P(z (i) = j | x (i) ;\u03c6,\u00b5,\u03a3) j i 127 Here,\u2018\u2018Q (z(i) = j)\u2019\u2019denotestheprobabilityofz(i) takingthevaluejunderthe i distributionQ. i Next, in the M-step, we need to maximize, with respect to our parameters \u03c6,\u00b5,\u03a3,thequantity: \u2211 n \u2211 Q (z (i))log p(x(i),z(i);\u03c6,\u00b5,\u03a3) i Q (z(i)) i=1z(i) i = \u2211 n \u2211 k Q (z (i) = j)log p(x(i) | z(i) = j;\u00b5,\u03a3)p(z(i) = j;\u03c6) i Q (z(i) = j) i=1j=1 i (cid:16) (cid:17) = \u2211 n \u2211 k w (i) log (2\u03c0)d/ 1 2|\u03a3 j |1/2 exp \u22121 2 (x(i)\u2212\u00b5 j )(cid:62)\u03a3\u2212 j 1(x(i)\u2212\u00b5 j ) \u00b7\u03c6 j j (i) i=1j=1 w j Let\u2019smaximizethiswithrespectto\u00b5 .Ifwetakethederivativewithrespectto\u00b5 , l l wefind: (cid:16) (cid:17) \u2207 \u2211 n \u2211 k w (i) log (2\u03c0)d/ 1 2|\u03a3 j |1/2 exp \u22121 2 (x(i)\u2212\u00b5 j )(cid:62)\u03a3\u2212 j 1(x(i)\u2212\u00b5 j ) \u00b7\u03c6 j \u00b5l j (i) i=1j=1 w j = \u2212\u2207 \u2211 n \u2211 k w (i)1 (x (i)\u2212\u00b5 )(cid:62)\u03a3\u22121(x (i)\u2212\u00b5 ) \u00b5l j 2 j j j i=1j=1 = 1 \u2211 n w (i) \u2207 2\u00b5 (cid:62)\u03a3\u22121x (i)\u2212\u00b5 (cid:62)\u03a3\u22121\u00b5 2 l \u00b5l l l l l l i=1 n (cid:16) (cid:17) = \u2211 w (i) \u03a3\u22121x (i)\u2212\u03a3\u22121\u00b5 l l l",
      "chunk_id": 211,
      "start_pos": 201718,
      "end_pos": 202918,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "=1 w j = \u2212\u2207 \u2211 n \u2211 k w (i)1 (x (i)\u2212\u00b5 )(cid:62)\u03a3\u22121(x (i)\u2212\u00b5 ) \u00b5l j 2 j j j i=1j=1 = 1 \u2211 n w (i) \u2207 2\u00b5 (cid:62)\u03a3\u22121x (i)\u2212\u00b5 (cid:62)\u03a3\u22121\u00b5 2 l \u00b5l l l l l l i=1 n (cid:16) (cid:17) = \u2211 w (i) \u03a3\u22121x (i)\u2212\u03a3\u22121\u00b5 l l l l i=1 Settingthistozeroandsolvingfor\u00b5 thereforeyieldstheupdaterule l \u2211n w (i) x(i) \u00b5 := i=1 l , l \u2211n w (i) i=1 l whichwaswhatwehadintheprevioussetofnotes. Let\u2019sdoonemoreexample,andderivetheM-stepupdatefortheparameters \u03c6.Groupingtogetheronlythetermsthatdependon\u03c6,wefindthatweneedto j j maximize: n k \u2211 \u2211 (i) w log\u03c6 j j i=1j=1 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu However,thereisanadditionalconstraintthatthe\u03c6\u2019ssumto1,sincetheyrep- j resent the probabilities \u03c6 = p(z(i) = j;\u03c6). To deal with the constraint that j \u2211k \u03c6 =1,weconstructtheLagrangian j=1 j (cid:32) (cid:33) n k k L(\u03c6) = \u2211 \u2211 w (i) log\u03c6 +\u03b2 \u2211 \u03c6 \u22121 , j j j i=1j=1 j=1 where\u03b2istheLagrangemultiplier.1Takingderivatives,wefind: 1Wedon\u2019tneedtoworryaboutthe constraintthat\u03c6j \u22650,becauseas (i) we\u2019llshortlysee,thesolutionwe\u2019ll \u2202 L(\u03c6) = \u2211 n w j +\u03b2 findfromthisderivationwillauto- \u2202\u03c6 \u03c6 maticallysatisfythatanyway.",
      "chunk_id": 212,
      "start_pos": 202718,
      "end_pos": 203815,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "gderivatives,wefind: 1Wedon\u2019tneedtoworryaboutthe constraintthat\u03c6j \u22650,becauseas (i) we\u2019llshortlysee,thesolutionwe\u2019ll \u2202 L(\u03c6) = \u2211 n w j +\u03b2 findfromthisderivationwillauto- \u2202\u03c6 \u03c6 maticallysatisfythatanyway. j i=1 j Settingthistozeroandsolving,weget: \u2211n w (i) i=1 j \u03c6 = j \u2212\u03b2 I.e.,\u03c6 \u221d \u2211n w (i) .Usingtheconstraintthat\u2211 \u03c6 =1,weeasilyfindthat\u2212\u03b2 = j i=1 j j j \u2211n \u2211k w (i) = \u2211n 1 = n.(Thisusedthefactthat w (i) = Q (z(i) = j),and i=1 j=1 j i=1 j i sinceprobabilitiessumto1,\u2211 w (i) =1.)WethereforehaveourM-stepupdates j j fortheparameters\u03c6: j \u03c6 := 1 \u2211 n w (i) (21.1) j n j i=1 ThederivationfortheM-stepupdatesto\u03a3 arealsoentirelystraightforward. j 22 Variational inference and variational auto-encoder Looselyspeaking,variationalauto-encoder1generallyreferstoafamilyofalgo- 1D.P. Kingma and M. Welling, rithmsthatextendtheEMalgorithmstomorecomplexmodelsparameterized \u2018\u2018Auto-Encoding Variational Bayes,\u2019\u2019 ArXiv Preprint byneuralnetworks.Itextendsthetechniqueofvariationalinferencewiththe ArXiv:1312.6114,2013. additional\u2018\u2018re-parametrizationtrick\u2019\u2019whichwillbeintroducedbelow.Variational auto-encodermaynotgivethebestperformanceformanydatasets,butitcontains severalcentralideasabouthowtoextendEMalgorithmstohigh-dimen",
      "chunk_id": 213,
      "start_pos": 203615,
      "end_pos": 204815,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ional\u2018\u2018re-parametrizationtrick\u2019\u2019whichwillbeintroducedbelow.Variational auto-encodermaynotgivethebestperformanceformanydatasets,butitcontains severalcentralideasabouthowtoextendEMalgorithmstohigh-dimensional 129 continuouslatentvariableswithnon-linearmodels.Understandingitwilllikely giveyouthelanguageandbackgroundstounderstandvariousrecentpapers relatedtoit. As a running example, we will consider the following parameterization of p(x,z;\u03b8)byaneuralnetwork.Let\u03b8bethecollectionoftheweightsofaneural networkg(z;\u03b8)thatmapsz \u2208Rk toRd.Let: z \u223c N(0,I k\u00d7k ) (22.1) x | z \u223c N(g(z;\u03b8),\u03c32I d\u00d7d ) (22.2) Here I k\u00d7k denotesidentitymatrixofdimensionkbyk,and\u03c3isascalarthatwe assumetobeknownforsimplicity. FortheGaussianmixturemodelsinsection20.1,theoptimalchoiceofQ(z) = p(z | x;\u03b8)foreachfixed\u03b8,thatistheposteriordistributionofz,canbeanalytically computed.Inmanymorecomplexmodelssuchasthemodel22.2,it\u2019sintractable tocomputetheexacttheposteriordistribution p(z | x;\u03b8). Recallthatfromequation(20.13),ELBOisalwaysalowerboundforanychoice of Q,andtherefore,wecanalsoaimforfindinganapproximationofthetrue posteriordistribution.Often,onehastousesomeparticularformtoapproximate thetrueposteriordistribution.LetQbeafamilyof",
      "chunk_id": 214,
      "start_pos": 204615,
      "end_pos": 205815,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "erboundforanychoice of Q,andtherefore,wecanalsoaimforfindinganapproximationofthetrue posteriordistribution.Often,onehastousesomeparticularformtoapproximate thetrueposteriordistribution.LetQbeafamilyofQ\u2019sthatweareconsidering, and we will aim to find a Q within the family of Q that is closest to the true posteriordistribution.Toformalize,recallthedefinitionoftheELBOlowerbound asafunctionofQand\u03b8definedinequation(20.19): ELBO(Q,\u03b8) = \u2211 n ELBO(x (i) ;Q,\u03b8) = \u2211\u2211 Q (z (i))log p(x(i),z(i);\u03b8) i i Q (z(i)) i=1 i z(i) i RecallthatEMcanbeviewedasalternatingmaximizationofELBO(Q,\u03b8).Here instead,weoptimizetheEBLOoverQ \u2208 Q: maxmaxELBO(Q,\u03b8) (22.3) Q\u2208Q \u03b8 NowthenextquestioniswhatformofQ(orwhatstructuralassumptionsto makeaboutQ)allowsustoefficientlymaximizetheobjectiveabove.Whenthe latentvariablezarehigh-dimensionaldiscretevariables,onepopularassumption is the mean field assumption, which assumes that Q (z) gives a distribution i withindependentcoordinates,orinotherwords, Q canbedecomposedinto i toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 130 chapter 22. variational inference and variational auto-encoder Q (z) = Q1(z )\u00b7\u00b7\u00b7Qk(z ).",
      "chunk_id": 215,
      "start_pos": 205615,
      "end_pos": 206767,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "notherwords, Q canbedecomposedinto i toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 130 chapter 22. variational inference and variational auto-encoder Q (z) = Q1(z )\u00b7\u00b7\u00b7Qk(z ). There are tremendous applications of mean field i i 1 i k assumptionstolearninggenerativemodelswithdiscretelatentvariables,and we refer to Blei, Kucukelbir, and McAuliffe for a survey of these models and theirimpacttoawiderangeofapplicationsincludingcomputationalbiology, computationalneuroscience,socialsciences.Wewillnotgetintothedetailsabout thediscretelatentvariablecases,andourmainfocusistodealwithcontinuous latentvariables,whichrequiresnotonlymeanfieldassumptions,butadditional techniques. When z \u2208 Rk is a continuous latent variable, there are several decisions to maketowardssuccessfullyoptimizingequation(22.3).Firstweneedtogivea succinctrepresentationofthedistributionQ becauseitisoveraninfinitenumber i ofpoints.AnaturalchoiceistoassumeQ isaGaussiandistributionwithsome i meanandvariance.Wewouldalsoliketohavemoresuccinctrepresentationof themeansofQ ofalltheexamples.NotethatQ (z(i))issupposedtoapproximate i i p(z(i) | x(i);\u03b8).ItwouldmakesenseletallthemeansoftheQ\u2019sbesomefunction i ofx(",
      "chunk_id": 216,
      "start_pos": 206567,
      "end_pos": 207767,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ".Wewouldalsoliketohavemoresuccinctrepresentationof themeansofQ ofalltheexamples.NotethatQ (z(i))issupposedtoapproximate i i p(z(i) | x(i);\u03b8).ItwouldmakesenseletallthemeansoftheQ\u2019sbesomefunction i ofx(i).Concretely,letq(\u00b7;\u03c6),v(\u00b7;\u03c6)betwofunctionsthatmapfromdimensiond tok,whichareparameterizedby\u03c6and\u03c8,weassumethat: Q = N(q(x (i) ;\u03c6),diag(v(x (i) ;\u03c8))2) (22.4) i Herediag(w)meansthek\u00d7kmatrixwiththeentriesofw \u2208Rk onthediagonal. Inotherwords,thedistributionQ isassumedtobeaGaussiandistributionwith i independentcoordinates,andthemeanandstandarddeviationsaregoverned by q and v.Ofteninvariationalauto-encoder, q and v arechosentobeneural networks.2Inrecentdeeplearningliterature,oftenq,varecalledencoder(inthe 2qandvcanalsoshareparameters. senseofencodingthedataintolatentcode),whereasg(z;\u03b8)ifoftenreferredto Wesweepthislevelofdetailsunder theruginthisnote. asthedecoder. WeremarkthatQ ofsuchforminmanycasesareveryfarfromagoodap- i proximationofthetrueposteriordistribution.However,someapproximationis necessaryforfeasibleoptimization.Infact,theformofQ needstosatisfyother i requirements(whichhappenedtobesatisfiedbytheform22.4) BeforeoptimizingtheELBO,let\u2019sfirstverifywhetherwecanefficientlyevaluate thev",
      "chunk_id": 217,
      "start_pos": 207567,
      "end_pos": 208767,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "essaryforfeasibleoptimization.Infact,theformofQ needstosatisfyother i requirements(whichhappenedtobesatisfiedbytheform22.4) BeforeoptimizingtheELBO,let\u2019sfirstverifywhetherwecanefficientlyevaluate thevalueoftheELBOforfixedQoftheform22.4and\u03b8.WerewritetheELBOas afunctionof\u03c6,\u03c8,\u03b8by: (cid:34) (cid:35) ELBO(\u03c6,\u03c8,\u03b8) = \u2211 n E log p(x(i),z(i);\u03b8) , (22.5) z(i)\u223cQi Q (z(i)) i=1 i 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 131 whereQ = N(q(x(i);\u03c6),diag(v(x(i);\u03c8))2).NotethattoevaluateQ (z(i))inside i i the expectation, we should be able to compute the density of Q. To estimate i theexpectationE ,weshouldbeabletosamplefromdistribution Q so z(i)\u223cQi i thatwecanbuildanempiricalestimatorwithsamples.IthappensthatforGaus- sian distribution Q = N(q(x(i);\u03c6),diag(v(x(i);\u03c8))2), we are able to do both i efficiently. Nowlet\u2019soptimizetheELBO.Itturnsoutthatwecanrungradientascentover \u03c6,\u03c8,\u03b8insteadofalternatingmaximization.Thereisnostrongneedtocomputethe maximumovereachvariableatamuchgreatercost.(ForGaussianmixturemodel insection20.1,computingthemaximumisanalyticallyfeasibleandrelatively cheap,andthereforewedidalternatingmaximization.)Mathematically,let\u03b7be thelearningrate,thegradientasce",
      "chunk_id": 218,
      "start_pos": 208567,
      "end_pos": 209767,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "cost.(ForGaussianmixturemodel insection20.1,computingthemaximumisanalyticallyfeasibleandrelatively cheap,andthereforewedidalternatingmaximization.)Mathematically,let\u03b7be thelearningrate,thegradientascentstepis: \u03b8 := \u03b8+\u03b7\u2207 ELBO(\u03c6,\u03c8,\u03b8) \u03b8 \u03c6 := \u03c6+\u03b7\u2207 ELBO(\u03c6,\u03c8,\u03b8) \u03c6 \u03c8 := \u03c8+\u03b7\u2207 ELBO(\u03c6,\u03c8,\u03b8) \u03c8 Computingthegradientover\u03b8issimplebecause: (cid:34) (cid:35) \u2207 ELBO(\u03c6,\u03c8,\u03b8) = \u2207 \u2211 n E logp(x(i),z(i);\u03b8) (22.6) \u03b8 \u03b8 z(i)\u223cQi Q (z(i)) i=1 i n (cid:104) (cid:105) = \u2207 \u2211E logp(x (i) ,z (i) ;\u03b8) (22.7) \u03b8 z(i)\u223cQi i=1 n (cid:104) (cid:105) = \u2211E \u2207 logp(x (i) ,z (i) ;\u03b8) (22.8) z(i)\u223cQi \u03b8 i=1 Butcomputingthegradientover\u03c6and\u03c8istrickybecausethesamplingdis- tributionQ dependson\u03c6and\u03c8.(Abstractlyspeaking,theissuewefacecanbe i simplifiedastheproblemofcomputingthegradientE z\u223cQ\u03c6 [f(\u03c6)]withrespect tovariable\u03c6.Weknowthatingeneral,\u2207E z\u223cQ\u03c6 [f(\u03c6)] (cid:54)=E z\u223cQ\u03c6 [\u2207f(\u03c6)]because thedependencyofQ on\u03c6hastobetakenintoaccountaswell.) \u03c6 Theideathatcomestorescueistheso-calledre-parameterizationtrick:we rewritez(i) \u223c Q = N(q(x(i);\u03c6),diag(v(x(i);\u03c8))2)inanequivalentway: i z (i) = q(x (i) ;\u03c6)+v(x (i) ;\u03c8)(cid:12)\u03be (i) where\u03be (i) \u223c N(0,I k\u00d7k ) (22.9) toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 132 chapter 22.",
      "chunk_id": 219,
      "start_pos": 209567,
      "end_pos": 210759,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ag(v(x(i);\u03c8))2)inanequivalentway: i z (i) = q(x (i) ;\u03c6)+v(x (i) ;\u03c8)(cid:12)\u03be (i) where\u03be (i) \u223c N(0,I k\u00d7k ) (22.9) toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 132 chapter 22. variational inference and variational auto-encoder Herex(cid:12)ydenotestheentry-wiseproductoftwovectorsofthesamedimen- sion.Hereweusedthefactthat x \u223c N(\u00b5,\u03c32)isequivalenttothat x = \u00b5+\u03be\u03c3 with\u03be \u223c N(0,1).Wemostlyjustusedthisfactineverydimensionsimultaneously fortherandomvariablez(i) \u223c Q. i Withthisre-parameterization,wehavethat: (cid:34) (cid:35) (cid:34) (cid:35) p(x(i),z(i);\u03b8) p(x(i),q(x(i);\u03c6)+v(x(i);\u03c8)(cid:12)\u03be(i);\u03b8) E log =E log (22.10) z(i)\u223cQi Q (z(i)) \u03be(i)\u223cN(0,1) Q (q(x(i);\u03c6)+v(x(i);\u03c8)(cid:12)\u03be(i)) i i Itfollowsthat: (cid:34) (cid:35) p(x(i),z(i);\u03b8) \u2207 E log (22.11) \u03c6 z(i)\u223cQi Q (z(i)) i (cid:34) (cid:35) p(x(i),q(x(i);\u03c6)+v(x(i);\u03c8)(cid:12)\u03be(i);\u03b8) = \u2207 E log (22.12) \u03c6 \u03be(i)\u223cN(0,1) Q (q(x(i);\u03c6)+v(x(i);\u03c8)(cid:12)\u03be(i)) i (cid:34) (cid:35) p(x(i),q(x(i);\u03c6)+v(x(i);\u03c8)(cid:12)\u03be(i);\u03b8) =E \u2207 log (22.13) \u03be(i)\u223cN(0,1) \u03c6 Q (q(x(i);\u03c6)+v(x(i);\u03c8)(cid:12)\u03be(i)) i We can now sample multiple copies of \u03be(i)\u2019s to estimate the expectation in theRHSoftheequationabove.3 Wecanestimatethegradientwithrespectto 3Emp",
      "chunk_id": 220,
      "start_pos": 210559,
      "end_pos": 211759,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "13) \u03be(i)\u223cN(0,1) \u03c6 Q (q(x(i);\u03c6)+v(x(i);\u03c8)(cid:12)\u03be(i)) i We can now sample multiple copies of \u03be(i)\u2019s to estimate the expectation in theRHSoftheequationabove.3 Wecanestimatethegradientwithrespectto 3Empirically people sometimes \u03c8similarly,andwiththese,wecanimplementthegradientascentalgorithmto justuseonesampletoestimateit formaximumcomputationaleffi- optimizetheELBOover\u03c6,\u03c8,\u03b8. ciency. There are not many high-dimensional distributions with analytically com- putabledensityfunctionareknowntobere-parameterizable.WerefertoKingma andWellingforafewotherchoicesthatcanreplaceGaussiandistribution. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc Part IX: Factor Analysis FromCS229Spring2021,Andrew Whenwehavedatax(i) \u2208RdthatcomesfromamixtureofseveralGaussians,the Ng,MosesCharikar&Christopher R\u00e9,StanfordUniversity. EMalgorithmcanbeappliedtofitamixturemodel.Inthissetting,weusually imagineproblemswherewehavesufficientdatatobeabletodiscernthemultiple- Gaussianstructureinthedata.Forinstance,thiswouldbethecaseifourtraining setsizenwassignificantlylargerthanthedimensiondofthedata.",
      "chunk_id": 221,
      "start_pos": 211559,
      "end_pos": 212657,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "y imagineproblemswherewehavesufficientdatatobeabletodiscernthemultiple- Gaussianstructureinthedata.Forinstance,thiswouldbethecaseifourtraining setsizenwassignificantlylargerthanthedimensiondofthedata. Now,considerasettinginwhichd (cid:29) n.Insuchaproblem,itmightbedifficult tomodelthedataevenwithasingleGaussian,muchlessamixtureofGaussian. Specifically,sincethendatapointsspanonlyalow-dimensionalsubspaceofRd, ifwemodelthedataasGaussian,andestimatethemeanandcovarianceusing theusualmaximumlikelihoodestimators, \u00b5 = 1 \u2211 n x (i) n i=1 \u03a3 = 1 \u2211 n (x (i)\u2212\u00b5)(x (i)\u2212\u00b5)(cid:62) , n i=1 wewouldfindthatthematrix\u03a3issingular.Thismeansthat\u03a3\u22121doesnotexist,and 1/|\u03a3|1/2 =1/0.Butbothofthesetermsareneededincomputingtheusualdensity ofamultivariateGaussiandistribution.Anotherwayofstatingthisdifficultyis thatmaximumlikelihoodestimatesoftheparametersresultinaGaussianthat places all of its probability in the affine space spanned by the data,4 and this 4Thisisthesetofpointsxsatisfy- correspondstoasingularcovariancematrix. ingx = \u2211 i n =1 \u03b1ix(i),forsome\u03b1i\u2019s Moregenerally,unlessnexceedsdbysomereasonableamount,themaximum sothat\u2211 i n =1 \u03b1i =1.",
      "chunk_id": 222,
      "start_pos": 212457,
      "end_pos": 213584,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "and this 4Thisisthesetofpointsxsatisfy- correspondstoasingularcovariancematrix. ingx = \u2211 i n =1 \u03b1ix(i),forsome\u03b1i\u2019s Moregenerally,unlessnexceedsdbysomereasonableamount,themaximum sothat\u2211 i n =1 \u03b1i =1. likelihoodestimatesofthemeanandcovariancemaybequitepoor.Nonetheless, wewouldstillliketobeabletofitareasonableGaussianmodeltothedata,and perhapscapturesomeinterestingcovariancestructureinthedata.Howcanwe dothis? Inthenextsection,webeginbyreviewingtwopossiblerestrictionson\u03a3that allowustofit\u03a3withsmallamountsofdatabutneitherwillgiveasatisfactory solutiontoourproblem.WenextdiscusssomepropertiesofGaussiansthatwill beneededlater;specifically,howtofindmarginalandconditonaldistributionsof Gaussians.Finally,wepresentthefactoranalysismodel,andEMforit. \u03a3 23 Restrictions of Ifwedonothavesufficientdatatofitafullcovariancematrix,wemayplace somerestrictionsonthespaceofmatrices\u03a3thatwewillconsider.Forinstance, wemaychoosetofitacovariancematrix\u03a3thatisdiagonal.Inthissetting,the readermayeasilyverifythatthemaximumlikelihoodestimateofthecovariance matrixisgivenbythediagonalmatrix\u03a3satisfying \u03a3 = 1 \u2211 n (x (i) \u2212\u00b5 )2. jj n j j i=1 Thus,\u03a3 isjusttheempiricalestimateofthevarianceofthej-thcoordinateofthe jj data.",
      "chunk_id": 223,
      "start_pos": 213384,
      "end_pos": 214583,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "aximumlikelihoodestimateofthecovariance matrixisgivenbythediagonalmatrix\u03a3satisfying \u03a3 = 1 \u2211 n (x (i) \u2212\u00b5 )2. jj n j j i=1 Thus,\u03a3 isjusttheempiricalestimateofthevarianceofthej-thcoordinateofthe jj data. RecallthatthecontoursofaGaussiandensityareellipses.Adiagonal\u03a3corre- spondstoaGaussianwherethemajoraxesoftheseellipsesareaxis-aligned. Sometimes,wemayplaceafurtherrestrictiononthecovariancematrixthatnot onlymustitbediagonal,butitsdiagonalentriesmustallbeequal.Inthissetting, wehave\u03a3 = \u03c32I,where\u03c32 istheparameterunderourcontrol.Themaximum likelihoodestimateof\u03c32canbefoundtobe: \u03c32 = 1 \u2211 d \u2211 n (x (i) \u2212\u00b5 )2. nd j j j=1i=1 ThismodelcorrespondstousingGaussianswhosedensitieshavecontoursthat arecircles(in2dimensions;orspheres/hyperspheresinhigherdimensions). Ifwearefittingafull,unconstrained,covariancematrix\u03a3todata,itisnecessary thatn \u2265 d+1inorderforthemaximumlikelihoodestimateof\u03a3nottobesingular. Undereitherofthetworestrictionsabove,wemayobtainnon-singular\u03a3when n \u22652. However,restricting\u03a3tobediagonalalsomeansmodelingthedifferentcoor- dinatesx,x ofthedataasbeinguncorrelatedandindependent.Often,itwould i j benicetobeabletocapturesomeinterestingcorrelationstructureinthedata.If weweretouseeitherofther",
      "chunk_id": 224,
      "start_pos": 214383,
      "end_pos": 215583,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "lsomeansmodelingthedifferentcoor- dinatesx,x ofthedataasbeinguncorrelatedandindependent.Often,itwould i j benicetobeabletocapturesomeinterestingcorrelationstructureinthedata.If weweretouseeitheroftherestrictionson\u03a3describedabove,wewouldtherefore failtodoso.Inthissetofnotes,wewilldescribethefactoranalysismodel,which usesmoreparametersthanthediagonal\u03a3andcapturessomecorrelationsinthe data,butalsowithouthavingtofitafullcovariancematrix. 135 24 Marginals and conditionals of Gaussians Beforedescribingfactoranalysis,wedigresstotalkabouthowtofindconditional andmarginaldistributionsofrandomvariableswithajointmultivariateGaussian distribution. Supposewehaveavector-valuedrandomvariable (cid:34) (cid:35) x x = 1 , x 2 wherex \u2208Rr,x \u2208Rs,andx \u2208Rr+s.Supposex \u223c N(\u00b5,\u03a3),where 1 2 (cid:34) (cid:35) (cid:34) (cid:35) \u00b5 \u03a3 \u03a3 \u00b5 = 1 , \u03a3 = 11 12 . \u00b5 \u03a3 \u03a3 2 21 22 Here, \u00b5 \u2208 Rr, \u00b5 \u2208 Rs, \u03a3 \u2208 Rr\u00d7r, \u03a3 \u2208 Rr\u00d7s, and so on. Note that since 1 2 11 12 covariancematricesaresymmetric,\u03a3 = \u03a3(cid:62). 12 21 Underourassumptions,x andx arejointlymultivariateGaussian.Whatis 1 2 themarginaldistributionof x ?ItisnothardtoseethatE[x ] = \u00b5 ,andthat 1 1 1 Cov(x ) = E[(x \u2212\u00b5 )(x \u2212\u00b5 )] = \u03a3 .Toseethatthelatteristrue,notethat 1 1 1 1 1 1",
      "chunk_id": 225,
      "start_pos": 215383,
      "end_pos": 216583,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "andx arejointlymultivariateGaussian.Whatis 1 2 themarginaldistributionof x ?ItisnothardtoseethatE[x ] = \u00b5 ,andthat 1 1 1 Cov(x ) = E[(x \u2212\u00b5 )(x \u2212\u00b5 )] = \u03a3 .Toseethatthelatteristrue,notethat 1 1 1 1 1 11 bydefinitionofthejointcovarianceofx andx ,wehavethat: 1 2 Cov(x) = \u03a3 (cid:34) (cid:35) \u03a3 \u03a3 = 11 12 \u03a3 \u03a3 21 22 =E[(x\u2212\u00b5)(x\u2212\u00b5)(cid:62)] (cid:104) (cid:105) =E (x1 \u2212\u00b51) (x1 \u2212\u00b51) (cid:62) x2 \u2212\u00b52 x2 \u2212\u00b52 (cid:34) (cid:35) (x \u2212\u00b5 )(x \u2212\u00b5 )(cid:62) (x \u2212\u00b5 )(x \u2212\u00b5 )(cid:62) =E 1 1 1 1 1 1 2 2 . (x \u2212\u00b5 )(x \u2212\u00b5 )(cid:62) (x \u2212\u00b5 )(x \u2212\u00b5 )(cid:62) 2 2 1 1 2 2 2 2 Matchingtheupper-leftsubblocksinthematricesinthesecondandthelastlines abovegivestheresult. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu SincemarginaldistributionsofGaussiansarethemselvesGaussian,wethere- forehavethatthemarginaldistributionofx isgivenbyx \u223c N(\u00b5 ,\u03a3 ).Also, 1 1 1 11 wecanask,whatistheconditionaldistributionof x givenx ?Byreferringto 1 2 the definition of the multivariate Gaussian distribution, it can be shown that x | x \u223c N(\u00b5 ,\u03a3 ),where: 1 2 1|2 1|2 \u00b5 = \u00b5 +\u03a3 \u03a3\u221212(x \u2212\u00b5 ) (24.1) 1|2 1 12 2 2 2 \u03a3 = \u03a3 \u2212\u03a3 \u03a3\u221212\u03a3 (24.2) 1|2 11 12 2 21 Whenweworkwiththefactoranalysismodelinthenextsection,theseformulas for finding co",
      "chunk_id": 226,
      "start_pos": 216383,
      "end_pos": 217583,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "at x | x \u223c N(\u00b5 ,\u03a3 ),where: 1 2 1|2 1|2 \u00b5 = \u00b5 +\u03a3 \u03a3\u221212(x \u2212\u00b5 ) (24.1) 1|2 1 12 2 2 2 \u03a3 = \u03a3 \u2212\u03a3 \u03a3\u221212\u03a3 (24.2) 1|2 11 12 2 21 Whenweworkwiththefactoranalysismodelinthenextsection,theseformulas for finding conditional and marginal distributions of Gaussians will be very useful. 25 The factor analysis model Inthefactoranalysismodel,wepositajointdistributionon(x,z)asfollows, wherez \u2208Rk isalatentrandomvariable: z \u223c N(0,I) x | z \u223c N(\u00b5+\u039bz,\u03a8) Here,theparametersofourmodelarethevector\u00b5 \u2208Rd,thematrix\u039b \u2208Rd\u00d7k, andthediagonalmatrix\u03a8 \u2208Rd\u00d7d.Thevalueofkisusuallychosentobesmaller thand. Thus,weimaginethateachdatapointx(i) isgeneratedbysamplingakdimen- sionmultivariateGaussianz(i).Then,itismappedtoad-dimensionalaffinespace ofRd bycomputing\u00b5+\u039bz(i).Lastly,x(i) isgeneratedbyaddingcovariance\u03a8 noiseto\u00b5+\u039bz(i). Equivalently(convinceyourselfthatthisisthecase),wecanthereforealso definethefactoranalysismodelaccordingto z \u223c N(0,I) (cid:101) \u223c N(0,\u03a8) x = \u00b5+\u039bz+(cid:101) 137 where(cid:101)andzareindependent. Let\u2019sworkoutexactlywhatdistributionourmodeldefines.Ourrandomvari- ableszandxhaveajointGaussiandistribution (cid:34) (cid:35) z \u223c N(\u00b5 ,\u03a3). zx x Wewillnowfind\u00b5 and\u03a3.",
      "chunk_id": 227,
      "start_pos": 217383,
      "end_pos": 218531,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "37 where(cid:101)andzareindependent. Let\u2019sworkoutexactlywhatdistributionourmodeldefines.Ourrandomvari- ableszandxhaveajointGaussiandistribution (cid:34) (cid:35) z \u223c N(\u00b5 ,\u03a3). zx x Wewillnowfind\u00b5 and\u03a3. zx WeknowthatE[z] =0,fromthefactthatz \u223c N(0,I).Also,wehavethat: E[x] =E[\u00b5+\u039bz+(cid:101)] = \u00b5+\u039bE[z]+E[(cid:101)] = \u00b5 Puttingthesetogether,weobtain (cid:34) (cid:35) 0 \u00b5 = zx \u00b5 Next,tofind\u03a3,weneedtocalculate: \u03a3 =E[(z\u2212E[z])(z\u2212E[z])(cid:62)] (theupper-leftblockof\u03a3) zz \u03a3 =E[(z\u2212E[z])(x\u2212E[x])(cid:62)] (upper-rightblock) zx \u03a3 =E[(x\u2212E[x])(x\u2212E[x])(cid:62)] (lower-rightblock) xx Now,sincez \u223c N(0,I),weeasilyfindthat\u03a3 =Cov(z) = I.Also, zz E[(z\u2212E[z])(x\u2212E[x])(cid:62)] =E[z(\u00b5+\u039bz+(cid:101)\u2212\u00b5)(cid:62)] =E[zz (cid:62)]\u039b(cid:62)+E[z(cid:101) (cid:62)] = \u039b(cid:62) Inthelaststep,weusedthefactthatE[zz(cid:62)] =Cov(z)(sincezhaszeromean), and E[z(cid:101)(cid:62)] = E[z]E[(cid:101)(cid:62)] = 0 (since z and (cid:101) are independent, and hence the expectationoftheirproductistheproductoftheirexpectations).Similarly,we canfind\u03a3 asfollows: xx E[(x\u2212E[x])(x\u2212E[x])(cid:62)] =E[(\u00b5+\u039bz+(cid:101)\u2212\u00b5)(\u00b5+\u039bz+(cid:101)\u2212\u00b5)(cid:62)] =E[\u039bzz (cid:62)\u039b(cid:62)+(cid:101)z (cid:62)\u039b(cid:62)+\u039bz(cid:101) (cid:62)+(cid:101)(cid:101) (",
      "chunk_id": 228,
      "start_pos": 218331,
      "end_pos": 219531,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "y,we canfind\u03a3 asfollows: xx E[(x\u2212E[x])(x\u2212E[x])(cid:62)] =E[(\u00b5+\u039bz+(cid:101)\u2212\u00b5)(\u00b5+\u039bz+(cid:101)\u2212\u00b5)(cid:62)] =E[\u039bzz (cid:62)\u039b(cid:62)+(cid:101)z (cid:62)\u039b(cid:62)+\u039bz(cid:101) (cid:62)+(cid:101)(cid:101) (cid:62)] = \u039bE[zz (cid:62)]\u039b(cid:62)+E[(cid:101)(cid:101) (cid:62)] = \u039b\u039b(cid:62)+\u03a8 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu Puttingeverythingtogether,wethereforehavethat (cid:34) (cid:35) (cid:32)(cid:34) (cid:35) (cid:34) (cid:35)(cid:33) z 0 I \u039b(cid:62) \u223c N , . (25.1) x \u00b5 \u039b \u039b\u039b(cid:62)+\u03a8 Hence,wealsoseethatthemarginaldistributionofxisgivenbyx \u223c N(\u00b5,\u039b\u039b(cid:62)+ \u03a8).Thus,givenatrainingsetx(i);i =1,...,n,wecanwritedowntheloglikeli- hoodoftheparameters: (cid:96)(\u00b5,\u039b,\u03a8) =log \u220f n 1 exp (cid:18) \u2212 1 (x (i)\u2212\u00b5)(cid:62)(\u039b\u039b(cid:62)+\u03a8)\u22121(x (i)\u2212\u00b5) (cid:19) (2\u03c0)d/2|\u039b\u039b(cid:62)+\u03a8|1/2 2 i=1 (25.2) To perform maximum likelihood estimation, we would like to maximize this quantitywithrespecttotheparameters.Butmaximizingthisformulaexplicitly ishard(tryityourself),andweareawareofnoalgorithmthatdoessoinclosed- form.So,wewillinsteadusetotheEMalgorithm.Inthenextsection,wederive EMforfactoranalysis.",
      "chunk_id": 229,
      "start_pos": 219331,
      "end_pos": 220447,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "arameters.Butmaximizingthisformulaexplicitly ishard(tryityourself),andweareawareofnoalgorithmthatdoessoinclosed- form.So,wewillinsteadusetotheEMalgorithm.Inthenextsection,wederive EMforfactoranalysis. 26 EM for factor analysis ThederivationfortheE-stepiseasy.WeneedtocomputeQ (z(i)) = p(z(i) | i x(i);\u00b5,\u039b,\u03a8).Bysubstitutingthedistributiongiveninequation(25.1)intothe formulas24.1-24.2usedforfindingtheconditionaldistributionofaGaussian, wefindthatz(i) | x(i);\u00b5,\u039b,\u03a8 \u223c N(\u00b5 ,\u03a3 ),where z(i)|x(i) z(i)|x(i) \u00b5 = \u039b(cid:62)(\u039b\u039b(cid:62)+\u03a8)\u22121(x (i)\u2212\u00b5) z(i)|x(i) \u03a3 = I\u2212\u039b(cid:62)(\u039b\u039b(cid:62)+\u03a8)\u22121\u039b z(i)|x(i) So,usingthesedefinitionsfor\u00b5 and\u03a3 ,wehave: z(i)|x(i) z(i)|x(i) (cid:18) (cid:19) 1 1 Q (z (i)) = exp \u2212 (z (i)\u2212\u00b5 )(cid:62)\u03a3\u22121 (z (i)\u2212\u00b5 ) i (2\u03c0)k/2|\u03a3 |1/2 2 z(i)|x(i) z(i)|x(i) z(i)|x(i) z(i)|x(i) Let\u2019snowworkouttheM-step.Here,weneedtomaximize \u2211 n (cid:90) Q (z (i))log p(x(i),z(i);\u00b5,\u039b,\u03a8) dz (i) (26.1) i=1 z(i) i Q i (z(i)) 139 withrespecttotheparameters\u00b5,\u039b,\u03a8.Wewillworkoutonlytheoptimization with respect to \u039b, and leave the derivations of the updates for \u00b5 and \u03a8 as an exercisetothereader.",
      "chunk_id": 230,
      "start_pos": 220247,
      "end_pos": 221330,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "(i) (26.1) i=1 z(i) i Q i (z(i)) 139 withrespecttotheparameters\u00b5,\u039b,\u03a8.Wewillworkoutonlytheoptimization with respect to \u039b, and leave the derivations of the updates for \u00b5 and \u03a8 as an exercisetothereader. Wecansimplifyequation(26.1)asfollows: n (cid:90) \u2211 Q (z (i))[logp(x (i) | z (i) ;\u00b5,\u039b,\u03a8)+logp(z (i))\u2212logQ (z (i))]dz (i) (26.2) i i i=1 z(i) n = \u2211E [logp(x (i) | z (i) ;\u00b5,\u039b,\u03a8)+logp(z (i))\u2212logQ (z (i))] (26.3) z(i)\u223cQi i i=1 Here,the\u2018\u2018z(i) \u223c Q\u2019\u2019subscriptindicatesthattheexpectationiswithrespectto i z(i) drawnfromQ.Inthesubsequentdevelopment,wewillomitthissubscript i whenthereisnoriskofambiguity.Droppingtermsthatdonotdependonthe parameters,wefindthatweneedtomaximize: n \u2211E[logp(x (i) | z (i) ;\u00b5,\u039b,\u03a8)] i=1 = \u2211 n E (cid:20) log 1 exp (cid:18) \u2212 1 (x (i)\u2212\u00b5\u2212\u039bz (i))(cid:62)\u03a8\u22121(x (i)\u2212\u00b5\u2212\u039bz (i)) (cid:19)(cid:21) (2\u03c0)d/2|\u03a8|1/2 2 i=1 = \u2211 n E (cid:20) \u2212 1 log|\u03a8|\u2212 n log(2\u03c0)\u2212 1 (x (i)\u2212\u00b5\u2212\u039bz (i))(cid:62)\u03a8\u22121(x (i)\u2212\u00b5\u2212\u039bz (i)) (cid:21) 2 2 2 i=1 Let\u2019smaximizethiswithrespectto\u039b.Onlythelasttermabovedependson\u039b. Taking derivatives, and using the facts that tr(a) = a (for a \u2208 R), tr(AB) = tr(BA),and\u2207 tr(ABA(cid:62)C) =CAB+C(cid:62)AB(cid:62),weget: A \u2207\u039b \u2211 n \u2212E (cid:20) 1 (x (i)\u2212\u00b5\u2212\u039bz (i))(cid:62)\u03a8\u22121(x (i)\u2212\u00b5\u2212\u039bz (i))",
      "chunk_id": 231,
      "start_pos": 221130,
      "end_pos": 222330,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "g derivatives, and using the facts that tr(a) = a (for a \u2208 R), tr(AB) = tr(BA),and\u2207 tr(ABA(cid:62)C) =CAB+C(cid:62)AB(cid:62),weget: A \u2207\u039b \u2211 n \u2212E (cid:20) 1 (x (i)\u2212\u00b5\u2212\u039bz (i))(cid:62)\u03a8\u22121(x (i)\u2212\u00b5\u2212\u039bz (i)) (cid:21) 2 i=1 = \u2211 n \u2207\u039b E (cid:20) \u2212tr( 1 z (i)(cid:62)\u039b(cid:62)\u03a8\u22121\u039bz (i))+tr(z (i)(cid:62)\u039b(cid:62)\u03a8\u22121(x (i)\u2212\u00b5)) (cid:21) 2 i=1 = \u2211 n \u2207\u039b E (cid:20) \u2212tr( 1 \u039b(cid:62)\u03a8\u22121\u039bz (i) z (i)(cid:62) )+tr(\u039b(cid:62)\u03a8\u22121(x (i)\u2212\u00b5)z (i)(cid:62) ) (cid:21) 2 i=1 n (cid:104) (cid:105) = \u2211E \u2212\u03a8\u22121\u039bz (i) z (i)(cid:62) +\u03a8\u22121(x (i)\u2212\u00b5)z (i)(cid:62) i=1 Settingthistozeroandsimplifying,weget: n (cid:104) (cid:105) n (cid:104) (cid:105) \u2211\u039bE z (i) z (i)(cid:62) = \u2211 (x (i)\u2212\u00b5)E z (i)(cid:62) z(i)\u223cQi z(i)\u223cQi i=1 i=1 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 140 chapter 26. em for factor analysis Hence,solvingfor\u039b,weobtain (cid:32) (cid:33)(cid:32) (cid:33)\u22121 n (cid:104) (cid:105) n (cid:104) (cid:105) \u039b = \u2211 (x (i)\u2212\u00b5)E z (i)(cid:62) \u2211E z (i) z (i)(cid:62) (26.4) z(i)\u223cQi z(i)\u223cQi i=1 i=1 Itisinterestingtonotethecloserelationshipbetweenthisequationandthenormal equationthatwe\u2019dderivedforleastsquaresregression, \u2018\u2018\u03b8 (cid:62) = (y (cid:62) X)(X (cid:62) X)\u22121.\u2019\u2019 Theanalogyisthathere,thex\u2019sarealine",
      "chunk_id": 232,
      "start_pos": 222130,
      "end_pos": 223330,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "erestingtonotethecloserelationshipbetweenthisequationandthenormal equationthatwe\u2019dderivedforleastsquaresregression, \u2018\u2018\u03b8 (cid:62) = (y (cid:62) X)(X (cid:62) X)\u22121.\u2019\u2019 Theanalogyisthathere,thex\u2019sarealinearfunctionofthez\u2019s(plusnoise).Given the\u2018\u2018guesses\u2019\u2019forzthattheE-stephasfound,wewillnowtrytoestimatethe unknownlinearity\u039brelatingthex\u2019sandz\u2019s.Itisthereforenosurprisethatwe obtainsomethingsimilartothenormalequation.Thereis,however,oneimportant differencebetweenthisandanalgorithmthatperformsleastsquaresusingjust the\u2018\u2018bestguesses\u2019\u2019ofthez\u2019s;wewillseethisdifferenceshortly. TocompleteourM-stepupdate,let\u2019sworkoutthevaluesoftheexpectations inequation(26.4).FromourdefinitionofQ beingGaussianwithmean\u00b5 i z(i)|x(i) andcovariance\u03a3 ,weeasilyfind z(i)|x(i) (cid:104) (cid:105) E z (i)(cid:62) = \u00b5 (cid:62) z(i)\u223cQi z(i)|x(i) (cid:104) (cid:105) E z (i) z (i)(cid:62) = \u00b5 \u00b5 (cid:62) +\u03a3 z(i)\u223cQi z(i)|x(i) z(i)|x(i) z(i)|x(i) Thelattercomesfromthefactthat,forarandomvariableY,Cov(Y) =E[YY(cid:62)]\u2212 E[Y]E[Y](cid:62),andhenceE[YY(cid:62)] = E[Y]E[Y](cid:62)+Cov(Y).Substitutingthisback intoequation(26.4),wegettheM-stepupdatefor\u039b: (cid:32) (cid:33)(cid:32) (cid:33)\u22121 n n \u039b = \u2211 (x (i)\u2212\u00b5)\u00b5 (cid:62) \u2211 \u00b5 \u00b5 (cid:62) +\u03a3",
      "chunk_id": 233,
      "start_pos": 223130,
      "end_pos": 224330,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ",andhenceE[YY(cid:62)] = E[Y]E[Y](cid:62)+Cov(Y).Substitutingthisback intoequation(26.4),wegettheM-stepupdatefor\u039b: (cid:32) (cid:33)(cid:32) (cid:33)\u22121 n n \u039b = \u2211 (x (i)\u2212\u00b5)\u00b5 (cid:62) \u2211 \u00b5 \u00b5 (cid:62) +\u03a3 (26.5) z(i)|x(i) z(i)|x(i) z(i)|x(i) z(i)|x(i) i=1 i=1 Itisimportanttonotethepresenceofthe\u03a3 ontherighthandsideofthis z(i)|x(i) equation.Thisisthecovarianceintheposteriordistribution p(z(i) | x(i))ofz(i) givenx(i),andtheM-stepmusttakeintoaccountthisuncertaintyaboutz(i)inthe posterior.AcommonmistakeinderivingEMistoassumethatintheE-step,we needtocalculateonlyexpectationE[z]ofthelatentrandomvariablez,andthen plugthatintotheoptimizationintheM-stepeverywherezoccurs.Whilethis workedforsimpleproblemssuchasthemixtureofGaussians,inourderivation 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 141 forfactoranalysis,weneededE[zz(cid:62)]aswellasE[z];andaswesaw,E[zz(cid:62)]and E[z]E[z](cid:62) differbythe quantity \u03a3 . Thus, the M-stepupdate must takeinto z|x accountthecovarianceofzintheposteriordistribution p(z(i) | x(i)). Lastly,wecanalsofindtheM-stepoptimizationsfortheparameters\u00b5and\u03a8. Itisnothardtoshowthatthefirstisgivenby \u00b5 = 1 \u2211 n x (i) .",
      "chunk_id": 234,
      "start_pos": 224130,
      "end_pos": 225297,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "takeinto z|x accountthecovarianceofzintheposteriordistribution p(z(i) | x(i)). Lastly,wecanalsofindtheM-stepoptimizationsfortheparameters\u00b5and\u03a8. Itisnothardtoshowthatthefirstisgivenby \u00b5 = 1 \u2211 n x (i) . n i=1 Sincethisdoesn\u2019tchangeastheparametersarevaried(i.e.,unliketheupdatefor \u039b,therighthandsidedoesnotdependonQ (z(i)) = p(z(i) | x(i);\u00b5,\u039b,\u03a8),which i inturndependsontheparameters),thiscanbecalculatedjustonceandneeds notbefurtherupdatedasthealgorithmisrun.Similarly,thediagonal\u03a8canbe foundbycalculating \u03a6 = 1 \u2211 n x (i) x (i)(cid:62) \u2212x (i) \u00b5 (cid:62) \u039b(cid:62)\u2212\u039b\u00b5 x (i)(cid:62) +\u039b (cid:16) \u00b5 \u00b5 (cid:62) +\u03a3 (cid:17) \u039b(cid:62) , n z(i)|x(i) z(i)|x(i) z(i)|x(i) z(i)|x(i) z(i)|x(i) i=1 andsetting\u03a8 = \u03a6 (i.e.,letting\u03a8bethediagonalmatrixcontainingonlythe ii ii diagonalentriesof\u03a6). toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu Part X: Principal Components Analysis FromCS229Spring2021,Andrew In our discussion of factor analysis, we gave a way to model data x \u2208 Rd as Ng,MosesCharikar&Christopher \u2018\u2018approximately\u2019\u2019lyinginsomek-dimensionsubspace,wherek (cid:28) d.Specifically, R\u00e9,StanfordUniversity.",
      "chunk_id": 235,
      "start_pos": 225097,
      "end_pos": 226218,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "discussion of factor analysis, we gave a way to model data x \u2208 Rd as Ng,MosesCharikar&Christopher \u2018\u2018approximately\u2019\u2019lyinginsomek-dimensionsubspace,wherek (cid:28) d.Specifically, R\u00e9,StanfordUniversity. weimaginedthateachpointx(i) wascreatedbyfirstgeneratingsomez(i) lying inthek-dimensionaffinespace\u039bz+\u00b5;z \u2208Rk,andthenadding\u03a8-covariance noise.Factoranalysisisbasedonaprobabilisticmodel,andparameterestimation usedtheiterativeEMalgorithm. Inthissetofnotes,wewilldevelopamethod,PrincipalComponentsAnalysis (PCA),thatalsotriestoidentifythesubspaceinwhichthedataapproximately lies.However,PCAwilldosomoredirectly,andwillrequireonlyaneigenvector calculation(easilydonewiththeeigfunctioninMatlab),anddoesnotneedto resorttoEM. Suppose we are given a dataset x(i);i =1,...,n of attributes of n different typesofautomobiles,suchastheirmaximumspeed,turnradius,andsoon.Let x(i) \u2208Rd foreachi(d (cid:28) n).Butunknowntous,twodifferentattributes\u2014some x andx \u2014respectivelygiveacar\u2019smaximumspeedmeasuredinmilesperhour, i j andthemaximumspeedmeasuredinkilometersperhour.Thesetwoattributes arethereforealmostlinearlydependent,uptoonlysmalldifferencesintroduced byroundingofftothenearestmphorkph.Thus,thedatareallyliesapp",
      "chunk_id": 236,
      "start_pos": 226018,
      "end_pos": 227218,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "i j andthemaximumspeedmeasuredinkilometersperhour.Thesetwoattributes arethereforealmostlinearlydependent,uptoonlysmalldifferencesintroduced byroundingofftothenearestmphorkph.Thus,thedatareallyliesapproximately onann\u22121dimensionalsubspace.Howcanweautomaticallydetect,andperhaps remove,thisredundancy? Foralesscontrivedexample,consideradatasetresultingfromasurveyof (i) pilots for radio-controlled helicopters, where x is a measure of the piloting 1 (i) skill of pilot i, and x captures how much he/she enjoys flying. Because RC 2 helicoptersareverydifficulttofly,onlythemostcommittedstudents,onesthat trulyenjoyflying,becomegoodpilots.So,thetwoattributesx andx arestrongly 1 2 correlated. Indeed, we might posit that that the data actually lies along some diagonal axis (the u direction) capturing the intrinsic piloting \u2018\u2018karma\u2019\u2019 of a 1 person,withonlyasmallamountofnoiselyingoffthisaxis.(Seefigure.)How canweautomaticallycomputethisu direction? 1 WewillshortlydevelopthePCAalgorithm.ButpriortorunningPCAperse, typicallywefirstpreprocessthedatabynormalizingeachfeaturetohavemean0 143 andvariance1.Wedothisbysubtractingthemeananddividingbytheempirical standarddeviation: x (i) \u2212\u00b5 x (i) \u2190 j j j \u03c3 j whe",
      "chunk_id": 237,
      "start_pos": 227018,
      "end_pos": 228218,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "gPCAperse, typicallywefirstpreprocessthedatabynormalizingeachfeaturetohavemean0 143 andvariance1.Wedothisbysubtractingthemeananddividingbytheempirical standarddeviation: x (i) \u2212\u00b5 x (i) \u2190 j j j \u03c3 j where \u00b5 = 1 \u2211n x (i) and \u03c32 = 1 \u2211n (x (i) \u2212\u00b5 )2 are the mean variance of j n i=1 j j n i=1 j j featurej,respectively. Subtracting\u00b5 zerosoutthemeanandmaybeomittedfordataknowntohave j zeromean(forinstance,timeseriescorrespondingtospeechorotheracoustic signals).Dividingbythestandarddeviation\u03c3 rescaleseachcoordinatetohave j unitvariance,whichensuresthatdifferentattributesarealltreatedonthesame \u2018\u2018scale.\u2019\u2019Forinstance,ifx wascars\u2019maximumspeedinmph(takingvaluesinthe 1 hightensorlowhundreds)andx werethenumberofseats(takingvaluesaround 2 2-4),thenthisrenormalizationrescalesthedifferentattributestomakethemmore comparable.Thisrescalingmaybeomittedifwehadaprioriknowledgethatthe differentattributesareallonthesamescale.Oneexampleofthisisifeachdata pointrepresentedagrayscaleimage,andeachx (i) tookavaluein{0,1,...,255} j correspondingtotheintensityvalueofpixeljinimagei.",
      "chunk_id": 238,
      "start_pos": 228018,
      "end_pos": 229080,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ethatthe differentattributesareallonthesamescale.Oneexampleofthisisifeachdata pointrepresentedagrayscaleimage,andeachx (i) tookavaluein{0,1,...,255} j correspondingtotheintensityvalueofpixeljinimagei. Now,havingnormalizedourdata,howdowecomputethe\u2018\u2018majoraxisofvari- ation\u2019\u2019u\u2014thatis,thedirectiononwhichthedataapproximatelylies?Onewayis toposethisproblemasfindingtheunitvectorusothatwhenthedataisprojected ontothedirectioncorrespondingtou,thevarianceoftheprojecteddataismaxi- mized.Intuitively,thedatastartsoffwithsomeamountofvariance/information init.Wewouldliketochooseadirectionusothatifweweretoapproximatethe dataaslyinginthedirection/subspacecorrespondingtou,asmuchaspossible ofthisvarianceisstillretained.Considerthefollowingdataset,onwhichwehave alreadycarriedoutthenormalizationsteps: Now,supposewepickutocorrespondthethedirectionshowninthefigure below.Thecirclesdenotetheprojectionsoftheoriginaldataontothisline. Weseethattheprojecteddatastillhasafairlylargevariance,andthepoints tendtobefarfromzero.Incontrast,supposehadinsteadpickedthefollowing direction: Here,theprojectionshaveasignificantlysmallervariance,andaremuchcloser totheorigin.",
      "chunk_id": 239,
      "start_pos": 228880,
      "end_pos": 230026,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "illhasafairlylargevariance,andthepoints tendtobefarfromzero.Incontrast,supposehadinsteadpickedthefollowing direction: Here,theprojectionshaveasignificantlysmallervariance,andaremuchcloser totheorigin. Wewouldliketoautomaticallyselectthedirectionucorrespondingtothefirst ofthetwofiguresshownabove.Toformalizethis,notethatgivenaunitvectoru toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 144 andapointx,thelengthoftheprojectionofxontouisgivenbyx(cid:62)u.I.e.,ifx(i) isapointinourdataset(oneofthecrossesintheplot),thenitsprojectionontou (thecorrespondingcircleinthefigure)isdistancex(cid:62)ufromtheorigin.Hence,to maximizethevarianceoftheprojections,wewouldliketochooseaunit-lengthu soastomaximize: 1 \u2211 n (x (i)(cid:62) u)2 = 1 \u2211 n u (cid:62) x (i) x (i)(cid:62) u n n i=1 i=1 (cid:32) (cid:33) = u (cid:62) 1 \u2211 n x (i) x (i)(cid:62) u n i=1 Weeasilyrecognizethatthemaximizingthissubjectto(cid:107)u(cid:107)2 =1givestheprin- cipaleigenvectorof\u03a3 = 1 \u2211n x(i)x(i)(cid:62) ,whichisjusttheempiricalcovariance n i=1 matrixofthedata(assumingithaszeromean).1 1Ifyouhaven\u2019tseenthisbefore,try Tosummarize,wehavefoundthatifwewishtofinda1-dimensionalsubspace usingthemethodofLagrangemul-",
      "chunk_id": 240,
      "start_pos": 229826,
      "end_pos": 231026,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "sjusttheempiricalcovariance n i=1 matrixofthedata(assumingithaszeromean).1 1Ifyouhaven\u2019tseenthisbefore,try Tosummarize,wehavefoundthatifwewishtofinda1-dimensionalsubspace usingthemethodofLagrangemul- tiplierstomaximizeu(cid:62)\u03a3usubject withwithtoapproximatethedata,weshouldchooseutobetheprincipaleigen- tothat u(cid:62)u = 1.Youshouldbe vectorof\u03a3.Moregenerally,ifwewishtoprojectourdataintoak-dimensional able to show that \u03a3u = \u03bbu, for subspace(k < d),weshouldchooseu ,...,u tobethetopkeigenvectorsof\u03a3. some\u03bb,whichimpliesuisaneigen- 1 k vectorof\u03a3,witheigenvalue\u03bb. Theu i \u2019snowformanew,orthogonalbasisforthedata.2 2Because\u03a3issymmetric,theui\u2019s Then,torepresentx(i) inthisbasis,weneedonlycomputethecorresponding will(oralwayscanbechosentobe) orthogonaltoeachother. vector \uf8ee u(cid:62)x(i)\uf8f9 1 \uf8efu(cid:62)x(i)\uf8fa y (i) = \uf8ef \uf8ef 2 . \uf8fa \uf8fa \u2208Rk. \uf8ef . \uf8fa . \uf8f0 \uf8fb u(cid:62)x(i) k Thus,whereasx(i) \u2208Rd,thevectory(i)nowgivesalower,k-dimensional,approx- imation/representationforx(i).PCAisthereforealsoreferredtoasadimension- alityreductionalgorithm.Thevectorsu ,...,u arecalledthefirstkprincipal 1 k componentsofthedata. Remark.",
      "chunk_id": 241,
      "start_pos": 230826,
      "end_pos": 231929,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "er,k-dimensional,approx- imation/representationforx(i).PCAisthereforealsoreferredtoasadimension- alityreductionalgorithm.Thevectorsu ,...,u arecalledthefirstkprincipal 1 k componentsofthedata. Remark. Althoughwehaveshownitformallyonlyforthecaseofk =1,using well-knownpropertiesofeigenvectorsitisstraightforwardtoshowthatofall possible orthogonal bases u ,...,u , the one that we have chosen maximizes 1 k \u2211 (cid:107)y(i)(cid:107)2.Thus,ourchoiceofabasispreservesasmuchvariabilityaspossiblein i 2 theoriginaldata. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 145 Inproblemset4,youwillseethatPCAcanalsobederivedbypickingthebasis thatminimizestheapproximationerrorarisingfromprojectingthedataontothe k-dimensionalsubspacespannedbythem. PCAhasmanyapplications;wewillcloseourdiscussionwithafewexamples. First,compression\u2014representingx(i)\u2019swithlowerdimensiony(i)\u2019s\u2014isanobvious application.Ifwereducehighdimensionaldatatok =2or3dimensions,then wecanalsoplotthey(i)\u2019stovisualizethedata.Forinstance,ifweweretoreduce ourautomobilesdatato2dimensions,thenwecanplotit(onepointinourplot wouldcorrespondtoonecartype,say)toseewhatcarsaresimilartoeachother andwhatgroupsofcarsmayclustertoge",
      "chunk_id": 242,
      "start_pos": 231729,
      "end_pos": 232929,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "data.Forinstance,ifweweretoreduce ourautomobilesdatato2dimensions,thenwecanplotit(onepointinourplot wouldcorrespondtoonecartype,say)toseewhatcarsaresimilartoeachother andwhatgroupsofcarsmayclustertogether. Anotherstandardapplicationistopreprocessadatasettoreduceitsdimension beforerunningasupervisedlearninglearningalgorithmwiththex(i)\u2019sasinputs. Apartfromcomputationalbenefits,reducingthedata\u2019sdimensioncanalsoreduce the complexity of the hypothesis class considered and help avoid overfitting (e.g.,linearclassifiersoverlowerdimensionalinputspaceswillhavesmallerVC dimension). Lastly,asinourRCpilotexample,wecanalsoviewPCAasanoisereduc- tionalgorithm.Inourexampleit,estimatestheintrinsic\u2018\u2018pilotingkarma\u2019\u2019from the noisy measures of piloting skill and enjoyment. In class, we also saw the application of this idea to face images, resulting in eigenfaces method. Here, eachpointx(i) \u2208R100\u00d7100wasa10000dimensionalvector,witheachcoordinate correspondingtoapixelintensityvalueina100\u00d7100imageofaface.UsingPCA, werepresenteachimagex(i) withamuchlowerdimensionaly(i).Indoingso,we hopethattheprincipalcomponentswefoundretaintheinteresting,systematic variationsbetweenfacesthatcapturewhatapersonreallylookslik",
      "chunk_id": 243,
      "start_pos": 232729,
      "end_pos": 233929,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "PCA, werepresenteachimagex(i) withamuchlowerdimensionaly(i).Indoingso,we hopethattheprincipalcomponentswefoundretaintheinteresting,systematic variationsbetweenfacesthatcapturewhatapersonreallylookslike,butnotthe \u2018\u2018noise\u2019\u2019intheimagesintroducedbyminorlightingvariations,slightlydifferent imagingconditions,andsoon.Wethenmeasuredistancesbetweenfacesiandj byworkinginthereduceddimension,andcomputing(cid:107)y(i)\u2212y(j)(cid:107) .Thisresulted 2 inasurprisinglygoodface-matchingandretrievalalgorithm. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 146 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc Part XI: Independent Components Analysis FromCS229Spring2021,Andrew OurnexttopicisIndependentComponentsAnalysis(ICA).SimilartoPCA, Ng,MosesCharikar&Christopher thiswillfindanewbasisinwhichtorepresentourdata.However,thegoalis R\u00e9,StanfordUniversity. verydifferent. As a motivating example, consider the \u2018\u2018cocktail party problem.\u2019\u2019 Here, d speakersarespeakingsimultaneouslyataparty,andanymicrophoneplacedin theroomrecordsonlyanoverlappingcombinationofthedspeakers\u2019voices.But letssaywehaveddifferentmicrophonesplacedintheroom,andbecauseeach microphoneisadifferen",
      "chunk_id": 244,
      "start_pos": 233729,
      "end_pos": 234929,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "multaneouslyataparty,andanymicrophoneplacedin theroomrecordsonlyanoverlappingcombinationofthedspeakers\u2019voices.But letssaywehaveddifferentmicrophonesplacedintheroom,andbecauseeach microphoneisadifferentdistancefromeachofthespeakers,itrecordsadifferent combinationofthespeakers\u2019voices.Usingthesemicrophonerecordings,canwe separateouttheoriginaldspeakers\u2019speechsignals? Toformalizethisproblem,weimaginethatthereissomedatas \u2208Rd thatis generatedviadindependentsources.Whatweobserveis x = As, where Aisanunknownsquarematrixcalledthemixingmatrix.Repeatedobser- vationsgivesusadatasetx(i);i =1,...,n,andourgoalistorecoverthesources s(i) thathadgeneratedourdata(x(i) = As(i)). Inourcocktailpartyproblem,s(i) isand-dimensionalvector,ands (i) isthe j soundthatspeakerjwasutteringattimei.Also,x(i) inand-dimensionalvector, (i) andx istheacousticreadingrecordedbymicrophonejattimei. j LetW = A\u22121betheunmixingmatrix.OurgoalistofindW,sothatgivenour microphonerecordingsx(i),wecanrecoverthesourcesbycomputings(i) =Wx(i). Fornotationalconvenience,wealsoletw(cid:62) denotethei-throwofW,sothat i \uf8ee \uf8f9 \u2014w(cid:62) \u2014 1 \uf8ef . \uf8fa W = \uf8ef . . \uf8fa \uf8f0 \uf8fb \u2014w(cid:62) \u2014 d Thus,w \u2208Rd,andthej-thsourcecanberecoveredass (i) = w(cid:62)x(i).",
      "chunk_id": 245,
      "start_pos": 234729,
      "end_pos": 235928,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "x(i). Fornotationalconvenience,wealsoletw(cid:62) denotethei-throwofW,sothat i \uf8ee \uf8f9 \u2014w(cid:62) \u2014 1 \uf8ef . \uf8fa W = \uf8ef . . \uf8fa \uf8f0 \uf8fb \u2014w(cid:62) \u2014 d Thus,w \u2208Rd,andthej-thsourcecanberecoveredass (i) = w(cid:62)x(i). i j j 148 chapter 27. ica ambiguities 27 ICA ambiguities TowhatdegreecanW = A\u22121 berecovered?Ifwehavenopriorknowledge about the sources and the mixing matrix, it is easy to see that there are some inherentambiguitiesin Athatareimpossibletorecover,givenonlythex(i)\u2019s. Specifically,letPbeanyd-by-dpermutationmatrix.Thismeansthateachrow andeachcolumnofPhasexactlyone\u2018\u20181.\u2019\u2019Herearesomeexamplesofpermutation matrices: \uf8ee \uf8f9 0 1 0 (cid:34) (cid:35) (cid:34) (cid:35) 0 1 1 0 P = \uf8ef1 0 0\uf8fa; P = ; P = . \uf8f0 \uf8fb 1 0 0 1 0 0 1 Ifzisavector,thenPzisanothervectorthatcontainsapermutedversionofz\u2019s coordinates.Givenonlythex(i)\u2019s,therewillbenowaytodistinguishbetweenW andPW.Specifically,thepermutationoftheoriginalsourcesisambiguous,which shouldbenosurprise.Fortunately,thisdoesnotmatterformostapplications. Further,thereisnowaytorecoverthecorrectscalingofthew\u2019s.Forinstance, i if Awerereplacedwith2A,andeverys(i) werereplacedwith(0.5)s(i),thenour observedx(i) = 2A\u00b7(0.5)s(i) wouldstillbethesame.Morebroadly,ifasingle colu",
      "chunk_id": 246,
      "start_pos": 235728,
      "end_pos": 236928,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "snowaytorecoverthecorrectscalingofthew\u2019s.Forinstance, i if Awerereplacedwith2A,andeverys(i) werereplacedwith(0.5)s(i),thenour observedx(i) = 2A\u00b7(0.5)s(i) wouldstillbethesame.Morebroadly,ifasingle columnof A werescaledbyafactorof \u03b1,andthecorrespondingsourcewere scaledbyafactorof1/\u03b1,thenthereisagainnowaytodeterminethatthishad happenedgivenonlythex(i)\u2019s.Thus,wecannotrecoverthe\u2018\u2018correct\u2019\u2019scalingof thesources.However,fortheapplicationsthatweareconcernedwith\u2014including the cocktail party problem\u2014this ambiguity also does not matter. Specifically, (i) scalingaspeaker\u2019sspeechsignals bysomepositivefactor\u03b1affectsonlythe j (i) volumeofthatspeaker\u2019sspeech.Also,signchangesdonotmatter,ands and j \u2212s (i) sound identical when played on a speaker. Thus, if the w found by an j i algorithmisscaledbyanynon-zerorealnumber,thecorrespondingrecovered sources = w(cid:62)xwillbescaledbythesamefactor;butthisusuallydoesnotmatter. i i (ThesecommentsalsoapplytoICAforthebrain/MEGdatathatwetalkedabout inclass.) Are these the only sources of ambiguity in ICA? It turns out that they are, so long as the sources s are non-Gaussian.",
      "chunk_id": 247,
      "start_pos": 236728,
      "end_pos": 237838,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "r. i i (ThesecommentsalsoapplytoICAforthebrain/MEGdatathatwetalkedabout inclass.) Are these the only sources of ambiguity in ICA? It turns out that they are, so long as the sources s are non-Gaussian. To see what the difficulty is with i 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc Gaussiandata,consideranexampleinwhichn =2,ands \u223c N(0,I).Here, I is the2\u00d72identitymatrix.Notethatthecontoursofthedensityofthestandard normaldistributionN(0,I)arecirclescenteredontheorigin,andthedensityis rotationallysymmetric. Now,supposeweobservesomex = As,where Aisourmixingmatrix.Then, thedistributionofxwillbeGaussian,x \u223c N(0,AA(cid:62)),since E [x] =E[As] = AE[s] =0 s\u223cN(0,I) Cov[x] =E [xx (cid:62)] =E[Ass (cid:62) A (cid:62)] = AE[ss (cid:62)]A (cid:62) = A\u00b7Cov[s]\u00b7A (cid:62) = AA (cid:62) s\u223cN(0,I) Now,letRbeanarbitraryorthogonal(lessformally,arotation/reflection)matrix, so that RR(cid:62) = R(cid:62)R = I, and let A(cid:48) = AR. Then if the data had been mixed according to A(cid:48) instead of A, we would have instead observed x(cid:48) = A(cid:48)s.",
      "chunk_id": 248,
      "start_pos": 237638,
      "end_pos": 238711,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "lection)matrix, so that RR(cid:62) = R(cid:62)R = I, and let A(cid:48) = AR. Then if the data had been mixed according to A(cid:48) instead of A, we would have instead observed x(cid:48) = A(cid:48)s. The distributionofx(cid:48) isalsoGaussian,x(cid:48) \u223c N(0,AA(cid:62)),sinceE [x(cid:48)(x(cid:48))(cid:62)] = s\u223cN(0,I) E[A(cid:48)ss(cid:62)(A(cid:48))(cid:62)] = E[ARss(cid:62)(AR)(cid:62)] = ARR(cid:62)A(cid:62) = AA(cid:62).Hence,whetherthe mixingmatrixis Aor A(cid:48),wewouldobservedatafromaN(0,AA(cid:62))distribution. Thus,thereisnowaytotellifthesourcesweremixedusingAandA(cid:48).Thereisan arbitraryrotationalcomponentinthemixingmatrixthatcannotbedetermined fromthedata,andwecannotrecovertheoriginalsources. Our argument above was based on the fact that the multivariate standard normaldistributionisrotationallysymmetric.Despitethebleakpicturethatthis paints for ICA on Gaussian data, it turns out that, so long as the data is not Gaussian,itispossible,givenenoughdata,torecoverthedindependentsources. 28 Densities and linear transformations BeforemovingontoderivetheICAalgorithmproper,wefirstdigressbriefly totalkabouttheeffectoflineartransformationsondensities.",
      "chunk_id": 249,
      "start_pos": 238511,
      "end_pos": 239685,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "hdata,torecoverthedindependentsources. 28 Densities and linear transformations BeforemovingontoderivetheICAalgorithmproper,wefirstdigressbriefly totalkabouttheeffectoflineartransformationsondensities. Supposearandomvariablesisdrawnaccordingtosomedensity p (s).For s simplicity, assume for now that s \u2208 R is a real number. Now, let the random variable xbedefinedaccordingto x = As(here, x \u2208 R,A \u2208 R).Let p bethe x densityofx.Whatis p ? x Let W = A\u22121. To calculate the \u2018\u2018probability\u2019\u2019 of a particular value of x, it is tempting to compute s = Wx, then then evaluate p at that point, and con- s clude that \u2018\u2018p (x) = p (Wx).\u2019\u2019 However, this is incorrect. For example, let s \u223c x s Uniform[0,1], so p (s) = 1{0 \u2264 s \u2264 1}. Now, let A = 2, so x = 2s. Clearly, s x is distributed uniformly in the interval [0,2]. Thus, its density is given by p (x) = (0.5)1{0\u2264 x \u22642}.Thisdoesnotequal p (Wx),whereW =0.5= A\u22121. x s Instead,thecorrectformulais p (x) = p (Wx)|W|. x s Moregenerally,ifsisavector-valueddistributionwithdensity p ,andx = As s forasquare,invertiblematrix A,thenthedensityofxisgivenby p (x) = p (Wx)\u00b7|W|, x s whereW = A\u22121. Remark.",
      "chunk_id": 250,
      "start_pos": 239485,
      "end_pos": 240613,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "is p (x) = p (Wx)|W|. x s Moregenerally,ifsisavector-valueddistributionwithdensity p ,andx = As s forasquare,invertiblematrix A,thenthedensityofxisgivenby p (x) = p (Wx)\u00b7|W|, x s whereW = A\u22121. Remark. If you\u2019re seen the result that A maps [0,1]d to a set of volume |A|, thenhere\u2019sanotherwaytoremembertheformulafor p givenabove,thatalso x generalizesourprevious1-dimensionalexample.Specifically,let A \u2208 Rd\u00d7d be given, and let W = A\u22121 as usual. Also let C = [0,1]d be the d-dimensional 1 hypercube,anddefineC = {As : s \u2208C } \u2286Rd tobetheimageofC underthe 2 1 1 mappinggivenby A.Thenitisastandardresultinlinearalgebra(and,indeed, oneofthewaysofdefiningdeterminants)thatthevolumeofC isgivenby|A|. 2 Now,supposesisuniformlydistributedin[0,1]d,soitsdensityis p (s) =1{s \u2208 s C }.ThenclearlyxwillbeuniformlydistributedinC .Itsdensityistherefore 1 2 found to be p (x) = 1{x \u2208 C }/vol(C ) (since it must integrate over C to x 2 2 2 1).Butusingthefactthatthedeterminantoftheinverseofamatrixisjustthe inverseofthedeterminant,wehave1/vol(C ) = 1/|A| = |A\u22121| = |W|.Thus, 2 p (x) =1{x \u2208C }|W| =1{Wx \u2208C }|W| = p (Wx)|W|. x 2 1 s 29 ICA algorithm We are now ready to derive an ICA algorithm.",
      "chunk_id": 251,
      "start_pos": 240413,
      "end_pos": 241585,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "atrixisjustthe inverseofthedeterminant,wehave1/vol(C ) = 1/|A| = |A\u22121| = |W|.Thus, 2 p (x) =1{x \u2208C }|W| =1{Wx \u2208C }|W| = p (Wx)|W|. x 2 1 s 29 ICA algorithm We are now ready to derive an ICA algorithm. We describe an algorithm by Bell and Sejnowski, and we give an interpretation of their algorithm as a methodformaximumlikelihoodestimation.(Thisisdifferentfromtheiroriginal interpretationinvolvingacomplicatedideacalledtheinfomaxprincipalwhichis nolongernecessarygiventhemodernunderstandingofICA.) 151 Wesupposethatthedistributionofeachsources isgivenbyadensity p ,and j s thatthejointdistributionofthesourcessisgivenby d \u220f p(s) = p (s ). s j j=1 Notethatbymodelingthejointdistributionasaproductofmarginals,wecapture theassumptionthatthesourcesareindependent.Usingourformulasfromthe previoussection,thisimpliesthefollowingdensityonx = As =W\u22121s: d p(x) = \u220f p (w (cid:62) x)\u00b7|W| s j j=1 Allthatremainsistospecifyadensityfortheindividualsources p .Recallthat, s givenareal-valuedrandomvariablez,itscumulativedistributionfunction(cdf)F isdefinedbyF(z ) = P(z \u2264 z ) = (cid:82)z0 p (z)dzandthedensityisthederivative 0 0 \u2212\u221e z ofthecdf: p (z) = F(cid:48)(z).",
      "chunk_id": 252,
      "start_pos": 241385,
      "end_pos": 242535,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "lthat, s givenareal-valuedrandomvariablez,itscumulativedistributionfunction(cdf)F isdefinedbyF(z ) = P(z \u2264 z ) = (cid:82)z0 p (z)dzandthedensityisthederivative 0 0 \u2212\u221e z ofthecdf: p (z) = F(cid:48)(z). z Thus,tospecifyadensityforthes\u2019s,allweneedtodoistospecifysomecdffor i it.Acdfhastobeamonotonicfunctionthatincreasesfromzerotoone.Following ourpreviousdiscussion,wecannotchoosetheGaussiancdf,asICAdoesn\u2019twork onGaussiandata.Whatwe\u2019llchooseinsteadasareasonable\u2018\u2018default\u2019\u2019cdfthat slowlyincreasesfrom0to1,isthesigmoidfunctiong(s) =1/(1+e\u2212s).Hence, p s (s) = g(cid:48)(s).1 1Ifyouhavepriorknowledgethat The square matrix W is the parameter in our model. Given a training set thesources\u2019densitiestakeacertain form,thenitisagoodideatosub- {x(i);i =1,...,n},theloglikelihoodisgivenby stitutethatinhere.Butintheab- senceofsuchknowledge,thesig- (cid:32) (cid:33) n d moidfunctioncanbethoughtofas (cid:96)(W) = \u2211 \u2211 logg (cid:48)(w (cid:62) j x (i))+log|W| . areasonabledefaultthatseemsto i=1 j=1 workwellformanyproblems.Also, thepresentationhereassumesthat WewouldliketomaximizethisintermsW.Bytakingderivativesandusingthe eitherthedata x(i) hasbeenpre- fact(fromthefirstsetofnotes)that\u2207 |W| = |W|(W\u22121)(cid:62),",
      "chunk_id": 253,
      "start_pos": 242335,
      "end_pos": 243535,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "yproblems.Also, thepresentationhereassumesthat WewouldliketomaximizethisintermsW.Bytakingderivativesandusingthe eitherthedata x(i) hasbeenpre- fact(fromthefirstsetofnotes)that\u2207 |W| = |W|(W\u22121)(cid:62),weeasilyderivea processed to have zero mean, or W thatitcannaturallybeexpectedto stochasticgradientascentlearningrule.Foratrainingexamplex(i),theupdate havezeromean(suchasacoustic ruleis: signals).Thisisnecessarybecause \uf8eb\uf8ee 1\u22122g(w(cid:62)x(i)) \uf8f9 \uf8f6 ourassumptionthatps (s)=g(cid:48)(s) 1 impliesE[s]=0(thederivativeof W :=W+\u03b1 \uf8ec \uf8ec \uf8ec \uf8ec \uf8ef \uf8ef \uf8ef \uf8ef 1\u22122g( . . w 2 (cid:62)x(i))\uf8fa \uf8fa \uf8fa \uf8fa x (i)(cid:62) +(W (cid:62))\u22121 \uf8f7 \uf8f7 \uf8f7 \uf8f7 , t fu he nc lo ti g o i n s , ti a c n f d un h c e t n io c n eg is iv a e s s y a m d m en e s t i r t i y c . \uf8ed\uf8f0 \uf8fb \uf8f8 corresponding to a random vari- 1\u22122g(w(cid:62)x(i)) ablewithzeromean),whichim- d pliesE[x]=E[As]=0. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 152 chapter 29. ica algorithm where\u03b1isthelearningrate. Afterthealgorithmconverges,wethencomputes(i) = Wx(i) torecoverthe originalsources. Remark.",
      "chunk_id": 254,
      "start_pos": 243335,
      "end_pos": 244386,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 152 chapter 29. ica algorithm where\u03b1isthelearningrate. Afterthealgorithmconverges,wethencomputes(i) = Wx(i) torecoverthe originalsources. Remark. Whenwritingdownthelikelihoodofthedata,weimplicitlyassumed thatthex(i)\u2019swereindependentofeachother(fordifferentvaluesofi;notethis issueisdifferentfromwhetherthedifferentcoordinatesofx(i) areindependent), sothatthelikelihoodofthetrainingsetwasgivenby\u220f p(x(i);W).Thisassump- i tionisclearlyincorrectforspeechdataandothertimeserieswherethex(i)\u2019sare dependent,butitcanbeshownthathavingcorrelatedtrainingexampleswillnot hurttheperformanceofthealgorithmifwehavesufficientdata.However,for problemswheresuccessivetrainingexamplesarecorrelated,whenimplementing stochasticgradientascent,itsometimeshelpsaccelerateconvergenceifwevisit trainingexamplesinarandomlypermutedorder(i.e.,runstochasticgradient ascentonarandomlyshuffledcopyofthetrainingset.) 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 153 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu Part XII: Reinforcement Learning and Control FromCS229Spring2021,Andrew Wenowbeginourstudyofreinforcementlearnin",
      "chunk_id": 255,
      "start_pos": 244186,
      "end_pos": 245386,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ord.edu toc 153 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu Part XII: Reinforcement Learning and Control FromCS229Spring2021,Andrew Wenowbeginourstudyofreinforcementlearningandadaptivecontrol. Ng,MosesCharikar&Christopher Insupervisedlearning,wesawalgorithmsthattriedtomaketheiroutputs R\u00e9,StanfordUniversity. mimic the labels y given in the training set. In that setting, the labels gave an unambiguous \u2018\u2018right answer\u2019\u2019 for each of the inputs x. In contrast, for many sequentialdecisionmakingandcontrolproblems,itisverydifficulttoprovide thistypeofexplicitsupervisiontoalearningalgorithm.Forexample,ifwehave justbuiltafour-leggedrobotandaretryingtoprogramittowalk,theninitially wehavenoideawhatthe\u2018\u2018correct\u2019\u2019actionstotakearetomakeitwalk,andsodo notknowhowtoprovideexplicitsupervisionforalearningalgorithmtotryto mimic. Inthereinforcementlearningframework,wewillinsteadprovideouralgo- rithmsonlyarewardfunction,whichindicatestothelearningagentwhenitis doingwell,andwhenitisdoingpoorly.Inthefour-leggedwalkingexample,the rewardfunctionmightgivetherobotpositiverewardsformovingforwards,and negativerewardsforeithermovingbackwardsorfallingover.Itwillthenbethe learningalgorith",
      "chunk_id": 256,
      "start_pos": 245186,
      "end_pos": 246386,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "doingpoorly.Inthefour-leggedwalkingexample,the rewardfunctionmightgivetherobotpositiverewardsformovingforwards,and negativerewardsforeithermovingbackwardsorfallingover.Itwillthenbethe learningalgorithm\u2019sjobtofigureouthowtochooseactionsovertimesoasto obtainlargerewards. Reinforcementlearninghasbeensuccessfulinapplicationsasdiverseasau- tonomoushelicopterflight,robotleggedlocomotion,cell-phonenetworkrouting, marketingstrategyselection,factorycontrol,andefficientweb-pageindexing. OurstudyofreinforcementlearningwillbeginwithadefinitionoftheMarkov decisionprocesses(MDP),whichprovidestheformalisminwhichRLproblems areusuallyposed. 30 Markov decision processes AMarkovdecisionprocessisatuple(cid:104)S,A,{P },\u03b3,R(cid:105),where: sa \u2022 Sisasetofstates.(Forexample,inautonomoushelicopterflight,Smightbe thesetofallpossiblepositionsandorientationsofthehelicopter.) \u2022 Aisasetofactions.(Forexample,thesetofallpossibledirectionsinwhich youcanpushthehelicopter\u2019scontrolsticks.) \u2022 P arethestatetransitionprobabilities.Foreachstates \u2208 Sandactiona \u2208 A, sa P isadistributionoverthestatespace.We\u2019llsaymoreaboutthislater,but sa briefly,P givesthedistributionoverwhatstateswewilltransitiontoifwe sa takeactionainsta",
      "chunk_id": 257,
      "start_pos": 246186,
      "end_pos": 247386,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "bilities.Foreachstates \u2208 Sandactiona \u2208 A, sa P isadistributionoverthestatespace.We\u2019llsaymoreaboutthislater,but sa briefly,P givesthedistributionoverwhatstateswewilltransitiontoifwe sa takeactionainstates. \u2022 \u03b3 \u2208 [0,1)iscalledthediscountfactor. \u2022 R : S\u00d7A (cid:55)\u2192Ristherewardfunction.(Rewardsaresometimesalsowritten asafunctionofastateSonly,inwhichcasewewouldhaveR : S (cid:55)\u2192R). ThedynamicsofanMDPproceedsasfollows:Westartinsomestates ,and 0 gettochoosesomeactiona \u2208 AtotakeintheMDP.Asaresultofourchoice,the 0 stateoftheMDPrandomlytransitionstosomesuccessorstates ,drawnaccording 1 to s \u223c P .Then,wegettopickanotheraction a .Asaresultofthisaction, 1 s0a0 1 thestatetransitionsagain,nowtosomes \u223c P .Wethenpicka ,andsoon. 2 s1a1 2 Pictorially,wecanrepresentthisprocessasfollows: s \u2212 a \u21920 s \u2212 a \u21921 s \u2212 a \u21922 s \u2212 a \u21923 ... 0 1 2 3 Uponvisitingthesequenceofstatess ,s ,...withactionsa ,a ,...,ourtotal 0 1 0 1 payoffisgivenby R(s ,a )+\u03b3R(s ,a )+\u03b32R(s ,a )+\u00b7\u00b7\u00b7 . 0 0 1 1 2 2 Or,whenwearewritingrewardsasafunctionofthestatesonly,thisbecomes R(s )+\u03b3R(s )+\u03b32R(s )+\u00b7\u00b7\u00b7 . 0 1 2 156 chapter 30.",
      "chunk_id": 258,
      "start_pos": 247186,
      "end_pos": 248268,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ",...,ourtotal 0 1 0 1 payoffisgivenby R(s ,a )+\u03b3R(s ,a )+\u03b32R(s ,a )+\u00b7\u00b7\u00b7 . 0 0 1 1 2 2 Or,whenwearewritingrewardsasafunctionofthestatesonly,thisbecomes R(s )+\u03b3R(s )+\u03b32R(s )+\u00b7\u00b7\u00b7 . 0 1 2 156 chapter 30. markov decision processes Formostofourdevelopment,wewillusethesimplerstate-rewardsR(s),though thegeneralizationtostate-actionrewardsR(s,a)offersnospecialdifficulties. Our goal in reinforcement learning is to choose actions over time so as to maximizetheexpectedvalueofthetotalpayoff: (cid:104) (cid:105) E R(s )+\u03b3R(s )+\u03b32R(s )+\u00b7\u00b7\u00b7 0 1 2 Notethattherewardattimesteptisdiscountedbyafactorof\u03b3t.Thus,tomake thisexpectationlarge,wewouldliketoaccruepositiverewardsassoonaspossible (andpostponenegativerewardsaslongaspossible).Ineconomicapplications where R(\u00b7) is the amount of money made, \u03b3 also has a natural interpretation intermsoftheinterestrate(whereadollartodayisworthmorethanadollar tomorrow). Apolicyisanyfunction\u03c0 : S (cid:55)\u2192 Amappingfromthestatestotheactions.We saythatweareexecutingsomepolicy\u03c0if,wheneverweareinstates,wetake actiona = \u03c0(s).Wealsodefinethevaluefunctionforapolicy\u03c0accordingto (cid:104) (cid:105) V\u03c0(s) =E R(s )+\u03b3R(s )+\u03b32R(s )+\u00b7\u00b7\u00b7 | s = s,\u03c0 .",
      "chunk_id": 259,
      "start_pos": 248068,
      "end_pos": 249232,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "s.We saythatweareexecutingsomepolicy\u03c0if,wheneverweareinstates,wetake actiona = \u03c0(s).Wealsodefinethevaluefunctionforapolicy\u03c0accordingto (cid:104) (cid:105) V\u03c0(s) =E R(s )+\u03b3R(s )+\u03b32R(s )+\u00b7\u00b7\u00b7 | s = s,\u03c0 . 0 1 2 0 V\u03c0(s)issimplytheexpectedsumofdiscountedrewardsuponstartinginstates, andtakingactionsaccordingto\u03c0.1 1Thisnotationinwhichwecondi- Givenafixedpolicy\u03c0,itsvaluefunctionV\u03c0 satisfiestheBellmanequations: tionon\u03c0isn\u2019ttechnicallycorrect because\u03c0isn\u2019tarandomvariable, V\u03c0(s) = R(s)+\u03b3 \u2211 P (s (cid:48))V\u03c0(s (cid:48)). butthisisquitestandardinthelit- s\u03c0(s) erature. s(cid:48)\u2208S ThissaysthattheexpectedsumofdiscountedrewardsV\u03c0(s) forstartingin s consistsoftwoterms:First,theimmediaterewardR(s)thatwegetrightaway simplyforstartinginstates,andsecond,theexpectedsumoffuturediscounted rewards.Examiningthesecondterminmoredetail,weseethatthesummation termabovecanberewrittenE s(cid:48)\u223cP s\u03c0(s) [V\u03c0(s(cid:48))].Thisistheexpectedsumofdis- countedrewardsforstartinginstates(cid:48),wheres(cid:48) isdistributedaccordingP , s\u03c0(s) whichisthedistributionoverwherewewillendupaftertakingthefirstaction \u03c0(s)intheMDPfromstates.Thus,thesecondtermabovegivestheexpected sumofdiscountedrewardsobtainedafterthefirststepintheMDP",
      "chunk_id": 260,
      "start_pos": 249032,
      "end_pos": 250232,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "dingP , s\u03c0(s) whichisthedistributionoverwherewewillendupaftertakingthefirstaction \u03c0(s)intheMDPfromstates.Thus,thesecondtermabovegivestheexpected sumofdiscountedrewardsobtainedafterthefirststepintheMDP. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 157 Bellman\u2019sequationscanbeusedtoefficientlysolveforV\u03c0.Specifically,ina finite-state MDP (|S| < \u221e), we can write down one such equation for V\u03c0(s) foreverystates.Thisgivesusasetof|S|linearequationsin|S|variables(the unknown V\u03c0(s)\u2019s, one for each state), which can be efficiently solved for the V\u03c0(s)\u2019s. Wealsodefinetheoptimalvaluefunctionaccordingto V \u2217(s) =maxV\u03c0(s). (30.1) \u03c0 Inotherwords,thisisthebestpossibleexpectedsumofdiscountedrewardsthat canbeattainedusinganypolicy.ThereisalsoaversionofBellman\u2019sequations fortheoptimalvaluefunction: V \u2217(s) = R(s)+max\u03b3 \u2211 P (s (cid:48))V \u2217(s (cid:48)). (30.2) sa a\u2208A s(cid:48)\u2208S Thefirsttermaboveistheimmediaterewardasbefore.Thesecondtermisthe maximumoverallactionsaoftheexpectedfuturesumofdiscountedrewards we\u2019llgetuponafteractiona.Youshouldmakesureyouunderstandthisequation and see why it makes sense.",
      "chunk_id": 261,
      "start_pos": 250032,
      "end_pos": 251149,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "mediaterewardasbefore.Thesecondtermisthe maximumoverallactionsaoftheexpectedfuturesumofdiscountedrewards we\u2019llgetuponafteractiona.Youshouldmakesureyouunderstandthisequation and see why it makes sense. (A derivation for equation (30.2) and the equa- tion(30.3)belowaregiveninchapter35)Wealsodefineapolicy\u03c0\u2217 : S (cid:55)\u2192 Aas follows: \u03c0 \u2217(s) =argmax \u2211 P (s (cid:48))V \u2217(s (cid:48)). (30.3) sa a\u2208A s(cid:48)\u2208S Note that \u03c0\u2217(s) gives the action a that attains the maximum in the \u2018\u2018max\u2019\u2019 in equation(30.2). Itisafactthatforeverystatesandeverypolicy\u03c0,wehave V \u2217(s) =V\u03c0\u2217 (s) \u2265V\u03c0(s). The first equality says that the V\u03c0\u2217 , the value function for \u03c0\u2217, is equal to the optimalvaluefunctionV\u2217 foreverystates.Further,theinequalityabovesays that\u03c0\u2217\u2019svalueisatleastaslargeasthevalueofanyotherotherpolicy.Inother words,\u03c0\u2217 asdefinedinequation(30.3)istheoptimalpolicy. Notethat\u03c0\u2217 hastheinterestingpropertythatitistheoptimalpolicyforall statess.Specifically,itisnotthecasethatifwewerestartinginsomestatesthen there\u2019dbesomeoptimalpolicyforthatstate,andifwewerestartinginsomeother states(cid:48) thenthere\u2019dbesomeotherpolicythat\u2019soptimalpolicyfors(cid:48).Thesame policy\u03c0\u2217 attainsthemaximuminequation(30.1)forallstatess.Thi",
      "chunk_id": 262,
      "start_pos": 250949,
      "end_pos": 252149,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "eoptimalpolicyforthatstate,andifwewerestartinginsomeother states(cid:48) thenthere\u2019dbesomeotherpolicythat\u2019soptimalpolicyfors(cid:48).Thesame policy\u03c0\u2217 attainsthemaximuminequation(30.1)forallstatess.Thismeansthat wecanusethesamepolicy\u03c0\u2217 nomatterwhattheinitialstateofourMDPis. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 31 Value iteration and policy iteration Wenowdescribetwoefficientalgorithmsforsolvingfinite-stateMDPs.Fornow, wewillconsideronlyMDPswithfinitestateandactionspaces(|S| < \u221e,|A| < \u221e). Inthissection,wewillalsoassumethatweknowthestatetransitionprobabilities {P }andtherewardfunctionR. sa Thefirstalgorithm,valueiteration,isasfollows: Foreachstates,initializeV(s) :=0. Algorithm31.1.Valueiteration. repeat foreverystates,updatedo V(S) := R(s)+max\u03b3 \u2211 P (s (cid:48))V(s (cid:48)). (31.1) sa a\u2208A s(cid:48) endfor untilconvergence Thisalgorithmcanbethoughtofasrepeatedlytryingtoupdatetheestimated valuefunctionusingtheBellmanequation(30.2). Therearetwopossiblewaysofperformingtheupdatesintheinnerloopof thealgorithm.Inthefirst,wecanfirstcomputethenewvaluesforV(s)forevery states,andthenoverwritealltheoldvalueswiththenewvalues.Thisiscalleda synchronousupdate.Inthi",
      "chunk_id": 263,
      "start_pos": 251949,
      "end_pos": 253149,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ofperformingtheupdatesintheinnerloopof thealgorithm.Inthefirst,wecanfirstcomputethenewvaluesforV(s)forevery states,andthenoverwritealltheoldvalueswiththenewvalues.Thisiscalleda synchronousupdate.Inthiscase,thealgorithmcanbeviewedasimplementing a\u2018\u2018Bellmanbackupoperator\u2019\u2019thattakesacurrentestimateofthevaluefunction, andmapsittoanewestimate.(Seehomeworkproblemfordetails.)Alternatively, wecanalsoperformasynchronousupdates.Here,wewouldloopoverthestates (insomeorder),updatingthevaluesoneatatime. Under either synchronous or asynchronous updates, it can be shown that valueiterationwillcauseVtoconvergetoV\u2217.HavingfoundV\u2217,wecanthenuse equation(30.3)tofindtheoptimalpolicy. Apartfromvalueiteration,thereisasecondstandardalgorithmforfindingan optimalpolicyforanMDP.Thepolicyiterationalgorithmproceedsasfollows: 159 Initialize\u03c0randomly. Algorithm31.2.Policyiteration. repeat LetV :=V\u03c0. (cid:46)typicallybylinearsystemsolver foreverystates,updatedo \u03c0(s) :=argmax \u2211 P (s (cid:48))V(s (cid:48)). (31.2) sa a\u2208A s(cid:48) endfor untilconvergence Thus,theinner-looprepeatedlycomputesthevaluefunctionforthecurrent policy,andthenupdatesthepolicyusingthecurrentvaluefunction.(Thepolicy \u03c0 foundinstep(b)isalsocalledth",
      "chunk_id": 264,
      "start_pos": 252949,
      "end_pos": 254149,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "s(cid:48) endfor untilconvergence Thus,theinner-looprepeatedlycomputesthevaluefunctionforthecurrent policy,andthenupdatesthepolicyusingthecurrentvaluefunction.(Thepolicy \u03c0 foundinstep(b)isalsocalledthepolicythatisgreedywithrespecttoV.) Note that step (a) can be done via solving Bellman\u2019s equations as described earlier,whichinthecaseofafixedpolicy,isjustasetof|S|linearequationsin|S| variables. Afteratmostafinitenumberofiterationsofthisalgorithm,V willconvergeto V\u2217,and\u03c0willconvergeto\u03c0\u2217.1 1Notethatvalueiterationcannot Bothvalueiterationandpolicyiterationarestandardalgorithmsforsolving reachtheexactV\u2217inafinitenum- berofiterations,whereaspolicyit- MDPs,andthereisn\u2019tcurrentlyuniversalagreementoverwhichalgorithmis erationwithanexactlinearsystem better. For small MDPs, policy iteration is often very fast and converges with solver,can.Thisisbecausewhen veryfewiterations.However,forMDPswithlargestatespaces,solvingforV\u03c0 theactionsspaceandpolicyspace arediscreteandfinite,andoncethe explicitlywouldinvolvesolvingalargesystemoflinearequations,andcouldbe policyreachestheoptimalpolicy difficult(andnotethatonehastosolvethelinearsystemmultipletimesinpolicy inpolicyiteration,thenitwillnot changeatall.",
      "chunk_id": 265,
      "start_pos": 253949,
      "end_pos": 255149,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "involvesolvingalargesystemoflinearequations,andcouldbe policyreachestheoptimalpolicy difficult(andnotethatonehastosolvethelinearsystemmultipletimesinpolicy inpolicyiteration,thenitwillnot changeatall.Ontheotherhand, iteration).Intheseproblems,valueiterationmaybepreferred.Forthisreason, even though value iteration will inpracticevalueiterationseemstobeusedmoreoftenthanpolicyiteration.For convergetotheV\u2217,butthereisal- wayssomenon-zeroerrorinthe somemorediscussionsonthecomparisonandconnectionofvalueiterationand learnedvaluefunction. policyiteration,pleaseseechapter34. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 32 Learning a model for an MDP So far, we have discussed MDPs and algorithms for MDPs assuming that the statetransitionprobabilitiesandrewardsareknown.Inmanyrealisticproblems, wearenotgivenstatetransitionprobabilitiesandrewardsexplicitly,butmust insteadestimatethemfromdata.(Usually,S, A,and\u03b3areknown.) Forexample,supposethat,fortheinvertedpendulumproblem(seeproblem set4),wehadanumberoftrialsintheMDP,thatproceededasfollows: s (1) \u2212 a 0 ( \u2192 1) s (1) \u2212 a 1 ( \u2192 1) s (1) \u2212 a 2 ( \u2192 1) s (1) \u2212 a 3 ( \u2192 1) ...",
      "chunk_id": 266,
      "start_pos": 254949,
      "end_pos": 256098,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "rexample,supposethat,fortheinvertedpendulumproblem(seeproblem set4),wehadanumberoftrialsintheMDP,thatproceededasfollows: s (1) \u2212 a 0 ( \u2192 1) s (1) \u2212 a 1 ( \u2192 1) s (1) \u2212 a 2 ( \u2192 1) s (1) \u2212 a 3 ( \u2192 1) ... 0 1 2 3 s (2) \u2212 a 0 ( \u2192 2) s (2) \u2212 a 1 ( \u2192 2) s (2) \u2212 a 2 ( \u2192 2) s (2) \u2212 a 3 ( \u2192 2) ... 0 1 2 3 ... (j) (j) Here,s isthestatewewereattimeioftrialj,anda isthecorresponding i i actionthatwastakenfromthatstate.Inpractice,eachofthetrialsabovemight berununtiltheMDPterminates(suchasifthepolefallsoverintheinverted pendulum problem), or it might be run for some large but finite number of timesteps. Giventhis\u2018\u2018experience\u2019\u2019intheMDPconsistingofanumberoftrials,wecan then easily derive the maximum likelihood estimates for the state transition probabilities: #timeswetookactionainstatesandgottos(cid:48) P (s (cid:48)) = (32.1) sa #timeswetookactionainstates Or,iftheratioaboveis\u2018\u20180/0\u2019\u2019\u2014correspondingtothecaseofneverhavingtaken actionainstatesbefore\u2014thewemightsimplyestimateP (s(cid:48))tobe1/|S|(i.e., sa estimateP tobetheuniformdistributionoverallstates.) sa Notethat,ifwegainmoreexperience(observemoretrials)intheMDP,there isanefficientwaytoupdateourestimatedstatetransitionprobabilitiesusingthe newexper",
      "chunk_id": 267,
      "start_pos": 255898,
      "end_pos": 257098,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "stimateP tobetheuniformdistributionoverallstates.) sa Notethat,ifwegainmoreexperience(observemoretrials)intheMDP,there isanefficientwaytoupdateourestimatedstatetransitionprobabilitiesusingthe newexperience.Specifically,ifwekeeparoundthecountsforboththenumerator anddenominatortermsofequation(32.1),thenasweobservemoretrials,we cansimplykeepaccumulatingthosecounts.Computingtheratioofthesecounts thengivenourestimateofP . sa 161 Usingasimilarprocedure,ifRisunknown,wecanalsopickourestimateof theexpectedimmediaterewardR(s)instatestobetheaveragerewardobserved instates. HavinglearnedamodelfortheMDP,wecanthenuseeithervalueiterationor policyiterationtosolvetheMDPusingtheestimatedtransitionprobabilitiesand rewards.Forexample,puttingtogethermodellearningandvalueiteration,here isonepossiblealgorithmforlearninginanMDPwithunknownstatetransition probabilities: 1. Initialize\u03c0randomly. 2. Repeat: (a) Execute\u03c0intheMDPforsomenumberoftrials. (b) UsingtheaccumulatedexperienceintheMDP,updateourestimatesforP sa (andR,ifapplicable). (c) Applyvalueiterationwiththeestimatedstatetransitionprobabilitiesand rewardstogetanewestimatedvaluefunctionV. (d) Update\u03c0tobethegreedypolicywithrespecttoV.",
      "chunk_id": 268,
      "start_pos": 256898,
      "end_pos": 258078,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "teourestimatesforP sa (andR,ifapplicable). (c) Applyvalueiterationwiththeestimatedstatetransitionprobabilitiesand rewardstogetanewestimatedvaluefunctionV. (d) Update\u03c0tobethegreedypolicywithrespecttoV. Wenotethat,forthisparticularalgorithm,thereisonesimpleoptimization that can make it run much more quickly. Specifically, in the inner loop of the algorithmwhereweapplyvalueiteration,ifinsteadofinitializingvalueiteration withV =0,weinitializeitwiththesolutionfoundduringthepreviousiteration ofouralgorithm,thenthatwillprovidevalueiterationwithamuchbetterinitial startingpointandmakeitconvergemorequickly. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 33 Continuous state MDPs Sofar,we\u2019vefocusedourattentiononMDPswithafinitenumberofstates.We nowdiscussalgorithmsforMDPsthatmayhaveaninfinitenumberofstates. Forexample,foracar,wemightrepresentthestateas(x,y,\u03b8,x\u02d9,y\u02d9,\u03b8\u02d9),compris- ingitsposition (x,y);orientation \u03b8;velocityinthe x and y directions x\u02d9 and y\u02d9; andangularvelocity\u03b8\u02d9.Hence,S = R6 isaninfinitesetofstates,becausethere is an infinite number of possible positions and orientations for the car.1 Simi- 1Technically,\u03b8isanorientationand larly, the inverted pendulum you s",
      "chunk_id": 269,
      "start_pos": 257878,
      "end_pos": 259078,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\u02d9.Hence,S = R6 isaninfinitesetofstates,becausethere is an infinite number of possible positions and orientations for the car.1 Simi- 1Technically,\u03b8isanorientationand larly, the inverted pendulum you saw in PS4 has states (x,\u03b8,x\u02d9,\u03b8\u02d9), where \u03b8 is sotherangeof\u03b8isbetterwritten \u03b8 \u2208 [\u2212\u03c0,\u03c0)than\u03b8 \u2208 R;butfor theangleofthepole.And,ahelicopterflyingin3dspacehasstatesoftheform ourpurposes,thisdistinctionisnot (x,y,z,\u03c6,\u03b8,\u03c8,x\u02d9,y\u02d9,z\u02d9,\u03c6\u02d9,\u03b8\u02d9,\u03c8\u02d9), where here the roll \u03c6, pitch \u03b8, and yaw \u03c8 angles important. specifythe3dorientationofthehelicopter. Inthissection,wewillconsidersettingswherethestatespaceisS =Rd,and describewaysforsolvingsuchMDPs. 33.1 Discretization Perhapsthesimplestwaytosolveacontinuous-stateMDPistodiscretizethe statespace,andthentouseanalgorithmlikevalueiterationorpolicyiteration, asdescribedpreviously. Forexample,ifwehave2dstates(s ,s ),wecanuseagridtodiscretizethe 1 2 statespace: Here,eachgridcellrepresentsaseparatediscretestates\u00af.Wecanthenapproxi- matethecontinuous-stateMDPviaadiscrete-stateone(S\u00af,A,{P },\u03b3,R),where s\u00afa S\u00afisthesetofdiscretestates,{P }areourstatetransitionprobabilitiesoverthe s\u00afa discretestates,andsoon.Wecanthenusevalueiterationorpolicyiterationto solvefortheV\u2217(s\u00af)an",
      "chunk_id": 270,
      "start_pos": 258878,
      "end_pos": 260078,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "-stateone(S\u00af,A,{P },\u03b3,R),where s\u00afa S\u00afisthesetofdiscretestates,{P }areourstatetransitionprobabilitiesoverthe s\u00afa discretestates,andsoon.Wecanthenusevalueiterationorpolicyiterationto solvefortheV\u2217(s\u00af)and\u03c0\u2217(s\u00af)inthediscretestateMDP(S\u00af,A,{P },\u03b3,R).When s\u00afa ouractualsystemisinsomecontinuous-valuedstates \u2208 Sandweneedtopickan actiontoexecute,wecomputethecorrespondingdiscretizedstates\u00af,andexecute action\u03c0\u2217(s\u00af). Thisdiscretizationapproachcanworkwellformanyproblems.However,there aretwodownsides.First,itusesafairlynaiverepresentationforV\u2217 (and\u03c0\u2217). Specifically,itassumesthatthevaluefunctionistakesaconstantvalueovereach 33.2. value function approximation 163 ofthediscretizationintervals(i.e.,thatthevaluefunctionispiecewiseconstant ineachofthegridcells). Tobetterunderstandthelimitationsofsucharepresentation,considerasu- pervisedlearningproblemoffittingafunctiontothisdataset: Clearly,linearregressionwoulddofineonthisproblem.However,ifweinstead discretizethex-axis,andthenusearepresentationthatispiecewiseconstantin eachofthediscretizationintervals,thenourfittothedatawouldlooklikethis: This piecewise constant representation just isn\u2019t a good representation for many smooth functions.",
      "chunk_id": 271,
      "start_pos": 259878,
      "end_pos": 261060,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ationthatispiecewiseconstantin eachofthediscretizationintervals,thenourfittothedatawouldlooklikethis: This piecewise constant representation just isn\u2019t a good representation for many smooth functions. It results in little smoothing over the inputs, and no generalizationoverthedifferentgridcells.Usingthissortofrepresentation,we wouldalsoneedaveryfinediscretization(verysmallgridcells)togetagood approximation. Aseconddownsideofthisrepresentationiscalledthecurseofdimensionality. Suppose S = Rd,andwediscretizeeachofthe d dimensionsofthestateinto k values. Then the total number of discrete states we have is kd. This grows exponentiallyquicklyinthedimensionofthestatespaced,andthusdoesnot scalewelltolargeproblems.Forexample,witha10dstate,ifwediscretizeeach statevariableinto100values,wewouldhave10010=1020discretestates,which isfartoomanytorepresentevenonamoderndesktopcomputer. Asaruleofthumb,discretizationusuallyworksextremelywellfor1dand 2dproblems(andhastheadvantageofbeingsimpleandquicktoimplement).",
      "chunk_id": 272,
      "start_pos": 260860,
      "end_pos": 261867,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "discretestates,which isfartoomanytorepresentevenonamoderndesktopcomputer. Asaruleofthumb,discretizationusuallyworksextremelywellfor1dand 2dproblems(andhastheadvantageofbeingsimpleandquicktoimplement). Perhapswithalittlebitofclevernessandsomecareinchoosingthediscretization method,itoftenworkswellforproblemswithupto4dstates.Ifyou\u2019reextremely clever,andsomewhatlucky,youmayevengetittoworkforsome6dproblems. Butitveryrarelyworksforproblemsanyhigherdimensionalthanthat. 33.2 Valuefunctionapproximation Wenowdescribeanalternativemethodforfindingpoliciesincontinuous-state MDPs, in which we approximate V\u2217 directly, without resorting to discretiza- tion.Thisapproach,calledvaluefunctionapproximation,hasbeensuccessfully appliedtomanyRLproblems. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 164 chapter 33. continuous state mdps 33.2.1 Usingamodelorsimulator Todevelopavaluefunctionapproximationalgorithm,wewillassumethatwe haveamodel,orsimulator,fortheMDP.Informally,asimulatorisablack-box thattakesasinputany(continuous-valued)states andactiona ,andoutputsa t t next-states t+1 sampledaccordingtothestatetransitionprobabilitiesP stat : Thereareseveralwaysthatonecangetsuchamode",
      "chunk_id": 273,
      "start_pos": 261667,
      "end_pos": 262867,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "sablack-box thattakesasinputany(continuous-valued)states andactiona ,andoutputsa t t next-states t+1 sampledaccordingtothestatetransitionprobabilitiesP stat : Thereareseveralwaysthatonecangetsuchamodel.Oneistousephysics simulation.Forexample,thesimulatorfortheinvertedpenduluminPS4was obtainedbyusingthelawsofphysicstocalculatewhatpositionandorientation the cart/pole will be in at time t+1, given the current state at time t and the actionataken,assumingthatweknowalltheparametersofthesystemsuchas thelengthofthepole,themassofthepole,andsoon.Alternatively,onecanalso useanoff-the-shelfphysicssimulationsoftwarepackagewhichtakesasinputa completephysicaldescriptionofamechanicalsystem,thecurrentstates and t actiona t ,andcomputesthestates t+1 ofthesystemasmallfractionofasecond intothefuture.2 2OpenDynamicsEngine(http:// AnalternativewaytogetamodelistolearnonefromdatacollectedintheMDP. www.ode.com)isoneexampleof afree/open-sourcephysicssimula- Forexample,supposeweexecutentrialsinwhichwerepeatedlytakeactionsin torthatcanbeusedtosimulatesys- anMDP,eachtrialforTtimesteps.Thiscanbedonepickingactionsatrandom, temsliketheinvertedpendulum, andthathasbeenareasonablypop- executingsomespecificpolicy,or",
      "chunk_id": 274,
      "start_pos": 262667,
      "end_pos": 263867,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "takeactionsin torthatcanbeusedtosimulatesys- anMDP,eachtrialforTtimesteps.Thiscanbedonepickingactionsatrandom, temsliketheinvertedpendulum, andthathasbeenareasonablypop- executingsomespecificpolicy,orviasomeotherwayofchoosingactions.We ularchoiceamongRLresearchers. wouldthenobservenstatesequenceslikethefollowing: s (1) \u2212 a 0 ( \u2192 1) s (1) \u2212 a 1 ( \u2192 1) s (1) \u2212 a 2 ( \u2192 1) s (1) \u2212 a 3 ( \u2192 1) \u2212 a ( T\u2192 1 \u2212 ) 1 s (1) 0 1 2 3 T s (2) \u2212 a 0 ( \u2192 2) s (2) \u2212 a 1 ( \u2192 2) s (2) \u2212 a 2 ( \u2192 2) s (2) \u2212 a 3 ( \u2192 2) \u2212 a ( T\u2192 2 \u2212 ) 1 s (2) 0 1 2 3 T ... s (n) \u2212 a 0 ( \u2192 n) s (n) \u2212 a 1 ( \u2192 n) s (n) \u2212 a 2 ( \u2192 n) s (n) \u2212 a 3 ( \u2192 n) \u2212 a ( T\u2192 n \u2212 ) 1 s (n) 0 1 2 3 T Wecanthenapplyalearningalgorithmtopredicts t+1 asafunctionofs t anda t . Forexample,onemaychoosetolearnalinearmodeloftheform s t+1 = As t +Ba t , (33.1) usinganalgorithmsimilartolinearregression.Here,theparametersofthemodel arethematricesAandB,andwecanestimatethemusingthedatacollectedfrom 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 33.2. value function approximation 165 ourntrials,bypicking n T\u22121 (cid:16) (cid:17) argmin \u2211 \u2211 (cid:107)s (i) \u2212 As (i) +Ba (i) (cid:107)2.",
      "chunk_id": 275,
      "start_pos": 263667,
      "end_pos": 264813,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "00:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 33.2. value function approximation 165 ourntrials,bypicking n T\u22121 (cid:16) (cid:17) argmin \u2211 \u2211 (cid:107)s (i) \u2212 As (i) +Ba (i) (cid:107)2. t+1 t t 2 A,B i=1t=0 Wecouldalsopotentiallyuseotherlossfunctionsforlearningthemodel.For example,ithasbeenfoundinrecentwork[?]thatusing(cid:107)\u00b7(cid:107) norm(withoutthe 2 square)maybehelpfulincertaincases. Having learned A and B, one option is to build a deterministic model, in whichgivenaninputs t anda t ,theoutputs t+1 isexactlydetermined.Specifically, wealwayscomputes t+1 accordingtoequation(33.1).Alternatively,wemayalso buildastochasticmodel,inwhich s t+1 isarandomfunctionoftheinputs,by modelingitas s t+1 = As t +Ba t +(cid:101) t , wherehere(cid:101) isanoiseterm,usuallymodeledas(cid:101) \u223c N(0,\u03a3).(Thecovariance t t matrix\u03a3canalsobeestimatedfromdatainastraightforwardway.) Here,we\u2019vewrittenthenext-states t+1 asalinearfunctionofthecurrentstate andaction;butofcourse,non-linearfunctionsarealsopossible.Specifically,one canlearnamodels t+1 = A\u03c6 s (s t )+B\u03c6 a (a t ),where\u03c6 s and\u03c6 a aresomenon-linear featuremappingsofthestatesandactions.Alternatively,onecanalsousenon- linearlearningalgo",
      "chunk_id": 276,
      "start_pos": 264613,
      "end_pos": 265813,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "arealsopossible.Specifically,one canlearnamodels t+1 = A\u03c6 s (s t )+B\u03c6 a (a t ),where\u03c6 s and\u03c6 a aresomenon-linear featuremappingsofthestatesandactions.Alternatively,onecanalsousenon- linearlearningalgorithms,suchaslocallyweightedlinearregression,tolearn toestimates t+1 asafunctionofs t anda t .Theseapproachescanalsobeusedto buildeitherdeterministicorstochasticsimulatorsofanMDP. 33.2.2 Fittedvalueiteration Wenowdescribethefittedvalueiterationalgorithmforapproximatingthevalue functionofacontinuousstateMDP.Inthesequel,wewillassumethattheproblem hasacontinuousstatespace S = Rd,butthattheactionspace A issmalland discrete.3Recallthatinvalueiteration,wewouldliketoperformtheupdate 3In practice, most MDPs have (cid:90) muchsmalleractionspacesthan V(s) := R(s)+\u03b3max P sa (s (cid:48))V(s (cid:48))ds (cid:48) (33.2) state spaces. E.g., a car has a 6d a s(cid:48) statespace,anda2dactionspace = R(s)+\u03b3m a axE s(cid:48)\u223cPsa [V(s (cid:48))] (33.3) ( th st e ee in ri v n e g rte a d nd pe v n e d lo u c l i u ty m c h o a n s tr a ol 4 s d ); (In chapter 31, we had written the value iteration update with a summation statespace,anda1dactionspace; ahelicopterhasa12dstatespace, V(s) := R(s)+\u03b3max a \u2211 s(ci",
      "chunk_id": 277,
      "start_pos": 265613,
      "end_pos": 266813,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "i u ty m c h o a n s tr a ol 4 s d ); (In chapter 31, we had written the value iteration update with a summation statespace,anda1dactionspace; ahelicopterhasa12dstatespace, V(s) := R(s)+\u03b3max a \u2211 s(cid:48) P sa (s(cid:48))V(s(cid:48)) rather than an integral over states; the anda4dactionspace.So,discretiz- newnotationreflectsthatwearenowworkingincontinuousstatesratherthan ingthissetofactionsisusuallyless ofaproblemthandiscretizingthe discretestates.) statespacewouldhavebeen. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 166 chapter 33. continuous state mdps Themainideaoffittedvalueiterationisthatwearegoingtoapproximately carry out this step, over a finite sample of states s(1),...,s(n). Specifically, we willuseasupervisedlearningalgorithm\u2014linearregressioninourdescription below\u2014toapproximatethevaluefunctionasalinearornon-linearfunctionof thestates: V(s) = \u03b8 (cid:62) \u03c6(s). Here,\u03c6issomeappropriatefeaturemappingofthestates. Foreachstatesinourfinitesampleofnstates,fittedvalueiterationwillfirst computeaquantityy(i),whichwillbeourapproximationtoR(s)+\u03b3max a E s(cid:48)\u223cPsa [V(s(cid:48))] (therighthandsideofequation(33.3)).Then,itwillapplyasupervisedlearning algori",
      "chunk_id": 278,
      "start_pos": 266613,
      "end_pos": 267813,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "fittedvalueiterationwillfirst computeaquantityy(i),whichwillbeourapproximationtoR(s)+\u03b3max a E s(cid:48)\u223cPsa [V(s(cid:48))] (therighthandsideofequation(33.3)).Then,itwillapplyasupervisedlearning algorithm to try to get V(s) close to R(s)+\u03b3max a E s(cid:48)\u223cPsa [V(s(cid:48))] (or, in other words,totrytogetV(s)closetoy(i)). Indetail,thealgorithmisasfollows: 1. Randomlysamplenstatess(1),s(2),...,s(n) \u2208 S. 2. Initialize\u03b8 :=0. 3. Repeat: Fori =1,...,n Foreachactiona \u2208 A Samples(cid:48),...,s(cid:48) \u223c P (usingamodeloftheMDP). 1 k s(i)a Setq(a) = 1\u2211k R(s(i))+\u03b3V(s(cid:48)) k j=1 j //Hence,q(a)isanestimateofR(s(i))+\u03b3E s(cid:48)\u223cP s(i)a [V(s(cid:48))]. Sety(i) =max q(a). a //Hence,y(i) isanestimateofR(s(i))+\u03b3max a E s(cid:48)\u223cP s(i)a [V(s(cid:48))]. //Intheoriginalvalueiterationalgorithm(overdiscretestates) //weupdatedthevaluefunctionaccordingtoV(s(i)) := y(i). //Inthisalgorithm,wewantV(s(i)) \u2248 y(i),whichwe\u2019llachieve //usingsupervisedlearning(linearregression). (cid:16) (cid:17)2 Set\u03b8 :=argmin 1\u2211n \u03b8(cid:62)\u03c6(s(i))\u2212y(i) \u03b8 2 i=1 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 33.2.",
      "chunk_id": 279,
      "start_pos": 267613,
      "end_pos": 268723,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\u2019llachieve //usingsupervisedlearning(linearregression). (cid:16) (cid:17)2 Set\u03b8 :=argmin 1\u2211n \u03b8(cid:62)\u03c6(s(i))\u2212y(i) \u03b8 2 i=1 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 33.2. value function approximation 167 Above, we had written out fitted value iteration using linear regression as thealgorithmtotrytomakeV(s(i)) closeto y(i).Thatstepofthealgorithmis completelyanalogoustoastandardsupervisedlearning(regression)problemin whichwehaveatrainingset(x(1),y(1)),(x(2),y(2)),...,(x(n),y(n)),andwantto learnafunctionmappingfromxtoy;theonlydifferenceisthatheresplaysthe roleofx.Eventhoughourdescriptionaboveusedlinearregression,clearlyother regressionalgorithms(suchaslocallyweightedlinearregression)canalsobe used. Unlikevalueiterationoveradiscretesetofstates,fittedvalueiterationcannot beprovedtoalwaystoconverge.However,inpractice,itoftendoesconverge(or approximatelyconverge),andworkswellformanyproblems.Notealsothatifwe areusingadeterministicsimulator/modeloftheMDP,thenfittedvalueiteration canbesimplifiedbysettingk =1inthealgorithm.Thisisbecausetheexpectation inequation(33.3)becomesanexpectationoveradeterministicdistribution,and soasingleexampleissufficienttoexactlycomput",
      "chunk_id": 280,
      "start_pos": 268523,
      "end_pos": 269723,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "valueiteration canbesimplifiedbysettingk =1inthealgorithm.Thisisbecausetheexpectation inequation(33.3)becomesanexpectationoveradeterministicdistribution,and soasingleexampleissufficienttoexactlycomputethatexpectation.Otherwise,in thealgorithmabove,wehadtodrawksamples,andaveragetotrytoapproximate thatexpectation(seethedefinitionofq(a),inthealgorithmpseudo-code). Finally,fittedvalueiterationoutputsV,whichisanapproximationtoV\u2217.This implicitlydefinesourpolicy.Specifically,whenoursystemisinsomestates,and weneedtochooseanaction,wewouldliketochoosetheaction argmaxE s(cid:48)\u223cPsa [V(s (cid:48))] (33.4) a Theprocessforcomputing/approximatingthisissimilartotheinner-loopoffitted valueiteration,whereforeachaction,wesamples(cid:48),...,s(cid:48) \u223c P toapproximate 1 k sa theexpectation.(Andagain,ifthesimulatorisdeterministic,wecansetk =1.) Inpractice,thereareoftenotherwaystoapproximatethisstepaswell.For example,oneverycommoncaseisifthesimulatorisoftheforms t+1 = f(s t ,a t )+ (cid:101) ,where f issomedeterministicfunctionofthestates(suchas f(s ,a ) = As + t t t t Ba ),and(cid:101)iszero-meanGaussiannoise.Inthiscase,wecanpicktheactiongiven t by argmaxV(f(s,a)).",
      "chunk_id": 281,
      "start_pos": 269523,
      "end_pos": 270687,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "(s t ,a t )+ (cid:101) ,where f issomedeterministicfunctionofthestates(suchas f(s ,a ) = As + t t t t Ba ),and(cid:101)iszero-meanGaussiannoise.Inthiscase,wecanpicktheactiongiven t by argmaxV(f(s,a)). a In other words, here we are just setting (cid:101) = 0 (i.e., ignoring the noise in the t simulator),andsettingk =1.Equivalent,thiscanbederivedfromequation(33.4) toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 168 chapter 33. continuous state mdps usingtheapproximation E s(cid:48) [V(s (cid:48))] \u2248V(E s(cid:48) [s (cid:48)]) (33.5) =V(f(s,a)), (33.6) whereheretheexpectationisovertherandoms(cid:48) \u223c P .Solongasthenoiseterms sa (cid:101) aresmall,thiswillusuallybeareasonableapproximation. t However,forproblemsthatdon\u2019tlendthemselvestosuchapproximations,hav- ingtosamplek|A|statesusingthemodel,inordertoapproximatetheexpectation above,canbecomputationallyexpensive. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 34 Connections between Policy and Value Iteration (Op- tional) Inthepolicyiteration,line3ofalgorithm31.2,wetypicallyuselinearsystem solvertocomputeV\u03c0.Alternatively,onecanalsotheiterativeBellmanupdates, similarlytothevalueiteration,",
      "chunk_id": 282,
      "start_pos": 270487,
      "end_pos": 271687,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "nd Value Iteration (Op- tional) Inthepolicyiteration,line3ofalgorithm31.2,wetypicallyuselinearsystem solvertocomputeV\u03c0.Alternatively,onecanalsotheiterativeBellmanupdates, similarlytothevalueiteration,toevaluateV\u03c0,asintheProcedureVE(\u00b7)inline1 ofalgorithm34.1below.Hereifwetakeoption1inline2oftheProcedureVE,then thedifferencebetweentheProcedureVEfromthevalueiteration(algorithm31.1) isthatonline4,theprocedureisusingtheactionfrom\u03c0insteadofthegreedy action. UsingtheProcedureVE,wecanbuildalgorithm34.1,whichisavariantof policyiterationthatservesanintermediatealgorithmthatconnectspolicyitera- tionandvalueiteration.Herewearegoingtouseoption2inVEtomaximizethe re-useofknowledgelearnedbefore.Onecanverifyindeedthatifwetakek =1 anduseoption2inline2inalgorithm34.1,thenalgorithm34.1issemantically equivalenttovalueiteration(algorithm31.2).Inotherwords,bothalgorithm34.1 andvalueiterationinterleavetheupdatesinequation(34.2)andequation(34.1). algorithm34.1alternatebetweenkstepsofupdateequation(34.1)andonestepof equation(34.2),whereasvalueiterationalternatesbetween1stepofupdateequa- tion(34.1)andonestepofequation(34.2).Thereforegenerallyalgorithm34.1 shouldnotbefasterthanvalueiteration,becauseassumingth",
      "chunk_id": 283,
      "start_pos": 271487,
      "end_pos": 272687,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "tepof equation(34.2),whereasvalueiterationalternatesbetween1stepofupdateequa- tion(34.1)andonestepofequation(34.2).Thereforegenerallyalgorithm34.1 shouldnotbefasterthanvalueiteration,becauseassumingthatupdateequa- tion(34.1)andequation(34.2)areequallyusefulandtime-consuming,thenthe optimalbalanceoftheupdatefrequenciescouldbejustk =1ork \u22481. On the other hand, if k steps of update equation (34.1) can be done much fasterthanktimesasinglestepofequation(34.1),thentakingadditionalstepsof equationequation(34.1)ingroupmightbeuseful.Thisiswhatpolicyiterationis leveraging\u2014thelinearsystemsolvercangiveustheresultofProcedureVEwith k = \u221emuchfasterthanusingtheProcedureVEforalargek.Ontheflipside, whensuchaspeeding-upeffectnolongerexists,e.g.,whenthestatespaceislarge andlinearsystemsolverisalsonotfast,thenvalueiterationismorepreferable. 170 chapter 34. connections between policy and value iteration (optional) functionVE(\u03c0,k) (cid:46)toevaluateV\u03c0 Algorithm34.1.Variantofpolicy iteration. Option1:InitializeV(s) :=0 Option2:InitializefromthecurrentV inthemainalgorithm. fori =0tok\u22121do foreverystates,updatedo V(s) := R(s)+\u03b3 \u2211 P (s (cid:48))V(s (cid:48)).",
      "chunk_id": 284,
      "start_pos": 272487,
      "end_pos": 273636,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "thm34.1.Variantofpolicy iteration. Option1:InitializeV(s) :=0 Option2:InitializefromthecurrentV inthemainalgorithm. fori =0tok\u22121do foreverystates,updatedo V(s) := R(s)+\u03b3 \u2211 P (s (cid:48))V(s (cid:48)). (34.1) s\u03c0(s) s(cid:48) endfor endfor returnV Require:hyperparameterk. Initialize\u03c0randomly. repeat LetV =VE(\u03c0,k). foreverystates,updatedo \u03c0(s) :=argmax \u2211 P (s (cid:48))V(s (cid:48)). (34.2) s\u03c0(s) a\u2208A s(cid:48) endfor untilconvergence 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc 35 Derivations for Bellman Equations HerewegiveaderivationfortheBellmanEquationgiveninchapter30.Recall thatthevaluefunctionforapolicy\u03c0isdefinedas (cid:104) (cid:105) V\u03c0(s) =E R(s )+\u03b3R(s )+\u03b32R(s )+\u00b7\u00b7\u00b7 | s = s,\u03c0 . 0 1 2 0 Therefore,wehave (cid:104) (cid:105) V\u03c0(s) =E R(s )+\u03b3R(s )+\u03b32R(s )+\u00b7\u00b7\u00b7 | s = s,\u03c0 0 1 2 0 = R(s)+\u03b3E[R(s )+\u03b3R(s )+\u00b7\u00b7\u00b7] 1 2 = R(s)+\u03b3E s1 \u223cP s\u03c0(s) [R(s 1 )+\u03b3R(s 2 )+\u00b7\u00b7\u00b7] = R(s)+\u03b3E s1 \u223cP s\u03c0(s) [V\u03c0(s 1 )]. NowwederivetheBellmanEquationfortheoptimalvaluefunction. V \u2217(s) =maxV\u03c0(s) \u03c0 (cid:32) (cid:33) =max R(s)+\u03b3 \u2211 P (s (cid:48))V\u03c0(s (cid:48)) s\u03c0(s) \u03c0 s(cid:48)\u2208S (cid:32) (cid:33) = R(s)+max \u03b3 \u2211 P (s (cid:48))V\u03c0(s (cid:48)) s\u03c0(s) \u03c0 s(cid:48)\u2208S = R(s)+max\u03b3 \u2211 P (s (cid:48))maxV\u03c0(s",
      "chunk_id": 285,
      "start_pos": 273436,
      "end_pos": 274636,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\u03c0 (cid:32) (cid:33) =max R(s)+\u03b3 \u2211 P (s (cid:48))V\u03c0(s (cid:48)) s\u03c0(s) \u03c0 s(cid:48)\u2208S (cid:32) (cid:33) = R(s)+max \u03b3 \u2211 P (s (cid:48))V\u03c0(s (cid:48)) s\u03c0(s) \u03c0 s(cid:48)\u2208S = R(s)+max\u03b3 \u2211 P (s (cid:48))maxV\u03c0(s (cid:48)) sa a \u03c0 s(cid:48)\u2208S = R(s)+max\u03b3 \u2211 P (s (cid:48))V \u2217(s (cid:48)). sa a\u2208A s(cid:48)\u2208S HerethefourthequalityisbecausethatforMDP,theoptimalactionatalater stateisindependentofactionsatpreviousstates,hencetheoptimalpolicyatthe currentstatecanbedecomposedtoanactionfollowedbytheoptimalpolicyat thenewstate. A Lagrange Multipliers FromCS229Spring2021,Andrew WeconsideraspecialcaseofLagrangeMultipliersforconstrainedoptimization. Ng,MosesCharikar&Christopher Theclassquicklysketchedthe\u2018\u2018geometric\u2019\u2019intuitionforLagrangemultipliers, R\u00e9,StanfordUniversity. andthisnoteconsidersashortalgebraicderivation. Inordertominimizeormaximizeafunctionwithlinearconstraints,wecon- siderfindingthecriticalpoints(whichmaybelocalmaxima,localminima,or saddlepoints)of f(x)subjectto Ax = b Here f : Rd (cid:55)\u2192 R is a convex (or concave) function, x \u2208 Rd, A \u2208 Rn\u00d7d, and b \u2208Rn.Tofindthecriticalpoints,wecannotjustsetthederivativeoftheobjective equalto0.1Thetechniqueweconsideristoturntheproblemfromaconstrained 1Seethee",
      "chunk_id": 286,
      "start_pos": 274436,
      "end_pos": 275636,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "convex (or concave) function, x \u2208 Rd, A \u2208 Rn\u00d7d, and b \u2208Rn.Tofindthecriticalpoints,wecannotjustsetthederivativeoftheobjective equalto0.1Thetechniqueweconsideristoturntheproblemfromaconstrained 1Seetheexampleattheendofthis problemintoanunconstrainedproblemusingtheLagrangian, chapter. L(x,\u00b5) = f(x)+\u00b5 (cid:62)(Ax\u2212b)inwhich\u00b5 \u2208Rn We\u2019llshowthatthecriticalpointsoftheconstrainedfunction f arecriticalpoints ofL(x,\u00b5). FindingtheSpaceofSolutions. Assumetheconstraintsaresatisfiable,thenlet x besuchthatAx = b.Letrank(A) =r,thenlet{u ,...,u }beanorthonormal 0 0 1 k basisforthenullspaceof Ainwhichk = d\u2212r.Noteifk =0,thenx isuniquely 0 defined.Soweconsiderk >0.Wewritethisbasisasamatrix: U = [u ,...,u ] \u2208Rd\u00d7k 1 k Since U is a basis, any solution for f(x) can be written as x = x +Uy. This 0 capturesallthefreeparametersofthesolution.Thus,weconsiderthefunction: g(y) = f(x +Uy)inwhichg :Rk (cid:55)\u2192R 0 Thecriticalpointsofgarecriticalpointsof f.Noticethatgisunconstrained,so wecanusestandardcalculustofinditscriticalpoints. \u2207 g(y) =0equivalentlyU (cid:62)\u2207f(x +Uy) =0. y 0 173 Tomakesurethetypesareclear:\u2207 g(y) \u2208Rk,\u2207f(z) \u2208Rd andU \u2208Rd\u00d7k.In y bothcases,0isthe0vectorinRk.",
      "chunk_id": 287,
      "start_pos": 275436,
      "end_pos": 276595,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ed,so wecanusestandardcalculustofinditscriticalpoints. \u2207 g(y) =0equivalentlyU (cid:62)\u2207f(x +Uy) =0. y 0 173 Tomakesurethetypesareclear:\u2207 g(y) \u2208Rk,\u2207f(z) \u2208Rd andU \u2208Rd\u00d7k.In y bothcases,0isthe0vectorinRk. Theaboveconditionsaysthatif y isacriticalpointfor g,then \u2207f(x) must beorthogonaltoU.However,U formsabasisforthenullspaceof A andthe rowspaceisorthogonaltoit.Inparticular,anyelementoftherowspacecanbe writtenz = A(cid:62)\u00b5 \u2208Rd.Weverifythatzandu =Uyareorthogonalsince: z (cid:62) u = \u00b5 (cid:62) Au = \u00b5 (cid:62) 0=0 SincewecandecomposeRdasadirectsumofnull(A)andtherowspaceofA,we knowthatanyvectororthogonaltoUmustbeintherowspace.Wecanrewrite thisorthogonalityconditionasfollows:thereissome\u00b5 \u2208Rn (dependingonx) suchthat \u2207f(x)+A (cid:62) \u00b5 =0 foracertainxsuchthat Ax = A(x +Uy) = Ax = b. 0 0 TheCleverLagrangian. WenowobservethatthecriticalpointsoftheLagrangian are(bydifferentiatingandsettingto0) \u2207 L(x,\u00b5) = \u2207f(x)+A (cid:62) \u00b5 =0 x and \u2207 L(x,\u00b5) = Ax\u2212b =0 \u00b5 Thefirstconditionisexactlytheconditionthatxbeacriticalpointintheway wederiveditabove,andthesecondconditionsaysthattheconstraintbesatisfied. Thus,ifxisacriticalpoint,thereexistssome\u00b5asabove,and(x,\u00b5)isacritical pointforL.",
      "chunk_id": 288,
      "start_pos": 276395,
      "end_pos": 277567,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "exactlytheconditionthatxbeacriticalpointintheway wederiveditabove,andthesecondconditionsaysthattheconstraintbesatisfied. Thus,ifxisacriticalpoint,thereexistssome\u00b5asabove,and(x,\u00b5)isacritical pointforL. Generalizing to Nonlinear Equality Constraints. Lagrange multipliers are a much more general technique. If you want to handle non-linear equality con- straints,thenyouwillneedalittleextramachinery:theimplicitfunctiontheorem. However,thekeyideaisthatyoufindthespaceofsolutionsandyouoptimize. Inthatcase,findingthecriticalpointsof f(x)s.t.g(x) = cleadstoL(x,\u00b5) = f(x)+\u00b5 (cid:62)(g(x)\u2212c). toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 174 appendix a. lagrange multipliers Thegradientconditionhereis\u2207f(x)+J(cid:62)\u00b5 =0,whereJistheJacobianmatrixof g.Forthecasewherewehaveasingleconstraint,thegradientconditionreduces to \u2207f(x) = \u2212\u00b5 \u2207 (x), which we can view as saying, \u2018\u2018at a critical point, the 1 g1 gradientofthesurfacemustbeparalleltothegradientofthefunction.\u2019\u2019Thisconnects usbacktothepicturethatwedrewduringlecture. Wegiveasimpleexampletoshowthatyoucannotjustsetthederivativesto ExampleA.1.Needforconstrained optimization.",
      "chunk_id": 289,
      "start_pos": 277367,
      "end_pos": 278513,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "eparalleltothegradientofthefunction.\u2019\u2019Thisconnects usbacktothepicturethatwedrewduringlecture. Wegiveasimpleexampletoshowthatyoucannotjustsetthederivativesto ExampleA.1.Needforconstrained optimization. 0.Consider f(x ,x ) = x andg(x ,x ) = x2+x2andso: 1 2 1 1 2 1 2 max f(x)subjecttog(x) =1. x Thisisjustalinearfunctionaloverthecircle,anditiscompact,sothefunction must achieve a maximum value. Intuitively, we can see that (1,0) is the maximumpossiblevalue(andhenceacriticalpoint).Here,wehave: (cid:18) (cid:19) (cid:18) (cid:19) 1 x \u2207f(x) = and\u2207g(x) =2 1 0 x 2 Noticethat\u2207f(x)isnotzeroanywhereonthecircle\u2014it\u2019sconstant!Forx \u2208 {(1,0),(\u22121,0)},\u2207f(x) = \u03bb\u2207g(x)(take\u03bb \u2208 {1/2,\u22121/2},respectively).On theotherhand,foranyotherpointonthecirclex (cid:54)=0,andsothegradient 2 of f andgarenotparallel.Thus,suchpointsarenotcriticalpoints. 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc B Boosting From CS229 Spring 2021, John B.1 Boosting Duchi,StanfordUniversity. Wehaveseensofarhowtosolveclassification(andother)problemswhenwehave adatarepresentationalreadychosen.Wenowtalkaboutaprocedure,knownas boosting,whichwasoriginallydiscoveredbyRobSchapire,andfurtherdeveloped bySchapireandYoavFre",
      "chunk_id": 290,
      "start_pos": 278313,
      "end_pos": 279513,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "classification(andother)problemswhenwehave adatarepresentationalreadychosen.Wenowtalkaboutaprocedure,knownas boosting,whichwasoriginallydiscoveredbyRobSchapire,andfurtherdeveloped bySchapireandYoavFreund,thatautomaticallychoosesfeaturerepresentations. Wetakeanoptimization-basedperspective,whichissomewhatdifferentfrom theoriginalinterpretationandjustificationofFreundandSchapire,butwhich lendsitselftoourapproachof(1)choosearepresentation,(2)choosealoss,and (3)minimizetheloss. Beforeformulatingtheproblem,wegivealittleintuitionforwhatwearegoing to do. Roughly, the idea of boosting is to take a weak learning algorithm\u2014any learning algorithm that gives a classifier that is slightly better than random\u2014 and transforms it into a strong classifier, which does much much better than random.Tobuildabitofintuitionforwhatthismeans,considerahypothetical digitrecognitionexperiment,wherewewishtodistinguish0sfrom1s,andwe receiveimageswemustclassify.Thenanaturalweaklearnermightbetotake themiddlepixeloftheimage,andifitiscolored,calltheimagea1,andifitis blank,calltheimagea0.Thisclassifiermaybefarfromperfect,butitislikely betterthanrandom.Boostingproceduresproceedbytakingacollectionofsuch weakclassifiers",
      "chunk_id": 291,
      "start_pos": 279313,
      "end_pos": 280513,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "image,andifitiscolored,calltheimagea1,andifitis blank,calltheimagea0.Thisclassifiermaybefarfromperfect,butitislikely betterthanrandom.Boostingproceduresproceedbytakingacollectionofsuch weakclassifiers,andthenreweightingtheircontributionstoformaclassifierwith muchbetteraccuracythananyindividualclassifier. Withthatinmind,letusformulatetheproblem.Ourinterpretationofboosting isasacoordinatedescentmethodinaninfinitedimensionalspace,which\u2014while itsoundscomplex\u2014isnotsobadasitseems.First,weassumewehaverawinput examplesx \u2208Rn withlabelsy \u2208 {\u22121,1},asisusualinbinaryclassification.We alsoassumewehaveaninfinitecollectionoffeaturefunctions\u03c6 :Rn (cid:55)\u2192 {\u22121,1} j andaninfinitevector\u03b8 = [\u03b8 \u03b8 \u00b7\u00b7\u00b7](cid:62),butwhichweassumealwayshasonlya 1 2 finitenumberofnon-zeroentries.Forourclassifierweuse (cid:32) \u221e (cid:33) \u2211 h (x) =sign \u03b8 \u03c6 (x) . \u03b8 j j j=1 176 appendix b. boosting Wewillabusenotation,anddefine\u03b8(cid:62)\u03c6(x) = \u2211\u221e \u03b8 \u03c6 (x). j=1 j j Inboosting,oneusuallycallsthefeatures\u03c6 weakhypotheses.Givenatrainingset j {(x(1),y(1)),...,(x(m),y(m))},wecallavector p = (p(1),...,p(m))adistribution ontheexamplesif p(i) \u22650foralliand m \u2211 p (i) =1.",
      "chunk_id": 292,
      "start_pos": 280313,
      "end_pos": 281440,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "j Inboosting,oneusuallycallsthefeatures\u03c6 weakhypotheses.Givenatrainingset j {(x(1),y(1)),...,(x(m),y(m))},wecallavector p = (p(1),...,p(m))adistribution ontheexamplesif p(i) \u22650foralliand m \u2211 p (i) =1. i=1 Thenwesaythatthereisaweaklearnerwithmargin\u03b3 >0ifforanydistribution p onthemtrainingexamplesthereexistsoneweakhypothesis\u03c6 suchthat j \u2211 m p (i)1 (cid:110) y (i) (cid:54)= \u03c6 (x (i)) (cid:111) \u2264 1 \u2212\u03b3. (B.1) j 2 i=1 That is, we assume that there is some classifier that does slightly better than randomguessingonthedataset.Theexistenceofaweaklearningalgorithmisan assumption,butthesurprisingthingisthatwecantransformanyweaklearning algorithmintoonewithperfectaccuracy. Inmoregenerality,weassumewehaveaccesstoaweaklearner,whichisan algorithmthattakesasinputadistribution(weights) ponthetrainingexamples andreturnsaclassifierdoingslightlybetterthanrandom.Wewillshowhow, givenaccesstoaweaklearningalgorithm,boostingcanreturnaclassifierwith perfectaccuracyonthetrainingdata.(Admittedly,wewouldliketheclassiferto generalizewelltounseendata,butfornow,weignorethisissue.) AlgorithmB.1.Weaklearningalgo- (i) Input:Adistribution p(1),...,p(m) andtrainingset{(x(i),y(i))} i m =1 with rithm.",
      "chunk_id": 293,
      "start_pos": 281240,
      "end_pos": 282420,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "y,wewouldliketheclassiferto generalizewelltounseendata,butfornow,weignorethisissue.) AlgorithmB.1.Weaklearningalgo- (i) Input:Adistribution p(1),...,p(m) andtrainingset{(x(i),y(i))} i m =1 with rithm. \u2211m p(i) =1and p(i) \u22650. i=1 (ii) Return:Aweakclassifier\u03c6 :Rn (cid:55)\u2192 {\u22121,1}suchthat j \u2211 m p (i)1 (cid:110) y (i) (cid:54)= \u03c6 (x (i)) (cid:111) \u2264 1 \u2212\u03b3. j 2 i=1 B.1.1 Theboostingalgorithm Roughly,boostingbeginsbyassigningeachtrainingexampleequalweightinthe dataset.Itthenreceivesaweak-hypothesisthatdoeswellaccordingtothecurrent 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc b.1. boosting 177 weightsontrainingexamples,whichitincorporatesintoitscurrentclassification model.Itthenreweightsthetrainingexamplessothatexamplesonwhichitmakes mistakes receive higher weight\u2014so that the weak learning algorithm focuses onaclassifierdoingwellonthoseexamples\u2014whileexampleswithnomistakes receivelowerweight.Thisrepeatedreweightingofthetrainingdatacoupledwith aweaklearnerdoingwellonexamplesforwhichtheclassifiercurrentlydoes poorlyyieldsclassifierswithgoodperformance.",
      "chunk_id": 294,
      "start_pos": 282220,
      "end_pos": 283303,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "mpleswithnomistakes receivelowerweight.Thisrepeatedreweightingofthetrainingdatacoupledwith aweaklearnerdoingwellonexamplesforwhichtheclassifiercurrentlydoes poorlyyieldsclassifierswithgoodperformance. Theboostingalgorithmspecificallyperformscoordinatedescentontheexponen- tiallossforclassificationproblems,wheretheobjectiveis J(\u03b8) = 1 \u2211 m exp(\u2212y (i) \u03b8 (cid:62) \u03c6(x (i))). m i=1 Wefirstshowhowtocomputetheexactformofthecoordinatedescentupdate fortherisk J(\u03b8).Coordinatedescentiteratesasfollows: (i) Chooseacoordinatej \u2208N. (ii) Update\u03b8 to j \u03b8 =argminJ(\u03b8) j \u03b8j whileleaving\u03b8 identicalforallk (cid:54)= j. k Weiteratetheaboveprocedureuntilconvergence. Inthecaseofboosting,thecoordinateupdatesarenottoochallengingtoderive becauseoftheanalyticconvenienceoftheexpfunction.Wenowshowhowto derivetheupdate.Supposewewishtoupdatecoordinatek.Define (cid:32) (cid:33) w (i) =exp \u2212y (i) \u2211 \u03b8 \u03c6 (x (i)) j j j(cid:54)=k tobeaweight,andnotethatoptimizingcoordinatekcorrespondstominimizing m \u2211 w (i) exp(\u2212y (i) \u03c6 (x (i))\u03b1) k i=1 in\u03b1 = \u03b8 .Now,define k W + := \u2211 w (i) and W \u2212 := \u2211 w (i) i:y(i)\u03c6k (x(i))=1 i:y(i)\u03c6k (x(i))=\u22121 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 178 appendix b.",
      "chunk_id": 295,
      "start_pos": 283103,
      "end_pos": 284292,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": ") \u03c6 (x (i))\u03b1) k i=1 in\u03b1 = \u03b8 .Now,define k W + := \u2211 w (i) and W \u2212 := \u2211 w (i) i:y(i)\u03c6k (x(i))=1 i:y(i)\u03c6k (x(i))=\u22121 toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 178 appendix b. boosting tobethesumsoftheweightsofexamplesthat\u03c6 classifiescorrectlyandincorrectly, k respectively.Thenfinding\u03b8 isthesameaschoosing k \u03b1 =argmin (cid:8) W + e \u2212\u03b1+W \u2212 e\u03b1(cid:9) = 1 log W+ . 2 W\u2212 \u03b1 Toseethefinalequality,takederivativesandsettheresultingequationtozero, sowehave\u2212W+e\u2212\u03b1+W\u2212e\u03b1 =0.Thatis,W\u2212e2\u03b1 =W+,or\u03b1 = 1logW+ . 2 W\u2212 What remains is to choose the particular coordinate to perform coordinate descenton.Weassumewehaveaccesstoaweak-learningalgorithmasinalgo- rithmB.1,whichatiterationttakesasinputadistributionponthetrainingsetand returnsaweakhypothesis\u03c6 satisfyingthemarginconditioninequation(B.1). t WepresentthefullboostingalgorithminalgorithmB.2.Itproceedsiniterations t =1,2,3,....Werepresentthesetofhypothesesreturnedbytheweaklearning algorithmattimetby{\u03c6 ,...,\u03c6 }. 1 t B.2 TheconvergenceofBoosting We now argue that the boosting procedure achieves 0 training error, and we alsoprovidearateofconvergencetozero.Todoso,wepresentalemmathat guaranteesprogressismade. LemmaB.1.",
      "chunk_id": 296,
      "start_pos": 284092,
      "end_pos": 285276,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "2 TheconvergenceofBoosting We now argue that the boosting procedure achieves 0 training error, and we alsoprovidearateofconvergencetozero.Todoso,wepresentalemmathat guaranteesprogressismade. LemmaB.1. Let (cid:32) (cid:33) J(\u03b8 (t)) = 1 \u2211 m exp \u2212y (i) \u2211 t \u03b8 \u03c6 (x (i)) . \u03c4 \u03c4 m i=1 \u03c4=1 Then (cid:113) J(\u03b8 (t)) \u2264 1\u22124\u03b32J(\u03b8 (t\u22121)). Astheproofofthelemmaissomewhatinvolvedandnotthecentralfocusof thesenotes\u2014thoughitisimportanttoknowone\u2019salgorithmwillconverge!\u2014we defertheprooftoappendixB.4.Letusdescribehowitguaranteesconvergenceof theboostingproceduretoaclassifierwithzerotrainingerror. Weinitializetheprocedureat\u03b8(0) =0,sothattheinitialempiricalriskJ(\u03b8(0)) = 1.Now,wenotethatforany\u03b8,themisclassificationerrorsatisfies (cid:110) (cid:111) (cid:110) (cid:111) (cid:16) (cid:17) 1 sign(\u03b8 (cid:62) \u03c6(x)) (cid:54)= y =1 y\u03b8 (cid:62) \u03c6(x) \u22640 \u2264exp \u2212y\u03b8 (cid:62) \u03c6(x) 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc b.2. the convergence of boosting 179 Foreachiterationt =1,2,...: AlgorithmB.2.Boostingalgorithm. (i) Defineweights (cid:32) (cid:33) t\u22121 w (i) =exp \u2212y (i) \u2211 \u03b8 \u03c6 (x (i)) \u03c4 \u03c4 \u03c4=1 anddistribution p(i) = w(i)/\u2211m w(j).",
      "chunk_id": 297,
      "start_pos": 285076,
      "end_pos": 286213,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "ergence of boosting 179 Foreachiterationt =1,2,...: AlgorithmB.2.Boostingalgorithm. (i) Defineweights (cid:32) (cid:33) t\u22121 w (i) =exp \u2212y (i) \u2211 \u03b8 \u03c6 (x (i)) \u03c4 \u03c4 \u03c4=1 anddistribution p(i) = w(i)/\u2211m w(j). j=1 (ii) Construct a weak hypothesis \u03c6 : Rn (cid:55)\u2192 {\u22121,1} from the distribution t p = (p(1),...,p(m))onthetrainingset. (iii) ComputeW + = \u2211 w(i) and W \u2212 = \u2211 w(i) and t i:y(i)\u03c6t (x(i))=1 t i:y(i)\u03c6t (x(i))=\u22121 set + 1 W \u03b8 = log t . t 2 W \u2212 t becauseez \u2265 1forallz \u2265 0.Thus,wehavethatthemisclassificationerrorrate hasupperbound 1 \u2211 m 1 (cid:110) sign(\u03b8 (cid:62) \u03c6(x (i))) (cid:54)= y (i) (cid:111) \u2264 J(\u03b8), m i=1 andsoifJ(\u03b8) < 1 thenthevector\u03b8makesnomistakesonthetrainingdata.After m titerationsofboosting,wefindthattheempiricalrisksatisfies J(\u03b8 (t)) \u2264 (1\u22124\u03b32)t/2J(\u03b8 (0)) = (1\u22124\u03b32)t/2. To find how many iterations are required to guarantee J(\u03b8(t)) < 1, we take m logarithmstofindthat J(\u03b8(t)) < 1 if m t 1 2logm log(1\u22124\u03b32) <log , or t > . 2 m \u2212log(1\u22124\u03b32) UsingafirstorderTaylorexpansion,thatis,thatlog(1\u22124\u03b32) \u2264 \u22124\u03b32,wesee that if the number of rounds of boosting\u2014the number of weak classifiers we use\u2014satisfies logm 2logm t > \u2265 , 2\u03b32 \u2212log(1\u22124\u03b32) then J(\u03b8(t)) < 1.",
      "chunk_id": 298,
      "start_pos": 286013,
      "end_pos": 287173,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "rstorderTaylorexpansion,thatis,thatlog(1\u22124\u03b32) \u2264 \u22124\u03b32,wesee that if the number of rounds of boosting\u2014the number of weak classifiers we use\u2014satisfies logm 2logm t > \u2265 , 2\u03b32 \u2212log(1\u22124\u03b32) then J(\u03b8(t)) < 1. m toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 180 appendix b. boosting B.3 Implementingweak-learners Oneofthemajoradvantagesofboostingalgorithmsisthattheyautomatically generatefeaturesfromrawdataforus.Moreover,becausetheweakhypotheses alwaysreturnvaluesin{\u22121,1},thereisnoneedtonormalizefeaturestohave similarscaleswhenusinglearningalgorithms,whichinpracticecanmakealarge difference.Additionally,andwhilethisisnottheoreticallywell-understood,many typesofweak-learningproceduresintroducenon-linearitiesintelligentlyintoour classifiers,whichcanyieldmuchmoreexpressivemodelsthanthesimplerlinear modelsoftheform\u03b8(cid:62)xthatwehaveseensofar. B.3.1 Decisionstumps Thereareanumberofstrategiesforweaklearners,andherewefocusonone, known as decision stumps. For concreteness in this description, let us suppose thattheinputvariablesx \u2208Rn arereal-valued.Adecisionstumpisafunction f, whichisparameterizedbyathresholdsandindexj \u22081,2,...,n,andreturns \uf8f1 \uf8f21 ifx \u2265 s \u03c6 (x) =sign(x \u2212s) = j",
      "chunk_id": 299,
      "start_pos": 286973,
      "end_pos": 288173,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "s description, let us suppose thattheinputvariablesx \u2208Rn arereal-valued.Adecisionstumpisafunction f, whichisparameterizedbyathresholdsandindexj \u22081,2,...,n,andreturns \uf8f1 \uf8f21 ifx \u2265 s \u03c6 (x) =sign(x \u2212s) = j (B.2) j,s j \uf8f3\u22121 otherwise. These classifiers are simple enough that we can fit them efficiently even to a weighteddataset,aswenowdescribe. Indeed,adecisionstumpweaklearnerproceedsasfollows.Webeginwitha distribution\u2014setofweightsp(1),...,p(m)summingto1\u2014onthetrainingset,and wewishtochooseadecisionstumpoftheformofequation(B.2)tominimize theerroronthetrainingset.Thatis,wewishtofindathresholds \u2208Randindex jsuchthat m (cid:110) (cid:111) m (cid:110) (cid:111) E(cid:99)rr(\u03c6 j ,s,p) = \u2211 p (i)1 \u03c6 j,s (x (i)) (cid:54)= y (i) = \u2211 p (i)1 y (i)(x ( j i) \u2212s) \u22640 (B.3) i=1 i=1 isminimized.Naively,thiscouldbeaninefficientcalculation,butamoreintelli- gentprocedureallowsustosolvethisprobleminroughlyO(nmlogm)time.For eachfeaturej =1,2,...,n,wesorttherawinputfeaturessothat x (i1 ) \u2265 x (i2 ) \u2265 \u00b7\u00b7\u00b7 \u2265 x (im ) . j j j 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc b.3.",
      "chunk_id": 300,
      "start_pos": 287973,
      "end_pos": 289053,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "nroughlyO(nmlogm)time.For eachfeaturej =1,2,...,n,wesorttherawinputfeaturessothat x (i1 ) \u2265 x (i2 ) \u2265 \u00b7\u00b7\u00b7 \u2265 x (im ) . j j j 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc b.3. implementing weak-learners 181 Astheonlyvaluessforwhichtheerrorofthedecisionstumpcanchangearethe (i) valuesx ,abitofcleverbook-keepingallowsustocompute j m (cid:110) (cid:111) m (cid:110) (cid:111) \u2211 p (i)1 y (i)(x (i) \u2212s) \u22640 = \u2211 p (ik )1 y (ik )(x (ik ) \u2212s) \u22640 j j i=1 k=1 efficiently by incrementally modifying the sum in sorted order, which takes timeO(m)afterwehavealreadysortedthevaluesx (i) .(Wedonotdescribethe j algorithmindetailhere,leavingthattotheinterestedreader.)Thus,performing thiscalcuationforeachoftheninputfeaturestakestotaltimeO(nmlogm),and wemaychoosetheindexjandthresholdsthatgivethebestdecisionstumpfor theerrorinequation(B.3). Oneveryimportantissuetonoteisthatbyflippingthesignofthethresholded decisionstump\u03c6 j,s ,weachieveerror1\u2212E(cid:99)rr(\u03c6 j,s ,p),thatis,theerrorof E(cid:99)rr(\u2212\u03c6 j,s ,p) =1\u2212E(cid:99)rr(\u03c6 j,s ,p).",
      "chunk_id": 301,
      "start_pos": 288853,
      "end_pos": 289895,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "on(B.3). Oneveryimportantissuetonoteisthatbyflippingthesignofthethresholded decisionstump\u03c6 j,s ,weachieveerror1\u2212E(cid:99)rr(\u03c6 j,s ,p),thatis,theerrorof E(cid:99)rr(\u2212\u03c6 j,s ,p) =1\u2212E(cid:99)rr(\u03c6 j,s ,p). (You should convince yourself that this is true.) Thus, it is important to also trackthesmallestvalueof1\u2212E(cid:99)rr(\u03c6 j,s ,p)overallthresholds,becausethismaybe smallerthanE(cid:99)rr(\u03c6 j,s ,p),whichgivesabetterweaklearner.Usingthisprocedure forourweaklearner(algorithmB.1)givesthebasic,butextremelyuseful,boosting classifier. B.3.2 Otherstrategies Thereareahugenumberofvariationsonthebasicboosteddecisionstumpsidea. First,wedonotrequirethattheinputfeaturesx bereal-valued.Someofthem j may be categorical, meaning that x \u2208 {1,2,...,k} for some k, in which case j naturaldecisionstumpsareoftheform \uf8f1 \uf8f21 ifx = l \u03c6 (x) = j j \uf8f3\u22121 otherwise. as well as variants setting \u03c6 (x) = 1 if x \u2208 C for some set C \u2282 {1,...,k} of j j categories. Anothernaturalvariationistheboosteddecisiontree,inwhichinsteadofasingle leveldecisionfortheweaklearners,weconsiderconjuctionsoffeaturesortrees ofdecisions.Googlecanhelpyoufindexamplesandinformationonthesetypes ofproblems.",
      "chunk_id": 302,
      "start_pos": 289695,
      "end_pos": 290847,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "onistheboosteddecisiontree,inwhichinsteadofasingle leveldecisionfortheweaklearners,weconsiderconjuctionsoffeaturesortrees ofdecisions.Googlecanhelpyoufindexamplesandinformationonthesetypes ofproblems. toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 182 appendix b. boosting We now give an example showing the behavior of boosting on a simple dataset.Inparticular,weconsideraproblemwithdatapointsx \u2208R2,where theoptimalclassifieris \uf8f1 \uf8f21 ifx <0.6andx <0.6 y = 1 2 (B.4) \uf8f3\u22121 otherwise. Thisisasimplenon-lineardecisionrule,butitisimpossibleforstandard linearclassifiers,suchaslogisticregression,tolearn.In??,weshowthebest decision line that logistic regression learns, where positive examples are circlesandnegativeexamplesarex\u2019s.Itisclearthatlogisticregressionisnot fittingthedataparticularlywell. Withboosteddecisionstumps,however,wecanachieveamuchbetter fitforthesimplenonlinearclassificationproblemB.4.??showstheboosted classifierswehavelearnedafterdifferentnumbersofiterationsofboosting, usingatrainingsetofsizem = 150.Fromthefigure,weseethatthefirst decisionstumpistothresholdthefeature x atthevalues \u2248 0.23,thatis, 1 \u03c6(x) =sign(x \u2212s)fors \u22480.23.",
      "chunk_id": 303,
      "start_pos": 290647,
      "end_pos": 291817,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "erdifferentnumbersofiterationsofboosting, usingatrainingsetofsizem = 150.Fromthefigure,weseethatthefirst decisionstumpistothresholdthefeature x atthevalues \u2248 0.23,thatis, 1 \u03c6(x) =sign(x \u2212s)fors \u22480.23. 1 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu toc b.4. proof of lemma B.1 183 B.4 ProofoflemmaB.1 Wenowreturntoprovetheprogresslemma.Weprovethisresultbydirectly showingtherelationshipoftheweightsattimettothoseattimet\u22121.Inparticular, wenotebyinspectionthat (cid:113) J(\u03b8 (t)) =min{W + e \u2212\u03b1+W \u2212 e\u03b1} =2 W + W \u2212 t t t t \u03b1 while (cid:32) (cid:33) J(\u03b8 (t\u22121)) = 1 \u2211 m exp \u2212y (i) t \u2211 \u22121 \u03b8 \u03c6 (x (i)) =W ++W \u2212 . m \u03c4 \u03c4 t t i=1 \u03c4=1 Weknowbytheweak-learningassumptionthat \u2211 m p (i)1 (cid:110) y (i) (cid:54)= \u03c6 (x (i)) (cid:111) \u2264 1 \u2212\u03b3, or 1 \u2211 w (i) \u2264 1 \u2212\u03b3. t 2 W ++W \u2212 2 i=1 t t i : y (i) \u03c6 (x (i)) = \u22121 t (cid:124) (cid:123)(cid:122) (cid:125) =Wt \u2212 \u2212 RewritingthisexpressionbynotingthatthesumontherightisnothingbutW , t wehave (cid:18) 1 (cid:19) 1+2\u03b3 W \u2212 \u2264 \u2212\u03b3 (W ++W \u2212), or W + \u2265 W \u2212 .",
      "chunk_id": 304,
      "start_pos": 291617,
      "end_pos": 292618,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "\u03c6 (x (i)) = \u22121 t (cid:124) (cid:123)(cid:122) (cid:125) =Wt \u2212 \u2212 RewritingthisexpressionbynotingthatthesumontherightisnothingbutW , t wehave (cid:18) 1 (cid:19) 1+2\u03b3 W \u2212 \u2264 \u2212\u03b3 (W ++W \u2212), or W + \u2265 W \u2212 . t 2 t t t 1\u22122\u03b3 t Bysubstituting\u03b1 = 1log 1+2\u03b3 intheminimumdefining J(\u03b8(t)),weobtain 2 1\u22122\u03b3 (cid:115) (cid:115) 1\u22122\u03b3 1+2\u03b3 J(\u03b8 (t)) \u2264W + +W \u2212 t 1+2\u03b3 t 1\u22122\u03b3 (cid:115) (cid:115) 1\u22122\u03b3 1+2\u03b3 =W + +W \u2212(1\u22122\u03b3+2\u03b3) t 1+2\u03b3 t 1\u22122\u03b3 (cid:115) (cid:115) (cid:115) 1\u22122\u03b3 1+2\u03b3 1\u22122\u03b3 1+2\u03b3 \u2264W + +W \u2212(1\u22122\u03b3) +2\u03b3 W + t 1+2\u03b3 t 1\u22122\u03b3 1+2\u03b3 1\u22122\u03b3 t (cid:34)(cid:115) (cid:115) (cid:35) 1\u22122\u03b3 1\u22122\u03b3 (cid:113) =W + +2\u03b3 +W \u2212 1\u22124\u03b32, t 1+2\u03b3 1+2\u03b3 t whereweusedthatW \u2212 \u2264 1\u22122\u03b3W + .Performingafewalgebraicmanipulations, t 1+2\u03b3 t weseethatthefinalexpressionisequalto (cid:112) 1\u22124\u03b32(W ++W \u2212).Thatis,J(\u03b8(t)) \u2264 t t (cid:112) 1\u22124\u03b32J(\u03b8(t\u22121)). toc 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu 184 index References 1. D.M.Blei,A.Kucukelbir,andJ.D.McAuliffe,\u2018\u2018VariationalInference:AReviewfor Statisticians,\u2019\u2019JournaloftheAmericanStatisticalAssociation,vol.112,no.518,pp.859\u2013 877,2017(cit.onp.130). 2. D.P.KingmaandM.Welling,\u2018\u2018Auto-EncodingVariationalBayes,\u2019\u2019ArXivPreprint ArXiv:1312.6114,2013(cit.onpp.128,132). 3.",
      "chunk_id": 305,
      "start_pos": 292418,
      "end_pos": 293603,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    },
    {
      "content": "aloftheAmericanStatisticalAssociation,vol.112,no.518,pp.859\u2013 877,2017(cit.onp.130). 2. D.P.KingmaandM.Welling,\u2018\u2018Auto-EncodingVariationalBayes,\u2019\u2019ArXivPreprint ArXiv:1312.6114,2013(cit.onpp.128,132). 3. M.J.KochenderferandT.A.Wheeler,AlgorithmsforOptimization.MITPress,2019(cit. onp.viii). 2021-05-2300:18:27-07:00,draft: sendcommentstomossr@cs.stanford.edu",
      "chunk_id": 306,
      "start_pos": 293403,
      "end_pos": 293758,
      "document_metadata": {
        "filename": "tmpbwdfyv4j.pdf",
        "file_path": "/Users/sumitkumarsingh/Downloads/End-to-End Build Instructions/vector_store/tmpbwdfyv4j.pdf",
        "file_size": 1294739,
        "file_type": ".pdf"
      },
      "source_document": "tmpbwdfyv4j.pdf"
    }
  ]
}